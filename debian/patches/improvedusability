Description: Improved usability and optional Shogun integration
 This patch improves usability by adding an implementation of N-th order Markov chains
 and simplified core-CRE prediction. This version also introduces an optional
 integration with the Shogun machine learning library for additional supervised
 learning algorithms.
 .
 mocca (1.1) stable; urgency=medium
 .
   * Improved ease of use functionality, and added optional integration
     with the Shogun machine learning library
Author: Bjørn Bredesen <bjorn@bjornbredesen.no>

---
The information above should follow the Patch Tagging Guidelines, please
checkout http://dep.debian.net/deps/dep3/ to learn about the format. Here
are templates for supplementary fields that you might want to add:

Origin: <vendor|upstream|other>, <url of original patch>
Bug: <url in upstream bugtracker>
Bug-Debian: https://bugs.debian.org/<bugnumber>
Bug-Ubuntu: https://launchpad.net/bugs/<bugnumber>
Forwarded: <no|not-needed|url proving that it has been forwarded>
Reviewed-By: <name and email of someone who approved the patch>
Last-Update: <YYYY-MM-DD>

--- mocca-1.1.orig/Makefile.am
+++ mocca-1.1/Makefile.am
@@ -1,4 +1,4 @@
 AUTOMAKE_OPTIONS = foreign subdir-objects
-AM_LDFLAGS = -shared -O3
+AM_LDFLAGS = -O3
 SUBDIRS = src
 
--- mocca-1.1.orig/README.md
+++ mocca-1.1/README.md
@@ -8,8 +8,13 @@ Copyright Bjørn Bredesen, 2011-2019
 -------------------------------------------------
 
 ## About
-MOCCA (Motif Occurrence Combinatorics Classification Algorithms) is a suite for modelling DNA cis-regulatory element sequences.
-Several types of motif-based models are included within MOCCA: a reimplementation of the PREdictor (Ringrose *et al.* 2003), implementations of the Dummy PREdictor and SVM-MOCCA (Bredesen *et al.* 2019), as various motif-based kernel functions that can be combined with log-odds and Support Vector Machine models.
+MOCCA (Motif Occurrence Combinatorics Classification Algorithms) is a suite for modelling DNA *cis*-regulatory element (CRE) sequences.
+With MOCCA, we include the first polished, efficient and configurable implementation of the Support Vector Machine Motif Occurrence Combinatorics Classification Algorithm (SVM-MOCCA), a method that we previously presented and found to improve generalization to Polycomb/Trithorax Response Elements (PREs) (Bredesen *et al.* 2019), a class of *cis*-regulatory elements (CREs) that maintains epigenetic memory.
+SVM-MOCCA is a hierarchical method based on Support Vector Machines (SVMs) and motifs, where one SVM is trained per motif to classify its occurrences, with the feature space consisting of local dinucleotide and motif occurrence frequencies. Positively classified motif occurrences are subsequently combined using a log-odds model for a final prediction score.
+SVM-MOCCA distinguishes itself from classical use of SVMs with motifs for the modelling of CRE sequences, where SVMs are trained with motif occurrence frequencies or *k*-spectra, whereas the MOCCA methods train one model per motif and combine predictions.
+MOCCA also includes a derivative method based on Random Forests called the Random Forest Motif Occurrence Combinatorics Classification Algorithm (RF-MOCCA).
+In addition, MOCCA implements support for training log-odds models and classical SVM and RF models using a variety of feature space formulations.
+MOCCA includes functionality for the generation of negative data, threshold calibration and genome-wide prediction, and also an automated mode that requires only that the user specifies positive sequences, motifs and a genome.
 
 #### References
  * Bredesen *et al.* 2019: https://academic.oup.com/nar/article/47/15/7781/5538007
@@ -20,12 +25,15 @@ Several types of motif-based models are
 
 ## Installing
 
-On Debian-based systems, the easiest way to install MOCCA is using `apt-get`. MOCCA is available via a PPA on launchpad. Run
+On Debian-based systems, the easiest way to install MOCCA is using `apt-get`. Ubuntu builds of MOCCA are available via a PPA on launchpad. Run
 ```sudo add-apt-repository ppa:bjornbredesen/mocca && sudo apt-get update && sudo apt-get install mocca```
 
 To build, run
 `autoreconf --install && ./configure && make`.
 
+MOCCA can optionally link with Shogun, for support for additional supervised learning methods. In order to link with Shogun (version => 16), run `./configure` with the additional directive `--use-shogun`.
+`autoreconf --install && ./configure --enable-shogun && make`.
+
 After building, MOCCA can be installed by running
 `sudo make install`.
 
@@ -59,14 +67,17 @@ See the `tutorial/` folder and `mocca --
      * Dummy PREdictor
      * CPREdictor
      * SVM-MOCCA (the Support Vector Machine Motif Occurrence Combinatorics Classification Algorithm)
+     * RF-MOCCA (the Random Forest Motif Occurrence Combinatorics Classification Algorithm)
      * Log-odds models with motif-based feature spaces
      * Support Vector Machines with motif-based feature spaces
+     * Random Forests with motif-based feature spaces
  - Motif handling
      * Command-line specification of IUPAC motifs
      * Loading of IUPAC motifs from XML
-     * Generation of random IUPAC motifs.
-     * Full *k*-mer sets.
+     * Generation of random IUPAC motifs
+     * Full *k*-mer sets
      * IUPAC motif occurrence parsing Finite State Machine
+     * Position Weight Matrix motifs
  - Feature spaces
      * Motif occurrence frequency spectrum
      * Motif pair occurrence frequency spectrum, with distance cutoff, and multiple distancing and overlap modes
@@ -76,8 +87,12 @@ See the `tutorial/` folder and `mocca --
  - Core usage features
      * Training with FASTA sequence files
      * Validation with FASTA sequence files
-     * Saving sequence scores to table
+     * Prediction threshold calibration for a desired precision
+     * Genome-wide prediction of candidate CREs to General Feature Format files
+     * Genome-wide prediction to Wiggle files
+     * Saving of sequence scores to table
      * Scoring of sequence files to Wiggle curves
+     * Automatic construction of negative training/test/calibration data
 
 
 
@@ -151,6 +166,18 @@ LIABILITY, WHETHER IN CONTRACT, STRICT L
 NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
+### Ranger
+
+MIT License
+
+Copyright (c) [2014-2018] [Marvin N. Wright]
+
+Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
 ### RapidXML
 
 Copyright (c) 2006, 2007 Marcin Kalicinski
@@ -173,3 +200,34 @@ LIABILITY, WHETHER IN AN ACTION OF CONTR
 OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS 
 IN THE SOFTWARE.
 
+### Shogun
+
+Copyright (c) Shogun Machine Learning Toolbox developers <shogun-team@shogun-toolbox.org>
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+  1. Redistributions of source code must retain the above copyright notice,
+     this list of conditions and the following disclaimer.
+
+  2. Redistributions in binary form must reproduce the above copyright
+     notice, this list of conditions and the following disclaimer in the
+     documentation and/or other materials provided with the distribution.
+
+  3. Neither the name of the copyright holder nor the names of its
+     contributors may be used to endorse or promote products derived from
+     this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
+LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+POSSIBILITY OF SUCH DAMAGE.
+
--- mocca-1.1.orig/configure.ac
+++ mocca-1.1/configure.ac
@@ -1,13 +1,23 @@
-AC_INIT(mocca, 1.0)
+AC_INIT(mocca, 1.1)
 
 AM_INIT_AUTOMAKE
 
+AC_ARG_ENABLE([shogun],
+[  --enable-shogun    Use Shogun machine learning library],
+[case "${enableval}" in
+  yes) shogun=true ;;
+  no)  shogun=false ;;
+  *) AC_MSG_ERROR([bad value ${enableval} for --enable-shogun]) ;;
+esac],[shogun=false])
+AM_CONDITIONAL([USE_SHOGUN], [test x$shogun = xtrue])
+
 AC_PROG_MAKE_SET
 
 AC_LANG(C++)
 AC_PROG_CXX
 
 CXXFLAGS="-std=c++11 -fPIC"
+LDFLAGS="-pthread"
 
 AC_CONFIG_FILES([Makefile src/Makefile])
 AC_OUTPUT
--- mocca-1.1.orig/src/Makefile.am
+++ mocca-1.1/src/Makefile.am
@@ -1,4 +1,12 @@
 AUTOMAKE_OPTIONS = foreign subdir-objects
 bin_PROGRAMS = mocca
-mocca_SOURCES = main.cpp aux.cpp config.cpp motifs.cpp sequencelist.cpp sequences.cpp validation.cpp models/baseclassifier.cpp models/cpredictor.cpp models/dummypredictor.cpp models/features.cpp models/seqdummy.cpp models/seqlo.cpp models/seqsvm.cpp models/sequenceclassifier.cpp models/svmmocca.cpp lib/libsvm-3.17/svm.cpp
+
+if USE_SHOGUN
+mocca_SOURCES = main.cpp aux.cpp config.cpp motifs.cpp sequencelist.cpp sequences.cpp validation.cpp models/baseclassifier.cpp models/cpredictor.cpp models/dummypredictor.cpp models/features.cpp models/seqdummy.cpp models/seqlo.cpp models/seqsvm.cpp models/seqrf.cpp models/seqlda.cpp models/seqperceptron.cpp models/sequenceclassifier.cpp models/svmmocca.cpp models/rfmocca.cpp lib/libsvm-3.17/svm.cpp lib/ranger/src/Forest.cpp lib/ranger/src/ForestClassification.cpp lib/ranger/src/ForestProbability.cpp lib/ranger/src/ForestRegression.cpp lib/ranger/src/Tree.cpp lib/ranger/src/TreeClassification.cpp lib/ranger/src/TreeProbability.cpp lib/ranger/src/TreeRegression.cpp lib/ranger/src/Data.cpp lib/ranger/src/utility.cpp
+mocca_LDFLAGS = -O3
+mocca_LDADD = -lshogun
+mocca_CPPFLAGS = -DUSE_SHOGUN
+else
+mocca_SOURCES = main.cpp aux.cpp config.cpp motifs.cpp sequencelist.cpp sequences.cpp validation.cpp models/baseclassifier.cpp models/cpredictor.cpp models/dummypredictor.cpp models/features.cpp models/seqdummy.cpp models/seqlo.cpp models/seqsvm.cpp models/seqrf.cpp models/sequenceclassifier.cpp models/svmmocca.cpp models/rfmocca.cpp lib/libsvm-3.17/svm.cpp lib/ranger/src/Forest.cpp lib/ranger/src/ForestClassification.cpp lib/ranger/src/ForestProbability.cpp lib/ranger/src/ForestRegression.cpp lib/ranger/src/Tree.cpp lib/ranger/src/TreeClassification.cpp lib/ranger/src/TreeProbability.cpp lib/ranger/src/TreeRegression.cpp lib/ranger/src/Data.cpp lib/ranger/src/utility.cpp
+endif
 
--- mocca-1.1.orig/src/config.cpp
+++ mocca-1.1/src/config.cpp
@@ -39,6 +39,7 @@ config _config = config {
 	true,
 	0.0,1.0,0.5,0.995,1.0,
 	0.0,
+	500, 8,
 	//bool MOCCA_nOcc, MOCCA_GC, MOCCA_DNT;
 	false, false, false,
 	EPSILON_SVR,
@@ -48,7 +49,10 @@ config _config = config {
 	"","","",
 	"",
 	"","","",
-	-1.
+	-1.,
+	4, // Background model order
+	cpmNone,
+	false
 };
 
 config*getConfiguration(){
@@ -71,6 +75,15 @@ void config::printInfo(){
 	}
 	if(motifPairsCanOverlap)cout << " (can overlap)\n";
 	else cout << " (can not overlap)\n";
+	cout << t_indent << "Core prediction: ";
+	switch(corePredictionMode){
+		case cpmNone:cout << "Disabled";break;
+		case cpmContinuous:cout << "Continuous";break;
+		case cpmMotifs:cout << "Motifs";break;
+		case cpmMotifsStrong:cout << "Motifs (strong)";break;
+	}
+	if(corePredictionMax) cout << " - maximum";
+	cout << "\n";
 	cout << t_indent << "Log-odds mode: ";
 	switch(wmMode){
 		case wmPREdictor:cout << "PREdictor";break;
--- mocca-1.1.orig/src/config.hpp
+++ mocca-1.1/src/config.hpp
@@ -37,10 +37,21 @@ enum classifierT{
 	cInvalid,
 	cCPREdictor,
 	cSVMMOCCA,
+	cRFMOCCA,
 	cDummyPREdictor,
 	cSEQSVM,
+	cSEQRF,
 	cSEQLO,
 	cSEQDummy,
+	cSEQLDA,
+	cSEQPerceptron,
+};
+
+enum corePredictionModeT{
+	cpmNone,
+	cpmContinuous,
+	cpmMotifs,
+	cpmMotifsStrong,
 };
 
 class config{
@@ -56,6 +67,7 @@ public:
 	bool motifPairsCanOverlap;
 	double SVM_c0,SVM_C,SVM_nu,SVM_p,SVM_gamma;
 	double threshold;
+	int RF_nTrees, nThreads;
 	bool MOCCA_nOcc, MOCCA_GC, MOCCA_DNT;
 	int svmtype;
 	weightMode wmMode;
@@ -67,6 +79,9 @@ public:
 	std::string predictGFFPath;
 	std::string predictWigPath;
 	double wantPrecision;
+	int bgOrder;
+	corePredictionModeT corePredictionMode;
+	bool corePredictionMax;
 	/*
 	printInfo
 		Prints out information
--- /dev/null
+++ mocca-1.1/src/lib/ranger/NEWS
@@ -0,0 +1,164 @@
+
+##### Version 0.12.2
+* Bug fixes
+
+##### Version 0.12.1
+* Bug fixes
+
+##### Version 0.12.0
+* Faster computation (in some cases)
+* Add local variable importance 
+* Add "hellinger" splitrule for binary classification
+* Add "beta" splitrule for bounded outcomes
+* Accept user-specified function in quantile prediction
+* Add regularization
+* Add x/y interface
+* Internal changes (seed differences possible, prediction incompatible with older versions)
+* Bug fixes
+
+##### Version 0.11.0
+* Add max.depth parameter to limit tree depth
+* Add inbag argument for manual selection of observations in trees
+* Add support of splitting weights for corrected impurity importance 
+* Internal changes (slightly improved computation speed)
+* Warning: Possible seed differences compared to older versions
+* Bug fixes
+
+##### Version 0.10.0
+* Change license of C++ core to MIT (R package is still GPL3)
+* Better 'order' mode for unordered factors for multiclass and survival
+* Add 'order' mode for unordered factors for GenABEL SNP data (binary classification and regression)
+* Add class-weighted Gini splitting
+* Add fixed proportion sampling
+* Add impurity importance for the maxstat splitting rule
+* Remove GenABEL from suggested packages (removed from CRAN). GenABEL data is still supported
+* Improve memory management (internal changes)
+* Bug fixes
+
+##### Version 0.9.0
+* Add bias-corrected impurity importance (actual impurity reduction, AIR)
+* Add quantile prediction as in quantile regression forests
+* Add treeInfo() function to extract human readable tree structure
+* Add standard error estimation with the infinitesimal jackknife (now the default)
+* Add impurity importance for survival forests
+* Faster aggregation of predictions
+* Fix memory issues on Windows 7
+* Bug fixes
+
+##### Version 0.8.0
+* Handle sparse data of class Matrix::dgCMatrix
+* Add prediction of standard errors to predict()
+* Allow devtools::install_github() without subdir and on Windows
+* Bug fixes
+
+##### Version 0.7.0
+* Add randomized splitting (extraTrees)
+* Better formula interface: Support interactions terms and faster computation
+* Split at mid-point between candidate values
+* Improvements in holdoutRF and importance p-value estimation
+* Drop unused factor levels in outcome before growing
+* Add predict.all for probability and survival prediction
+* Bug fixes
+
+##### Version 0.6.0
+* Set write.forest=TRUE by default
+* Add num.trees option to predict()
+* Faster version of getTerminalNodeIDs(), included in predict()
+* Handle new factor levels in 'order' mode
+* Use unadjusted p-value for 2 categories in maxstat splitting
+* Bug fixes
+
+##### Version 0.5.0
+* Add Windows multithreading support for new toolchain
+* Add splitting by maximally selected rank statistics for survival and regression forests
+* Faster method for unordered factor splitting
+* Add p-values for variable importance
+* Runtime improvement for regression forests on classification data
+* Bug fixes
+
+##### Version 0.4.0
+* Reduce memory usage of savest forest objects (changed child.nodeIDs interface)
+* Add keep.inbag option to track in-bag counts
+* Add option sample.fraction for fraction of sampled observations
+* Add tree-wise split.select.weights
+* Add predict.all option in predict() to get individual predictions for each tree for classification and regression
+* Add case-specific random forests
+* Add case weights (weighted bootstrapping or subsampling)
+* Remove tuning functions, please use mlr or caret
+* Catch error of outdated gcc not supporting C++11 completely
+* Bug fixes
+
+##### Version 0.3.0
+* Allow the user to interrupt computation from R
+* Transpose classification.table and rename to confusion.matrix
+* Respect R seed for prediction
+* Memory improvements for variable importance computation
+* Fix bug: Probability prediction for single observations
+* Fix bug: Results not identical when using alternative interface
+
+##### Version 0.2.7 
+* Small fixes for Solaris compiler
+
+##### Version 0.2.6 
+* Add C-index splitting
+* Fix NA SNP handling
+
+##### Version 0.2.5 
+* Fix matrix and gwaa alternative survival interface
+* Version submitted to JSS
+
+##### Version 0.2.4 
+* Small changes in documentation
+
+##### Version 0.2.3 
+* Preallocate memory for splitting
+
+##### Version 0.2.2 
+* Remove recursive splitting
+
+##### Version 0.2.1 
+* Allow matrix as input data in R version
+
+##### Version 0.2.0 
+* Fix prediction of classification forests in R
+
+##### Version 0.1.9 
+* Speedup growing for continuous covariates
+* Add memory save option to save memory for very large datasets (but slower)
+* Remove memory mode option from R version since no performance gain
+
+##### Version 0.1.8 
+* Fix problems when using Rcpp <0.11.4
+
+##### Version 0.1.7 
+* Add option to split on unordered categorical covariates
+
+##### Version 0.1.6 
+* Optimize memory management for very large survival forests
+
+##### Version  0.1.5 
+* Set required Rcpp version to 0.11.2
+* Fix large $call objects when using BatchJobs
+* Add details and example on GenABEL usage to documentation
+* Minor changes to documentation
+
+##### Version 0.1.4 
+* Speedup for survival forests with continuous covariates
+* R version: Generate seed from R. It is no longer necessary to set the
+  seed argument in ranger calls.
+
+##### Version 0.1.3 
+* Windows support for R version (without multithreading)
+
+##### Version 0.1.2 
+* Speedup growing of regression and probability prediction forests
+* Prediction forests are now handled like regression forests: MSE used for
+	prediction error and permutation importance
+* Fixed name conflict with randomForest package for "importance"
+* Fixed a bug: prediction function is now working for probability
+	prediction forests
+* Slot "predictions" for probability forests now contains class probabilities
+* importance function is now working even if randomForest package is
+	loaded after ranger
+* Fixed a bug: Split selection weights are now working as expected
+* Small changes in documentation
--- /dev/null
+++ mocca-1.1/src/lib/ranger/NEWS.md
@@ -0,0 +1,314 @@
+
+##### Version 0.12.2
+* Bug fixes
+
+##### Version 0.12.1
+* Bug fixes
+
+##### Version 0.12.0
+* New CRAN version
+
+##### Version 0.11.8
+* Add regularization
+* Faster computation (in some cases)
+
+##### Version 0.11.7
+* Add local variable importance 
+
+##### Version 0.11.6
+* Add "hellinger" splitrule for binary classification
+
+##### Version 0.11.5
+* Add x/y interface
+* Internal changes (seed differences possible, prediction incompatible with older versions)
+
+##### Version 0.11.4
+* Add "beta" splitrule for bounded outcomes
+
+##### Version 0.11.3
+* Accept user-specified function in quantile prediction
+
+##### Version 0.11.2
+* Bug fixes
+
+##### Version 0.11.1
+* Bug fixes
+
+##### Version 0.11.0
+* New CRAN version
+
+##### Version 0.10.6
+* Internal changes (slightly improved computation speed)
+* Warning: Possible seed differences compared to older versions
+* Bug fixes
+
+##### Version 0.10.5
+* Add support of splitting weights for corrected impurity importance 
+* Bug fixes
+
+##### Version 0.10.4
+* Add inbag argument for manual selection of observations in trees
+
+##### Version 0.10.3
+* Bug fixes
+
+##### Version 0.10.2
+* Add max.depth parameter to limit tree depth
+
+##### Version 0.10.1
+* Bug fixes
+
+##### Version 0.10.0
+* New CRAN version
+
+##### Version 0.9.12
+* Remove GenABEL from suggested packages (removed from CRAN). GenABEL data is still supported
+
+##### Version 0.9.11
+* Improve memory management (internal changes)
+
+##### Version 0.9.10
+* Add impurity importance for the maxstat splitting rule
+* Bug fixes
+
+##### Version 0.9.9
+* Add 'order' mode for unordered factors for GenABEL SNP data (binary classification and regression)
+
+##### Version 0.9.8
+* Bug fixes
+
+##### Version 0.9.7
+* Change license of C++ core to MIT (R package is still GPL3)
+
+##### Version 0.9.6
+* Better 'order' mode for unordered factors for multiclass and survival
+
+##### Version 0.9.5
+* Bug fixes
+
+##### Version 0.9.4
+* Add class-weighted Gini splitting
+
+##### Version 0.9.3
+* Bug fixes
+
+##### Version 0.9.2
+* Add fixed proportion sampling
+
+##### Version 0.9.1
+* Bug fixes
+
+##### Version 0.9.0
+* New CRAN version
+
+##### Version 0.8.5
+* Faster aggregation of predictions
+* Fix memory issues on Windows 7
+* Add treeInfo() function to extract human readable tree structure
+
+##### Version 0.8.4
+* Add quantile prediction as in quantile regression forests
+
+##### Version 0.8.3
+* Add standard error estimation with the infinitesimal jackknife (now the default)
+
+##### Version 0.8.2
+* Add bias-corrected impurity importance (actual impurity reduction, AIR)
+* Add impurity importance for survival forests
+
+##### Version 0.8.1
+* Bug fixes
+
+##### Version 0.8.0
+* New CRAN version
+
+##### Version 0.7.2
+* Handle sparse data of class Matrix::dgCMatrix
+* Add prediction of standard errors to predict()
+
+##### Version 0.7.1
+* Allow devtools::install_github() without subdir and on Windows
+* Bug fixes
+
+##### Version 0.7.0
+* New CRAN version
+
+##### Version 0.6.7
+* Improvements in holdoutRF and importance p-value estimation
+
+##### Version 0.6.6
+* Split at mid-point between candidate values
+
+##### Version 0.6.5
+* Better formula interface: Support interactions terms and faster computation
+
+##### Version 0.6.4
+* Add randomized splitting (extraTrees)
+
+##### Version 0.6.3
+* Bug fixes
+
+##### Version 0.6.2
+* Drop unused factor levels in outcome before growing
+* Add predict.all for probability and survival prediction
+
+##### Version 0.6.1
+* Bug fixes
+
+##### Version 0.6.0
+* New CRAN version
+
+##### Version 0.5.6
+* Faster version of getTerminalNodeIDs(), included in predict()
+
+##### Version 0.5.5
+* Handle new factor levels in 'order' mode
+* Bug fixes
+
+##### Version 0.5.4
+* Set write.forest=TRUE by default
+* Add num.trees option to predict()
+* Bug fixes
+
+##### Version 0.5.3
+* Bug fixes
+
+##### Version 0.5.2
+* Use unadjusted p-value for 2 categories in maxstat splitting
+
+##### Version 0.5.1
+* Bug fixes
+
+##### Version 0.5.0
+* New CRAN version
+
+##### Version 0.4.7
+* Add splitting by maximally selected rank statistics for regression forests
+
+##### Version 0.4.6
+* Bug fixes
+
+##### Version 0.4.5
+* Use faster method for unordered factor splitting
+
+##### Version 0.4.4
+* Add p-values for variable importance
+* Bug fixes
+
+##### Version 0.4.3
+* Add splitting by maximally selected rank statistics for survival forests
+* Bug fixes
+
+##### Version 0.4.2
+* Add Windows multithreading support for new toolchain
+
+##### Version 0.4.1
+* Runtime improvement for regression forests on classification data
+
+##### Version 0.4.0
+* New CRAN version. New CRAN versions will be 0.x.0, development versions 0.x.y
+
+##### Version 0.3.9
+* Reduce memory usage of savest forest objects (changed child.nodeIDs interface)
+
+##### Version 0.3.8
+* Remove tuning functions, please use mlr or caret
+
+##### Version 0.3.7
+* Fix bug with alternative interface and prediction
+* Small fixes
+
+##### Version 0.3.6
+* Add keep.inbag option to track in-bag counts
+* Add option sample.fraction for fraction of sampled observations
+
+##### Version 0.3.5
+* Add tree-wise split.select.weights
+
+##### Version 0.3.4
+* Add predict.all option in predict() to get individual predictions for each tree for classification and regression
+* Small changes in documentation
+
+##### Version 0.3.3
+* Add case-specific random forests
+
+##### Version 0.3.2
+* Add case weights (weighted bootstrapping or subsampling)
+
+##### Version 0.3.1
+* Catch error of outdated gcc not supporting C++11 completely
+
+##### Version 0.3.0
+* Allow the user to interrupt computation from R
+* Transpose classification.table and rename to confusion.matrix
+* Respect R seed for prediction
+* Memory improvements for variable importance computation
+* Fix bug: Probability prediction for single observations
+* Fix bug: Results not identical when using alternative interface
+
+##### Version 0.2.7 
+* Small fixes for Solaris compiler
+
+##### Version 0.2.6 
+* Add C-index splitting
+* Fix NA SNP handling
+
+##### Version 0.2.5 
+* Fix matrix and gwaa alternative survival interface
+* Version submitted to JSS
+
+##### Version 0.2.4 
+* Small changes in documentation
+
+##### Version 0.2.3 
+* Preallocate memory for splitting
+
+##### Version 0.2.2 
+* Remove recursive splitting
+
+##### Version 0.2.1 
+* Allow matrix as input data in R version
+
+##### Version 0.2.0 
+* Fix prediction of classification forests in R
+
+##### Version 0.1.9 
+* Speedup growing for continuous covariates
+* Add memory save option to save memory for very large datasets (but slower)
+* Remove memory mode option from R version since no performance gain
+
+##### Version 0.1.8 
+* Fix problems when using Rcpp <0.11.4
+
+##### Version 0.1.7 
+* Add option to split on unordered categorical covariates
+
+##### Version 0.1.6 
+* Optimize memory management for very large survival forests
+
+##### Version  0.1.5 
+* Set required Rcpp version to 0.11.2
+* Fix large $call objects when using BatchJobs
+* Add details and example on GenABEL usage to documentation
+* Minor changes to documentation
+
+##### Version 0.1.4 
+* Speedup for survival forests with continuous covariates
+* R version: Generate seed from R. It is no longer necessary to set the
+  seed argument in ranger calls.
+
+##### Version 0.1.3 
+* Windows support for R version (without multithreading)
+
+##### Version 0.1.2 
+* Speedup growing of regression and probability prediction forests
+* Prediction forests are now handled like regression forests: MSE used for
+	prediction error and permutation importance
+* Fixed name conflict with randomForest package for "importance"
+* Fixed a bug: prediction function is now working for probability
+	prediction forests
+* Slot "predictions" for probability forests now contains class probabilities
+* importance function is now working even if randomForest package is
+	loaded after ranger
+* Fixed a bug: Split selection weights are now working as expected
+* Small changes in documentation
--- /dev/null
+++ mocca-1.1/src/lib/ranger/README.md
@@ -0,0 +1,82 @@
+[![Travis Build Status](https://travis-ci.org/imbs-hl/ranger.svg?branch=master)](https://travis-ci.org/imbs-hl/ranger)
+[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/imbs-hl/ranger?branch=master&svg=true)](https://ci.appveyor.com/project/mnwright/ranger)
+[![Coverage Status](https://coveralls.io/repos/github/imbs-hl/ranger/badge.svg?branch=master)](https://coveralls.io/github/imbs-hl/ranger?branch=master)
+![CRAN Downloads month](http://cranlogs.r-pkg.org/badges/ranger?color=brightgreen)
+![CRAN Downloads overall](http://cranlogs.r-pkg.org/badges/grand-total/ranger?color=brightgreen)
+## ranger: A Fast Implementation of Random Forests
+Marvin N. Wright
+
+### Introduction
+ranger is a fast implementation of random forests (Breiman 2001) or recursive partitioning, particularly suited for high dimensional data. Classification, regression, and survival forests are supported. Classification and regression forests are implemented as in the original Random Forest (Breiman 2001), survival forests as in Random Survival Forests (Ishwaran et al. 2008). Includes implementations of extremely randomized trees (Geurts et al. 2006) and quantile regression forests (Meinshausen 2006).
+
+ranger is written in C++, but a version for R is available, too. We recommend to use the R version. It is easy to install and use and the results are readily available for further analysis. The R version is as fast as the standalone C++ version.
+
+### Installation
+#### R version
+To install the ranger R package from CRAN, just run
+
+```R
+install.packages("ranger")
+```
+
+R version >= 3.1 is required. With recent R versions, multithreading on Windows platforms should just work. If you compile yourself, the new RTools toolchain is required.
+
+To install the development version from GitHub using `devtools`, run
+
+```R
+devtools::install_github("imbs-hl/ranger")
+```
+
+#### Standalone C++ version
+To install the C++ version of ranger in Linux or Mac OS X you will need a compiler supporting C++11 (i.e. gcc >= 4.7 or Clang >= 3.0) and Cmake. To build start a terminal from the ranger main directory and run the following commands
+
+```bash
+cd cpp_version
+mkdir build
+cd build
+cmake ..
+make
+```
+
+After compilation there should be an executable called "ranger" in the build directory. 
+
+To run the C++ version in Microsoft Windows please cross compile or ask for a binary.
+
+### Usage
+#### R version
+For usage of the R version see ?ranger in R. Most importantly, see the Examples section. As a first example you could try 
+
+```R  
+ranger(Species ~ ., data = iris)
+```
+
+#### Standalone C++ version
+In the C++ version type 
+
+```bash
+./ranger --help 
+```
+
+for a list of commands. First you need a training dataset in a file. This file should contain one header line with variable names and one line with variable values per sample (numeric only). Variable names must not contain any whitespace, comma or semicolon. Values can be seperated by whitespace, comma or semicolon but can not be mixed in one file. A typical call of ranger would be for example
+
+```bash
+./ranger --verbose --file data.dat --depvarname Species --treetype 1 --ntree 1000 --nthreads 4
+```
+
+If you find any bugs, or if you experience any crashes, please report to us. If you have any questions just ask, we won't bite. 
+
+Please cite our paper if you use ranger.
+
+### References
+* Wright, M. N. & Ziegler, A. (2017). ranger: A fast implementation of random forests for high dimensional data in C++ and R. J Stat Softw 77:1-17. https://doi.org/10.18637/jss.v077.i01.
+* Schmid, M., Wright, M. N. & Ziegler, A. (2016). On the use of Harrell's C for clinical risk prediction via random survival forests. Expert Syst Appl 63:450-459. https://doi.org/10.1016/j.eswa.2016.07.018.
+* Wright, M. N., Dankowski, T. & Ziegler, A. (2017). Unbiased split variable selection for random survival forests using maximally selected rank statistics. Stat Med 36:1272-1284. https://doi.org/10.1002/sim.7212.
+* Nembrini, S., König, I. R. & Wright, M. N. (2018). The revival of the Gini Importance? Bioinformatics. https://doi.org/10.1093/bioinformatics/bty373.
+* Breiman, L. (2001). Random forests. Mach Learn, 45:5-32. https://doi.org/10.1023/A:1010933404324.
+* Ishwaran, H., Kogalur, U. B., Blackstone, E. H., & Lauer, M. S. (2008). Random survival forests. Ann Appl Stat 2:841-860. https://doi.org/10.1097/JTO.0b013e318233d835.
+* Malley, J. D., Kruppa, J., Dasgupta, A., Malley, K. G., & Ziegler, A. (2012). Probability machines: consistent probability estimation using nonparametric learning machines. Methods Inf Med 51:74-81. https://doi.org/10.3414/ME00-01-0052.
+* Hastie, T., Tibshirani, R., Friedman, J. (2009). The Elements of Statistical Learning. Springer, New York. 2nd edition.
+* Geurts, P., Ernst, D., Wehenkel, L. (2006). Extremely randomized trees. Mach Learn 63:3-42. https://doi.org/10.1007/s10994-006-6226-1.
+* Meinshausen (2006). Quantile Regression Forests. J Mach Learn Res 7:983-999. http://www.jmlr.org/papers/v7/meinshausen06a.html.
+* Sandri, M. & Zuccolotto, P. (2008). A bias correction algorithm for the Gini variable importance measure in classification trees. J Comput Graph Stat, 17:611-628. https://doi.org/10.1198/106186008X344522.
+* Coppersmith D., Hong S. J., Hosking J. R. (1999). Partitioning nominal attributes in decision trees. Data Min Knowl Discov 3:197-217. https://doi.org/10.1023/A:1009869804967.
--- /dev/null
+++ mocca-1.1/src/lib/ranger/cpp_version/COPYING
@@ -0,0 +1,9 @@
+MIT License
+
+Copyright (c) [2014-2018] [Marvin N. Wright]
+
+Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
\ No newline at end of file
--- /dev/null
+++ mocca-1.1/src/lib/ranger/inst/CITATION
@@ -0,0 +1,22 @@
+bibentry(bibtype = "Article",
+  title        = "{ranger}: A Fast Implementation of Random Forests for High Dimensional Data in {C++} and {R}",
+  author       = c(person(given = c("Marvin", "N."),
+                          family = "Wright"),
+                   person(given = "Andreas",
+                          family = "Ziegler",
+                          email = "ziegler@imbs.uni-luebeck.de")),
+  journal      = "Journal of Statistical Software",
+  year         = "2017",
+  volume       = "77",
+  number       = "1",
+  pages        = "1--17",
+  doi          = "10.18637/jss.v077.i01",
+
+  header       = "To cite ranger in publications use:",
+  textVersion  =
+  paste("Marvin N. Wright, Andreas Ziegler (2017).",
+        "ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.",
+        "Journal of Statistical Software, 77(1), 1-17.",
+        "doi:10.18637/jss.v077.i01")
+)
+
--- /dev/null
+++ mocca-1.1/src/lib/ranger/inst/include/ranger.h
@@ -0,0 +1,2 @@
+#include "../../src/globals.h"
+using namespace ranger;
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/AAA_check_cpp11.cpp
@@ -0,0 +1,6 @@
+#ifndef WIN_R_BUILD
+#if __cplusplus < 201103L
+#error Error: ranger requires a real C++11 compiler, e.g., gcc >= 4.7 or Clang >= 3.0. You probably have to update your C++ compiler.
+#endif
+#endif
+
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/Data.cpp
@@ -0,0 +1,342 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#include <fstream>
+#include <sstream>
+#include <stdexcept>
+#include <algorithm>
+#include <iterator>
+
+#include "Data.h"
+#include "utility.h"
+
+namespace ranger {
+
+Data::Data() :
+    num_rows(0), num_rows_rounded(0), num_cols(0), snp_data(0), num_cols_no_snp(0), externalData(true), index_data(0), max_num_unique_values(
+        0), order_snps(false) {
+}
+
+size_t Data::getVariableID(const std::string& variable_name) const {
+  auto it = std::find(variable_names.cbegin(), variable_names.cend(), variable_name);
+  if (it == variable_names.cend()) {
+    throw std::runtime_error("Variable " + variable_name + " not found.");
+  }
+  return (std::distance(variable_names.cbegin(), it));
+}
+
+// #nocov start (cannot be tested anymore because GenABEL not on CRAN)
+void Data::addSnpData(unsigned char* snp_data, size_t num_cols_snp) {
+  num_cols = num_cols_no_snp + num_cols_snp;
+  num_rows_rounded = roundToNextMultiple(num_rows, 4);
+  this->snp_data = snp_data;
+}
+// #nocov end
+
+// #nocov start
+bool Data::loadFromFile(std::string filename, std::vector<std::string>& dependent_variable_names) {
+
+  bool result;
+
+  // Open input file
+  std::ifstream input_file;
+  input_file.open(filename);
+  if (!input_file.good()) {
+    throw std::runtime_error("Could not open input file.");
+  }
+
+  // Count number of rows
+  size_t line_count = 0;
+  std::string line;
+  while (getline(input_file, line)) {
+    ++line_count;
+  }
+  num_rows = line_count - 1;
+  input_file.close();
+  input_file.open(filename);
+
+  // Check if comma, semicolon or whitespace seperated
+  std::string header_line;
+  getline(input_file, header_line);
+
+  // Find out if comma, semicolon or whitespace seperated and call appropriate method
+  if (header_line.find(",") != std::string::npos) {
+    result = loadFromFileOther(input_file, header_line, dependent_variable_names, ',');
+  } else if (header_line.find(";") != std::string::npos) {
+    result = loadFromFileOther(input_file, header_line, dependent_variable_names, ';');
+  } else {
+    result = loadFromFileWhitespace(input_file, header_line, dependent_variable_names);
+  }
+
+  externalData = false;
+  input_file.close();
+  return result;
+}
+
+bool Data::loadFromFileWhitespace(std::ifstream& input_file, std::string header_line,
+    std::vector<std::string>& dependent_variable_names) {
+
+  size_t num_dependent_variables = dependent_variable_names.size();
+  std::vector<size_t> dependent_varIDs;
+  dependent_varIDs.resize(num_dependent_variables);
+
+  // Read header
+  std::string header_token;
+  std::stringstream header_line_stream(header_line);
+  size_t col = 0;
+  while (header_line_stream >> header_token) {
+    bool is_dependent_var = false;
+    for (size_t i = 0; i < dependent_variable_names.size(); ++i) {
+      if (header_token == dependent_variable_names[i]) {
+        dependent_varIDs[i] = col;
+        is_dependent_var = true;
+      }
+    }
+    if (!is_dependent_var) {
+      variable_names.push_back(header_token);
+    }
+    ++col;
+  }
+
+  num_cols = variable_names.size();
+  num_cols_no_snp = num_cols;
+
+  // Read body
+  reserveMemory(num_dependent_variables);
+  bool error = false;
+  std::string line;
+  size_t row = 0;
+  while (getline(input_file, line)) {
+    double token;
+    std::stringstream line_stream(line);
+    size_t column = 0;
+    while (readFromStream(line_stream, token)) {
+      size_t column_x = column;
+      bool is_dependent_var = false;
+      for (size_t i = 0; i < dependent_varIDs.size(); ++i) {
+        if (column == dependent_varIDs[i]) {
+          set_y(i, row, token, error);
+          is_dependent_var = true;
+          break;
+        } else if (column > dependent_varIDs[i]) {
+          --column_x;
+        }
+      }
+      if (!is_dependent_var) {
+        set_x(column_x, row, token, error);
+      }
+      ++column;
+    }
+    if (column > (num_cols + num_dependent_variables)) {
+      throw std::runtime_error(
+          std::string("Could not open input file. Too many columns in row ") + std::to_string(row) + std::string("."));
+    } else if (column < (num_cols + num_dependent_variables)) {
+      throw std::runtime_error(
+          std::string("Could not open input file. Too few columns in row ") + std::to_string(row)
+              + std::string(". Are all values numeric?"));
+    }
+    ++row;
+  }
+  num_rows = row;
+  return error;
+}
+
+bool Data::loadFromFileOther(std::ifstream& input_file, std::string header_line,
+    std::vector<std::string>& dependent_variable_names, char seperator) {
+
+  size_t num_dependent_variables = dependent_variable_names.size();
+  std::vector<size_t> dependent_varIDs;
+  dependent_varIDs.resize(num_dependent_variables);
+
+  // Read header
+  std::string header_token;
+  std::stringstream header_line_stream(header_line);
+  size_t col = 0;
+  while (getline(header_line_stream, header_token, seperator)) {
+    bool is_dependent_var = false;
+    for (size_t i = 0; i < dependent_variable_names.size(); ++i) {
+      if (header_token == dependent_variable_names[i]) {
+        dependent_varIDs[i] = col;
+        is_dependent_var = true;
+      }
+    }
+    if (!is_dependent_var) {
+      variable_names.push_back(header_token);
+    }
+    ++col;
+  }
+
+  num_cols = variable_names.size();
+  num_cols_no_snp = num_cols;
+
+  // Read body
+  reserveMemory(num_dependent_variables);
+  bool error = false;
+  std::string line;
+  size_t row = 0;
+  while (getline(input_file, line)) {
+    std::string token_string;
+    double token;
+    std::stringstream line_stream(line);
+    size_t column = 0;
+    while (getline(line_stream, token_string, seperator)) {
+      std::stringstream token_stream(token_string);
+      readFromStream(token_stream, token);
+
+      size_t column_x = column;
+      bool is_dependent_var = false;
+      for (size_t i = 0; i < dependent_varIDs.size(); ++i) {
+        if (column == dependent_varIDs[i]) {
+          set_y(i, row, token, error);
+          is_dependent_var = true;
+          break;
+        } else if (column > dependent_varIDs[i]) {
+          --column_x;
+        }
+      }
+      if (!is_dependent_var) {
+        set_x(column_x, row, token, error);
+      }
+      ++column;
+    }
+    ++row;
+  }
+  num_rows = row;
+  return error;
+}
+// #nocov end
+
+void Data::getAllValues(std::vector<double>& all_values, std::vector<size_t>& sampleIDs, size_t varID, size_t start,
+    size_t end) const {
+
+  // All values for varID (no duplicates) for given sampleIDs
+  if (getUnpermutedVarID(varID) < num_cols_no_snp) {
+    
+    all_values.reserve(end - start);
+    for (size_t pos = start; pos < end; ++pos) {
+      all_values.push_back(get_x(sampleIDs[pos], varID));
+    }
+    std::sort(all_values.begin(), all_values.end());
+    all_values.erase(std::unique(all_values.begin(), all_values.end()), all_values.end());
+  } else {
+    // If GWA data just use 0, 1, 2
+    all_values = std::vector<double>( { 0, 1, 2 });
+  }
+}
+
+void Data::getMinMaxValues(double& min, double&max, std::vector<size_t>& sampleIDs, size_t varID, size_t start,
+    size_t end) const {
+  if (sampleIDs.size() > 0) {
+    min = get_x(sampleIDs[start], varID);
+    max = min;
+  }
+  for (size_t pos = start; pos < end; ++pos) {
+    double value = get_x(sampleIDs[pos], varID);
+    if (value < min) {
+      min = value;
+    }
+    if (value > max) {
+      max = value;
+    }
+  }
+}
+
+void Data::sort() {
+
+  // Reserve memory
+  index_data.resize(num_cols_no_snp * num_rows);
+
+  // For all columns, get unique values and save index for each observation
+  for (size_t col = 0; col < num_cols_no_snp; ++col) {
+
+    // Get all unique values
+    std::vector<double> unique_values(num_rows);
+    for (size_t row = 0; row < num_rows; ++row) {
+      unique_values[row] = get_x(row, col);
+    }
+    std::sort(unique_values.begin(), unique_values.end());
+    unique_values.erase(unique(unique_values.begin(), unique_values.end()), unique_values.end());
+
+    // Get index of unique value
+    for (size_t row = 0; row < num_rows; ++row) {
+      size_t idx = std::lower_bound(unique_values.begin(), unique_values.end(), get_x(row, col))
+          - unique_values.begin();
+      index_data[col * num_rows + row] = idx;
+    }
+
+    // Save unique values
+    unique_data_values.push_back(unique_values);
+    if (unique_values.size() > max_num_unique_values) {
+      max_num_unique_values = unique_values.size();
+    }
+  }
+}
+
+// TODO: Implement ordering for multiclass and survival
+// #nocov start (cannot be tested anymore because GenABEL not on CRAN)
+void Data::orderSnpLevels(bool corrected_importance) {
+  // Stop if now SNP data
+  if (snp_data == 0) {
+    return;
+  }
+
+  size_t num_snps;
+  if (corrected_importance) {
+    num_snps = 2 * (num_cols - num_cols_no_snp);
+  } else {
+    num_snps = num_cols - num_cols_no_snp;
+  }
+
+  // Reserve space
+  snp_order.resize(num_snps, std::vector<size_t>(3));
+
+  // For each SNP
+  for (size_t i = 0; i < num_snps; ++i) {
+    size_t col = i;
+    if (i >= (num_cols - num_cols_no_snp)) {
+      // Get unpermuted SNP ID
+      col = i - num_cols + num_cols_no_snp;
+    }
+
+    // Order by mean response
+    std::vector<double> means(3, 0);
+    std::vector<double> counts(3, 0);
+    for (size_t row = 0; row < num_rows; ++row) {
+      size_t row_permuted = row;
+      if (i >= (num_cols - num_cols_no_snp)) {
+        row_permuted = getPermutedSampleID(row);
+      }
+      size_t idx = col * num_rows_rounded + row_permuted;
+      size_t value = (((snp_data[idx / 4] & mask[idx % 4]) >> offset[idx % 4]) - 1);
+
+      // TODO: Better way to treat missing values?
+      if (value > 2) {
+        value = 0;
+      }
+
+      means[value] += get_y(row, 0);
+      ++counts[value];
+    }
+
+    for (size_t value = 0; value < 3; ++value) {
+      means[value] /= counts[value];
+    }
+
+    // Save order
+    snp_order[i] = order(means, false);
+  }
+
+  order_snps = true;
+}
+// #nocov end
+
+} // namespace ranger
+
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/Data.h
@@ -0,0 +1,228 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#ifndef DATA_H_
+#define DATA_H_
+
+#include <vector>
+#include <iostream>
+#include <numeric>
+#include <random>
+#include <algorithm>
+
+#include "globals.h"
+
+namespace ranger {
+
+class Data {
+public:
+  Data();
+
+  Data(const Data&) = delete;
+  Data& operator=(const Data&) = delete;
+
+  virtual ~Data() = default;
+
+  virtual double get_x(size_t row, size_t col) const = 0;
+  virtual double get_y(size_t row, size_t col) const = 0;
+
+  size_t getVariableID(const std::string& variable_name) const;
+
+  virtual void reserveMemory(size_t y_cols) = 0;
+
+  virtual void set_x(size_t col, size_t row, double value, bool& error) = 0;
+  virtual void set_y(size_t col, size_t row, double value, bool& error) = 0;
+
+  void addSnpData(unsigned char* snp_data, size_t num_cols_snp);
+
+  bool loadFromFile(std::string filename, std::vector<std::string>& dependent_variable_names);
+  bool loadFromFileWhitespace(std::ifstream& input_file, std::string header_line,
+      std::vector<std::string>& dependent_variable_names);
+  bool loadFromFileOther(std::ifstream& input_file, std::string header_line,
+      std::vector<std::string>& dependent_variable_names, char seperator);
+
+  void getAllValues(std::vector<double>& all_values, std::vector<size_t>& sampleIDs, size_t varID, size_t start,
+      size_t end) const;
+
+  void getMinMaxValues(double& min, double&max, std::vector<size_t>& sampleIDs, size_t varID, size_t start,
+      size_t end) const;
+
+  size_t getIndex(size_t row, size_t col) const {
+    // Use permuted data for corrected impurity importance
+    size_t col_permuted = col;
+    if (col >= num_cols) {
+      col = getUnpermutedVarID(col);
+      row = getPermutedSampleID(row);
+    }
+
+    if (col < num_cols_no_snp) {
+      return index_data[col * num_rows + row];
+    } else {
+      return getSnp(row, col, col_permuted);
+    }
+  }
+
+  // #nocov start (cannot be tested anymore because GenABEL not on CRAN)
+  size_t getSnp(size_t row, size_t col, size_t col_permuted) const {
+    // Get data out of snp storage. -1 because of GenABEL coding.
+    size_t idx = (col - num_cols_no_snp) * num_rows_rounded + row;
+    size_t result = ((snp_data[idx / 4] & mask[idx % 4]) >> offset[idx % 4]) - 1;
+
+    // TODO: Better way to treat missing values?
+    if (result > 2) {
+      result = 0;
+    }
+
+    // Order SNPs
+    if (order_snps) {
+      if (col_permuted >= num_cols) {
+        result = snp_order[col_permuted - 2 * num_cols_no_snp][result];
+      } else {
+        result = snp_order[col - num_cols_no_snp][result];
+      }
+    }
+    return result;
+  }
+  // #nocov end
+
+  double getUniqueDataValue(size_t varID, size_t index) const {
+    // Use permuted data for corrected impurity importance
+    if (varID >= num_cols) {
+      varID = getUnpermutedVarID(varID);
+    }
+
+    if (varID < num_cols_no_snp) {
+      return unique_data_values[varID][index];
+    } else {
+      // For GWAS data the index is the value
+      return (index);
+    }
+  }
+
+  size_t getNumUniqueDataValues(size_t varID) const {
+    // Use permuted data for corrected impurity importance
+    if (varID >= num_cols) {
+      varID = getUnpermutedVarID(varID);
+    }
+
+    if (varID < num_cols_no_snp) {
+      return unique_data_values[varID].size();
+    } else {
+      // For GWAS data 0,1,2
+      return (3);
+    }
+  }
+
+  void sort();
+
+  void orderSnpLevels(bool corrected_importance);
+
+  const std::vector<std::string>& getVariableNames() const {
+    return variable_names;
+  }
+  size_t getNumCols() const {
+    return num_cols;
+  }
+  size_t getNumRows() const {
+    return num_rows;
+  }
+
+  size_t getMaxNumUniqueValues() const {
+    if (snp_data == 0 || max_num_unique_values > 3) {
+      // If no snp data or one variable with more than 3 unique values, return that value
+      return max_num_unique_values;
+    } else {
+      // If snp data and no variable with more than 3 unique values, return 3
+      return 3;
+    }
+  }
+
+  std::vector<bool>& getIsOrderedVariable() noexcept {
+    return is_ordered_variable;
+  }
+
+  void setIsOrderedVariable(const std::vector<std::string>& unordered_variable_names) {
+    is_ordered_variable.resize(num_cols, true);
+    for (auto& variable_name : unordered_variable_names) {
+      size_t varID = getVariableID(variable_name);
+      is_ordered_variable[varID] = false;
+    }
+  }
+
+  void setIsOrderedVariable(std::vector<bool>& is_ordered_variable) {
+    this->is_ordered_variable = is_ordered_variable;
+  }
+
+  bool isOrderedVariable(size_t varID) const {
+    // Use permuted data for corrected impurity importance
+    if (varID >= num_cols) {
+      varID = getUnpermutedVarID(varID);
+    }
+    return is_ordered_variable[varID];
+  }
+
+  void permuteSampleIDs(std::mt19937_64 random_number_generator) {
+    permuted_sampleIDs.resize(num_rows);
+    std::iota(permuted_sampleIDs.begin(), permuted_sampleIDs.end(), 0);
+    std::shuffle(permuted_sampleIDs.begin(), permuted_sampleIDs.end(), random_number_generator);
+  }
+
+  size_t getPermutedSampleID(size_t sampleID) const {
+    return permuted_sampleIDs[sampleID];
+  }
+
+  size_t getUnpermutedVarID(size_t varID) const {
+    if (varID >= num_cols) {
+      varID -= num_cols;
+    }
+    return varID;
+  }
+
+  // #nocov start (cannot be tested anymore because GenABEL not on CRAN)
+  const std::vector<std::vector<size_t>>& getSnpOrder() const {
+    return snp_order;
+  }
+
+  void setSnpOrder(std::vector<std::vector<size_t>>& snp_order) {
+    this->snp_order = snp_order;
+    order_snps = true;
+  }
+  // #nocov end
+
+protected:
+  std::vector<std::string> variable_names;
+  size_t num_rows;
+  size_t num_rows_rounded;
+  size_t num_cols;
+
+  unsigned char* snp_data;
+  size_t num_cols_no_snp;
+
+  bool externalData;
+
+  std::vector<size_t> index_data;
+  std::vector<std::vector<double>> unique_data_values;
+  size_t max_num_unique_values;
+
+  // For each varID true if ordered
+  std::vector<bool> is_ordered_variable;
+
+  // Permuted samples for corrected impurity importance
+  std::vector<size_t> permuted_sampleIDs;
+
+  // Order of 0/1/2 for ordered splitting
+  std::vector<std::vector<size_t>> snp_order;
+  bool order_snps;
+};
+
+} // namespace ranger
+
+#endif /* DATA_H_ */
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/DataChar.h
@@ -0,0 +1,75 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+// Ignore in coverage report (not used in R package)
+// #nocov start
+#ifndef DATACHAR_H_
+#define DATACHAR_H_
+
+#include <vector>
+#include <utility>
+
+#include "globals.h"
+#include "utility.h"
+#include "Data.h"
+
+namespace ranger {
+
+class DataChar: public Data {
+public:
+  DataChar() = default;
+
+  DataChar(const DataChar&) = delete;
+  DataChar& operator=(const DataChar&) = delete;
+
+  virtual ~DataChar() override = default;
+
+  double get_x(size_t row, size_t col) const override {
+    // Use permuted data for corrected impurity importance
+    size_t col_permuted = col;
+    if (col >= num_cols) {
+      col = getUnpermutedVarID(col);
+      row = getPermutedSampleID(row);
+    }
+
+    if (col < num_cols_no_snp) {
+      return x[col * num_rows + row];
+    } else {
+      return getSnp(row, col, col_permuted);
+    }
+  }
+
+  double get_y(size_t row, size_t col) const override {
+    return y[col * num_rows + row];
+  }
+
+  void reserveMemory(size_t y_cols) override {
+    x.resize(num_cols * num_rows);
+    y.resize(y_cols * num_rows);
+  }
+
+  void set_x(size_t col, size_t row, double value, bool& error) override {
+    x[col * num_rows + row] = value;
+  }
+
+  void set_y(size_t col, size_t row, double value, bool& error) override {
+    y[col * num_rows + row] = value;
+  }
+
+private:
+  std::vector<char> x;
+  std::vector<char> y;
+};
+
+} // namespace ranger
+
+#endif /* DATACHAR_H_ */
+// #nocov end
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/DataDouble.h
@@ -0,0 +1,75 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+// Ignore in coverage report (not used in R package)
+// #nocov start
+#ifndef DATADOUBLE_H_
+#define DATADOUBLE_H_
+
+#include <vector>
+#include <utility>
+
+#include "globals.h"
+#include "utility.h"
+#include "Data.h"
+
+namespace ranger {
+
+class DataDouble: public Data {
+public:
+  DataDouble() = default;
+  
+  DataDouble(const DataDouble&) = delete;
+  DataDouble& operator=(const DataDouble&) = delete;
+
+  virtual ~DataDouble() override = default;
+
+  double get_x(size_t row, size_t col) const override {
+    // Use permuted data for corrected impurity importance
+    size_t col_permuted = col;
+    if (col >= num_cols) {
+      col = getUnpermutedVarID(col);
+      row = getPermutedSampleID(row);
+    }
+
+    if (col < num_cols_no_snp) {
+      return x[col * num_rows + row];
+    } else {
+      return getSnp(row, col, col_permuted);
+    }
+  }
+
+  double get_y(size_t row, size_t col) const override {
+    return y[col * num_rows + row];
+  }
+
+  void reserveMemory(size_t y_cols) override {
+    x.resize(num_cols * num_rows);
+    y.resize(y_cols * num_rows);
+  }
+
+  void set_x(size_t col, size_t row, double value, bool& error) override {
+    x[col * num_rows + row] = value;
+  }
+
+  void set_y(size_t col, size_t row, double value, bool& error) override {
+    y[col * num_rows + row] = value;
+  }
+
+private:
+  std::vector<double> x;
+  std::vector<double> y;
+};
+
+} // namespace ranger
+
+#endif /* DATADOUBLE_H_ */
+// #nocov end
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/DataFloat.h
@@ -0,0 +1,76 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+// Ignore in coverage report (not used in R package)
+// #nocov start
+#ifndef DATAFLOAT_H_
+#define DATAFLOAT_H_
+
+#include <vector>
+#include <utility>
+
+#include "globals.h"
+#include "utility.h"
+#include "Data.h"
+
+namespace ranger {
+
+class DataFloat: public Data {
+public:
+  DataFloat() = default;
+
+  DataFloat(const DataFloat&) = delete;
+  DataFloat& operator=(const DataFloat&) = delete;
+
+  virtual ~DataFloat() override = default;
+
+  double get_x(size_t row, size_t col) const override {
+    // Use permuted data for corrected impurity importance
+    size_t col_permuted = col;
+    if (col >= num_cols) {
+      col = getUnpermutedVarID(col);
+      row = getPermutedSampleID(row);
+    }
+
+    if (col < num_cols_no_snp) {
+      return x[col * num_rows + row];
+    } else {
+      return getSnp(row, col, col_permuted);
+    }
+  }
+
+  double get_y(size_t row, size_t col) const override {
+    return y[col * num_rows + row];
+  }
+
+  void reserveMemory(size_t y_cols) override {
+    x.resize(num_cols * num_rows);
+    y.resize(y_cols * num_rows);
+  }
+
+  void set_x(size_t col, size_t row, double value, bool& error) override {
+    x[col * num_rows + row] = value;
+  }
+
+  void set_y(size_t col, size_t row, double value, bool& error) override {
+    y[col * num_rows + row] = value;
+  }
+
+private:
+  std::vector<float> x;
+  std::vector<float> y;
+};
+
+} // namespace ranger
+
+#endif /* DATAFLOAT_H_ */
+// #nocov end
+
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/DataSparse.cpp
@@ -0,0 +1,44 @@
+/*-------------------------------------------------------------------------------
+ This file is part of Ranger.
+
+ Ranger is free software: you can redistribute it and/or modify
+ it under the terms of the GNU General Public License as published by
+ the Free Software Foundation, either version 3 of the License, or
+ (at your option) any later version.
+
+ Ranger is distributed in the hope that it will be useful,
+ but WITHOUT ANY WARRANTY; without even the implied warranty of
+ MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ GNU General Public License for more details.
+
+ You should have received a copy of the GNU General Public License
+ along with Ranger. If not, see <http://www.gnu.org/licenses/>.
+
+ Written by:
+
+ Marvin N. Wright
+ Institut für Medizinische Biometrie und Statistik
+ Universität zu Lübeck
+ Ratzeburger Allee 160
+ 23562 Lübeck
+ Germany
+
+ http://www.imbs-luebeck.de
+ #-------------------------------------------------------------------------------*/
+
+#include "DataSparse.h"
+
+namespace ranger {
+
+DataSparse::DataSparse(Eigen::SparseMatrix<double>& x, Rcpp::NumericMatrix& y, std::vector<std::string> variable_names, size_t num_rows,
+    size_t num_cols) :
+    x { }{
+  this->x.swap(x);
+  this->y = y;
+  this->variable_names = variable_names;
+  this->num_rows = num_rows;
+  this->num_cols = num_cols;
+  this->num_cols_no_snp = num_cols;
+}
+
+} // namespace ranger
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/DataSparse.h
@@ -0,0 +1,85 @@
+/*-------------------------------------------------------------------------------
+ This file is part of Ranger.
+
+ Ranger is free software: you can redistribute it and/or modify
+ it under the terms of the GNU General Public License as published by
+ the Free Software Foundation, either version 3 of the License, or
+ (at your option) any later version.
+
+ Ranger is distributed in the hope that it will be useful,
+ but WITHOUT ANY WARRANTY; without even the implied warranty of
+ MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ GNU General Public License for more details.
+
+ You should have received a copy of the GNU General Public License
+ along with Ranger. If not, see <http://www.gnu.org/licenses/>.
+
+ Written by:
+
+ Marvin N. Wright
+ Institut für Medizinische Biometrie und Statistik
+ Universität zu Lübeck
+ Ratzeburger Allee 160
+ 23562 Lübeck
+
+ http://www.imbs-luebeck.de
+ #-------------------------------------------------------------------------------*/
+
+#ifndef DATASPARSE_H_
+#define DATASPARSE_H_
+
+#include <RcppEigen.h>
+
+#include "globals.h"
+#include "utility.h"
+#include "Data.h"
+
+namespace ranger {
+
+class DataSparse: public Data {
+public:
+  DataSparse() = default;
+  
+  DataSparse(Eigen::SparseMatrix<double>& x, Rcpp::NumericMatrix& y, std::vector<std::string> variable_names, size_t num_rows,
+      size_t num_cols);
+
+  DataSparse(const DataSparse&) = delete;
+  DataSparse& operator=(const DataSparse&) = delete;
+
+  virtual ~DataSparse() override = default;
+
+  double get_x(size_t row, size_t col) const override {
+    // Use permuted data for corrected impurity importance
+    if (col >= num_cols) {
+      col = getUnpermutedVarID(col);
+      row = getPermutedSampleID(row);
+    }
+    return x.coeff(row, col);
+  }
+  
+  double get_y(size_t row, size_t col) const override {
+    return y[col * num_rows + row];
+  }
+
+  // #nocov start 
+  void reserveMemory(size_t y_cols) override {
+    // Not needed
+  }
+
+  void set_x(size_t col, size_t row, double value, bool& error) override {
+    x.coeffRef(row, col) = value;
+  }
+  
+  void set_y(size_t col, size_t row, double value, bool& error) override {
+    y[col * num_rows + row] = value;
+  }
+  // #nocov end 
+
+private:
+  Eigen::SparseMatrix<double> x;
+  Rcpp::NumericMatrix y;
+};
+
+} // namespace ranger
+
+#endif /* DATASPARSE_H_ */
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/Forest.cpp
@@ -0,0 +1,1087 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#include <math.h>
+#include <algorithm>
+#include <stdexcept>
+#include <string>
+#include <ctime>
+#include <functional>
+#ifndef OLD_WIN_R_BUILD
+#include <thread>
+#include <chrono>
+#endif
+
+#include "utility.h"
+#include "Forest.h"
+#include "DataChar.h"
+#include "DataDouble.h"
+#include "DataFloat.h"
+
+namespace ranger {
+
+Forest::Forest() :
+    verbose_out(0), num_trees(DEFAULT_NUM_TREE), mtry(0), min_node_size(0), num_independent_variables(0), seed(0), num_samples(
+        0), prediction_mode(false), memory_mode(MEM_DOUBLE), sample_with_replacement(true), memory_saving_splitting(
+        false), splitrule(DEFAULT_SPLITRULE), predict_all(false), keep_inbag(false), sample_fraction( { 1 }), holdout(
+        false), prediction_type(DEFAULT_PREDICTIONTYPE), num_random_splits(DEFAULT_NUM_RANDOM_SPLITS), max_depth(
+        DEFAULT_MAXDEPTH), alpha(DEFAULT_ALPHA), minprop(DEFAULT_MINPROP), num_threads(DEFAULT_NUM_THREADS), data { }, overall_prediction_error(
+    NAN), importance_mode(DEFAULT_IMPORTANCE_MODE), regularization_usedepth(false), progress(0) {
+}
+
+// #nocov start
+void Forest::initCpp(std::string dependent_variable_name, MemoryMode memory_mode, std::string input_file, uint mtry,
+    std::string output_prefix, uint num_trees, std::ostream* verbose_out, uint seed, uint num_threads,
+    std::string load_forest_filename, ImportanceMode importance_mode, uint min_node_size,
+    std::string split_select_weights_file, const std::vector<std::string>& always_split_variable_names,
+    std::string status_variable_name, bool sample_with_replacement,
+    const std::vector<std::string>& unordered_variable_names, bool memory_saving_splitting, SplitRule splitrule,
+    std::string case_weights_file, bool predict_all, double sample_fraction, double alpha, double minprop, bool holdout,
+    PredictionType prediction_type, uint num_random_splits, uint max_depth, const std::vector<double>& regularization_factor,
+    bool regularization_usedepth) {
+
+  this->verbose_out = verbose_out;
+
+  if (!dependent_variable_name.empty()) {
+    if (status_variable_name.empty()) {
+      this->dependent_variable_names = {dependent_variable_name};
+    } else {
+      this->dependent_variable_names = {dependent_variable_name, status_variable_name};
+    }
+  }
+
+  // Set prediction mode
+  bool prediction_mode = false;
+  if (!load_forest_filename.empty()) {
+    prediction_mode = true;
+  }
+
+  // Sample fraction default and convert to vector
+  if (sample_fraction == 0) {
+    if (sample_with_replacement) {
+      sample_fraction = DEFAULT_SAMPLE_FRACTION_REPLACE;
+    } else {
+      sample_fraction = DEFAULT_SAMPLE_FRACTION_NOREPLACE;
+    }
+  }
+  std::vector<double> sample_fraction_vector = { sample_fraction };
+
+  if (prediction_mode) {
+    loadDependentVariableNamesFromFile(load_forest_filename);
+  }
+
+  // Call other init function
+  init(memory_mode, loadDataFromFile(input_file), mtry, output_prefix, num_trees, seed, num_threads, importance_mode,
+      min_node_size, prediction_mode, sample_with_replacement, unordered_variable_names, memory_saving_splitting,
+      splitrule, predict_all, sample_fraction_vector, alpha, minprop, holdout, prediction_type, num_random_splits,
+      false, max_depth, regularization_factor, regularization_usedepth);
+
+  if (prediction_mode) {
+    loadFromFile(load_forest_filename);
+  }
+  // Set variables to be always considered for splitting
+  if (!always_split_variable_names.empty()) {
+    setAlwaysSplitVariables(always_split_variable_names);
+  }
+
+  // TODO: Read 2d weights for tree-wise split select weights
+  // Load split select weights from file
+  if (!split_select_weights_file.empty()) {
+    std::vector<std::vector<double>> split_select_weights;
+    split_select_weights.resize(1);
+    loadDoubleVectorFromFile(split_select_weights[0], split_select_weights_file);
+    if (split_select_weights[0].size() != num_independent_variables) {
+      throw std::runtime_error("Number of split select weights is not equal to number of independent variables.");
+    }
+    setSplitWeightVector(split_select_weights);
+  }
+
+  // Load case weights from file
+  if (!case_weights_file.empty()) {
+    loadDoubleVectorFromFile(case_weights, case_weights_file);
+    if (case_weights.size() != num_samples) {
+      throw std::runtime_error("Number of case weights is not equal to number of samples.");
+    }
+  }
+
+  // Sample from non-zero weights in holdout mode
+  if (holdout && !case_weights.empty()) {
+    size_t nonzero_weights = 0;
+    for (auto& weight : case_weights) {
+      if (weight > 0) {
+        ++nonzero_weights;
+      }
+    }
+    this->sample_fraction[0] = this->sample_fraction[0] * ((double) nonzero_weights / (double) num_samples);
+  }
+
+  // Check if all catvars are coded in integers starting at 1
+  if (!unordered_variable_names.empty()) {
+    std::string error_message = checkUnorderedVariables(*data, unordered_variable_names);
+    if (!error_message.empty()) {
+      throw std::runtime_error(error_message);
+    }
+  }
+}
+// #nocov end
+
+void Forest::initR(std::unique_ptr<Data> input_data, uint mtry, uint num_trees, std::ostream* verbose_out, uint seed,
+    uint num_threads, ImportanceMode importance_mode, uint min_node_size,
+    std::vector<std::vector<double>>& split_select_weights, const std::vector<std::string>& always_split_variable_names,
+    bool prediction_mode, bool sample_with_replacement, const std::vector<std::string>& unordered_variable_names,
+    bool memory_saving_splitting, SplitRule splitrule, std::vector<double>& case_weights,
+    std::vector<std::vector<size_t>>& manual_inbag, bool predict_all, bool keep_inbag,
+    std::vector<double>& sample_fraction, double alpha, double minprop, bool holdout, PredictionType prediction_type,
+    uint num_random_splits, bool order_snps, uint max_depth, const std::vector<double>& regularization_factor, bool regularization_usedepth) {
+
+  this->verbose_out = verbose_out;
+
+  // Call other init function
+  init(MEM_DOUBLE, std::move(input_data), mtry, "", num_trees, seed, num_threads, importance_mode, min_node_size,
+      prediction_mode, sample_with_replacement, unordered_variable_names, memory_saving_splitting, splitrule,
+      predict_all, sample_fraction, alpha, minprop, holdout, prediction_type, num_random_splits, order_snps, max_depth,
+      regularization_factor, regularization_usedepth);
+
+  // Set variables to be always considered for splitting
+  if (!always_split_variable_names.empty()) {
+    setAlwaysSplitVariables(always_split_variable_names);
+  }
+
+  // Set split select weights
+  if (!split_select_weights.empty()) {
+    setSplitWeightVector(split_select_weights);
+  }
+
+  // Set case weights
+  if (!case_weights.empty()) {
+    if (case_weights.size() != num_samples) {
+      throw std::runtime_error("Number of case weights not equal to number of samples.");
+    }
+    this->case_weights = case_weights;
+  }
+
+  // Set manual inbag
+  if (!manual_inbag.empty()) {
+    this->manual_inbag = manual_inbag;
+  }
+
+  // Keep inbag counts
+  this->keep_inbag = keep_inbag;
+}
+
+void Forest::init(MemoryMode memory_mode, std::unique_ptr<Data> input_data, uint mtry, std::string output_prefix,
+    uint num_trees, uint seed, uint num_threads, ImportanceMode importance_mode, uint min_node_size,
+    bool prediction_mode, bool sample_with_replacement, const std::vector<std::string>& unordered_variable_names,
+    bool memory_saving_splitting, SplitRule splitrule, bool predict_all, std::vector<double>& sample_fraction,
+    double alpha, double minprop, bool holdout, PredictionType prediction_type, uint num_random_splits, bool order_snps,
+    uint max_depth, const std::vector<double>& regularization_factor, bool regularization_usedepth) {
+
+  // Initialize data with memmode
+  this->data = std::move(input_data);
+
+  // Initialize random number generator and set seed
+  if (seed == 0) {
+    std::random_device random_device;
+    random_number_generator.seed(random_device());
+  } else {
+    random_number_generator.seed(seed);
+  }
+
+  // Set number of threads
+  if (num_threads == DEFAULT_NUM_THREADS) {
+#ifdef OLD_WIN_R_BUILD
+    this->num_threads = 1;
+#else
+    this->num_threads = std::thread::hardware_concurrency();
+#endif
+  } else {
+    this->num_threads = num_threads;
+  }
+
+  // Set member variables
+  this->num_trees = num_trees;
+  this->mtry = mtry;
+  this->seed = seed;
+  this->output_prefix = output_prefix;
+  this->importance_mode = importance_mode;
+  this->min_node_size = min_node_size;
+  this->memory_mode = memory_mode;
+  this->prediction_mode = prediction_mode;
+  this->sample_with_replacement = sample_with_replacement;
+  this->memory_saving_splitting = memory_saving_splitting;
+  this->splitrule = splitrule;
+  this->predict_all = predict_all;
+  this->sample_fraction = sample_fraction;
+  this->holdout = holdout;
+  this->alpha = alpha;
+  this->minprop = minprop;
+  this->prediction_type = prediction_type;
+  this->num_random_splits = num_random_splits;
+  this->max_depth = max_depth;
+  this->regularization_factor = regularization_factor;
+  this->regularization_usedepth = regularization_usedepth;
+
+  // Set number of samples and variables
+  num_samples = data->getNumRows();
+  num_independent_variables = data->getNumCols();
+
+  // Set unordered factor variables
+  if (!prediction_mode) {
+    data->setIsOrderedVariable(unordered_variable_names);
+  }
+
+  initInternal();
+
+  // Init split select weights
+  split_select_weights.push_back(std::vector<double>());
+
+  // Init manual inbag
+  manual_inbag.push_back(std::vector<size_t>());
+
+  // Check if mtry is in valid range
+  if (this->mtry > num_independent_variables) {
+    throw std::runtime_error("mtry can not be larger than number of variables in data.");
+  }
+
+  // Check if any observations samples
+  if ((size_t) num_samples * sample_fraction[0] < 1) {
+    throw std::runtime_error("sample_fraction too small, no observations sampled.");
+  }
+
+  // Permute samples for corrected Gini importance
+  if (importance_mode == IMP_GINI_CORRECTED) {
+    data->permuteSampleIDs(random_number_generator);
+  }
+
+  // Order SNP levels if in "order" splitting
+  if (!prediction_mode && order_snps) {
+    data->orderSnpLevels((importance_mode == IMP_GINI_CORRECTED));
+  }
+
+  // Regularization
+  if (regularization_factor.size() > 0) {
+    if (regularization_factor.size() == 1 && num_independent_variables > 1) {
+      double single_regularization_factor = regularization_factor[0];
+      this->regularization_factor.resize(num_independent_variables, single_regularization_factor);
+    } else if (regularization_factor.size() != num_independent_variables) {
+      throw std::runtime_error("Use 1 or p (the number of predictor variables) regularization factors.");
+    }
+
+    // Set all variables to not used
+    split_varIDs_used.resize(num_independent_variables, false);
+  }
+}
+
+void Forest::run(bool verbose, bool compute_oob_error) {
+
+  if (prediction_mode) {
+    if (verbose && verbose_out) {
+      *verbose_out << "Predicting .." << std::endl;
+    }
+    predict();
+  } else {
+    if (verbose && verbose_out) {
+      *verbose_out << "Growing trees .." << std::endl;
+    }
+
+    grow();
+
+    if (verbose && verbose_out) {
+      *verbose_out << "Computing prediction error .." << std::endl;
+    }
+
+    if (compute_oob_error) {
+      computePredictionError();
+    }
+
+    if (importance_mode == IMP_PERM_BREIMAN || importance_mode == IMP_PERM_LIAW ||
+        importance_mode == IMP_PERM_RAW || importance_mode == IMP_PERM_CASEWISE) {
+      if (verbose && verbose_out) {
+        *verbose_out << "Computing permutation variable importance .." << std::endl;
+      }
+      computePermutationImportance();
+    }
+  }
+}
+
+// #nocov start
+void Forest::writeOutput() {
+
+  if (verbose_out)
+    *verbose_out << std::endl;
+  writeOutputInternal();
+  if (verbose_out) {
+    if (dependent_variable_names.size() >= 1) {
+      *verbose_out << "Dependent variable name:           " << dependent_variable_names[0] << std::endl;
+    }
+    *verbose_out << "Number of trees:                   " << num_trees << std::endl;
+    *verbose_out << "Sample size:                       " << num_samples << std::endl;
+    *verbose_out << "Number of independent variables:   " << num_independent_variables << std::endl;
+    *verbose_out << "Mtry:                              " << mtry << std::endl;
+    *verbose_out << "Target node size:                  " << min_node_size << std::endl;
+    *verbose_out << "Variable importance mode:          " << importance_mode << std::endl;
+    *verbose_out << "Memory mode:                       " << memory_mode << std::endl;
+    *verbose_out << "Seed:                              " << seed << std::endl;
+    *verbose_out << "Number of threads:                 " << num_threads << std::endl;
+    *verbose_out << std::endl;
+  }
+
+  if (prediction_mode) {
+    writePredictionFile();
+  } else {
+    if (verbose_out) {
+      *verbose_out << "Overall OOB prediction error:      " << overall_prediction_error << std::endl;
+      *verbose_out << std::endl;
+    }
+
+    if (!split_select_weights.empty() & !split_select_weights[0].empty()) {
+      if (verbose_out) {
+        *verbose_out
+            << "Warning: Split select weights used. Variable importance measures are only comparable for variables with equal weights."
+            << std::endl;
+      }
+    }
+
+    if (importance_mode != IMP_NONE) {
+      writeImportanceFile();
+    }
+
+    writeConfusionFile();
+  }
+}
+
+void Forest::writeImportanceFile() {
+
+  // Open importance file for writing
+  std::string filename = output_prefix + ".importance";
+  std::ofstream importance_file;
+  importance_file.open(filename, std::ios::out);
+  if (!importance_file.good()) {
+    throw std::runtime_error("Could not write to importance file: " + filename + ".");
+  }
+
+  if (importance_mode == IMP_PERM_CASEWISE) {
+    // Write variable names
+    for (auto& variable_name : data->getVariableNames()) {
+      importance_file << variable_name << " ";
+    }
+    importance_file << std::endl;
+
+    // Write importance values
+    for (size_t i = 0; i < num_samples; ++i) {
+      for (size_t j = 0; j < num_independent_variables; ++j) {
+        if (variable_importance_casewise.size() <= (j * num_samples + i)) {
+          throw std::runtime_error("Memory error in local variable importance.");
+        }
+        importance_file << variable_importance_casewise[j * num_samples + i] << " ";
+      }
+      importance_file << std::endl;
+    }
+  } else {
+    // Write importance to file
+    for (size_t i = 0; i < variable_importance.size(); ++i) {
+      std::string variable_name = data->getVariableNames()[i];
+      importance_file << variable_name << ": " << variable_importance[i] << std::endl;
+    }
+  }
+
+  importance_file.close();
+  if (verbose_out)
+    *verbose_out << "Saved variable importance to file " << filename << "." << std::endl;
+}
+
+void Forest::saveToFile() {
+
+  // Open file for writing
+  std::string filename = output_prefix + ".forest";
+  std::ofstream outfile;
+  outfile.open(filename, std::ios::binary);
+  if (!outfile.good()) {
+    throw std::runtime_error("Could not write to output file: " + filename + ".");
+  }
+
+  // Write dependent variable names
+  uint num_dependent_variables = dependent_variable_names.size();
+  if (num_dependent_variables >= 1) {
+    outfile.write((char*) &num_dependent_variables, sizeof(num_dependent_variables));
+    for (auto& var_name : dependent_variable_names) {
+      size_t length = var_name.size();
+      outfile.write((char*) &length, sizeof(length));
+      outfile.write((char*) var_name.c_str(), length * sizeof(char));
+    }
+  } else {
+    throw std::runtime_error("Missing dependent variable name.");
+  }
+
+  // Write num_trees
+  outfile.write((char*) &num_trees, sizeof(num_trees));
+
+  // Write is_ordered_variable
+  saveVector1D(data->getIsOrderedVariable(), outfile);
+
+  saveToFileInternal(outfile);
+
+  // Write tree data for each tree
+  for (auto& tree : trees) {
+    tree->appendToFile(outfile);
+  }
+
+  // Close file
+  outfile.close();
+  if (verbose_out)
+    *verbose_out << "Saved forest to file " << filename << "." << std::endl;
+}
+// #nocov end
+
+void Forest::grow() {
+
+  // Create thread ranges
+  equalSplit(thread_ranges, 0, num_trees - 1, num_threads);
+
+  // Call special grow functions of subclasses. There trees must be created.
+  growInternal();
+
+  // Init trees, create a seed for each tree, based on main seed
+  std::uniform_int_distribution<uint> udist;
+  for (size_t i = 0; i < num_trees; ++i) {
+    uint tree_seed;
+    if (seed == 0) {
+      tree_seed = udist(random_number_generator);
+    } else {
+      tree_seed = (i + 1) * seed;
+    }
+
+    // Get split select weights for tree
+    std::vector<double>* tree_split_select_weights;
+    if (split_select_weights.size() > 1) {
+      tree_split_select_weights = &split_select_weights[i];
+    } else {
+      tree_split_select_weights = &split_select_weights[0];
+    }
+
+    // Get inbag counts for tree
+    std::vector<size_t>* tree_manual_inbag;
+    if (manual_inbag.size() > 1) {
+      tree_manual_inbag = &manual_inbag[i];
+    } else {
+      tree_manual_inbag = &manual_inbag[0];
+    }
+
+    trees[i]->init(data.get(), mtry, num_samples, tree_seed, &deterministic_varIDs,
+        tree_split_select_weights, importance_mode, min_node_size, sample_with_replacement, memory_saving_splitting,
+        splitrule, &case_weights, tree_manual_inbag, keep_inbag, &sample_fraction, alpha, minprop, holdout,
+        num_random_splits, max_depth, &regularization_factor, regularization_usedepth, &split_varIDs_used);
+  }
+
+// Init variable importance
+  variable_importance.resize(num_independent_variables, 0);
+
+// Grow trees in multiple threads
+#ifdef OLD_WIN_R_BUILD
+  // #nocov start
+  progress = 0;
+  clock_t start_time = clock();
+  clock_t lap_time = clock();
+  for (size_t i = 0; i < num_trees; ++i) {
+    trees[i]->grow(&variable_importance);
+    progress++;
+    showProgress("Growing trees..", start_time, lap_time);
+  }
+  // #nocov end
+#else
+  progress = 0;
+#ifdef R_BUILD
+  aborted = false;
+  aborted_threads = 0;
+#endif
+
+  std::vector<std::thread> threads;
+  threads.reserve(num_threads);
+
+// Initialize importance per thread
+  std::vector<std::vector<double>> variable_importance_threads(num_threads);
+
+  for (uint i = 0; i < num_threads; ++i) {
+    if (importance_mode == IMP_GINI || importance_mode == IMP_GINI_CORRECTED) {
+      variable_importance_threads[i].resize(num_independent_variables, 0);
+    }
+    threads.emplace_back(&Forest::growTreesInThread, this, i, &(variable_importance_threads[i]));
+  }
+  showProgress("Growing trees..", num_trees);
+  for (auto &thread : threads) {
+    thread.join();
+  }
+
+#ifdef R_BUILD
+  if (aborted_threads > 0) {
+    throw std::runtime_error("User interrupt.");
+  }
+#endif
+
+  // Sum thread importances
+  if (importance_mode == IMP_GINI || importance_mode == IMP_GINI_CORRECTED) {
+    variable_importance.resize(num_independent_variables, 0);
+    for (size_t i = 0; i < num_independent_variables; ++i) {
+      for (uint j = 0; j < num_threads; ++j) {
+        variable_importance[i] += variable_importance_threads[j][i];
+      }
+    }
+    variable_importance_threads.clear();
+  }
+
+#endif
+
+// Divide importance by number of trees
+  if (importance_mode == IMP_GINI || importance_mode == IMP_GINI_CORRECTED) {
+    for (auto& v : variable_importance) {
+      v /= num_trees;
+    }
+  }
+}
+
+void Forest::predict() {
+
+// Predict trees in multiple threads and join the threads with the main thread
+#ifdef OLD_WIN_R_BUILD
+  // #nocov start
+  progress = 0;
+  clock_t start_time = clock();
+  clock_t lap_time = clock();
+  for (size_t i = 0; i < num_trees; ++i) {
+    trees[i]->predict(data.get(), false);
+    progress++;
+    showProgress("Predicting..", start_time, lap_time);
+  }
+
+  // For all samples get tree predictions
+  allocatePredictMemory();
+  for (size_t sample_idx = 0; sample_idx < data->getNumRows(); ++sample_idx) {
+    predictInternal(sample_idx);
+  }
+  // #nocov end
+#else
+  progress = 0;
+#ifdef R_BUILD
+  aborted = false;
+  aborted_threads = 0;
+#endif
+
+  // Predict
+  std::vector<std::thread> threads;
+  threads.reserve(num_threads);
+  for (uint i = 0; i < num_threads; ++i) {
+    threads.emplace_back(&Forest::predictTreesInThread, this, i, data.get(), false);
+  }
+  showProgress("Predicting..", num_trees);
+  for (auto &thread : threads) {
+    thread.join();
+  }
+
+  // Aggregate predictions
+  allocatePredictMemory();
+  threads.clear();
+  threads.reserve(num_threads);
+  progress = 0;
+  for (uint i = 0; i < num_threads; ++i) {
+    threads.emplace_back(&Forest::predictInternalInThread, this, i);
+  }
+  showProgress("Aggregating predictions..", num_samples);
+  for (auto &thread : threads) {
+    thread.join();
+  }
+
+#ifdef R_BUILD
+  if (aborted_threads > 0) {
+    throw std::runtime_error("User interrupt.");
+  }
+#endif
+#endif
+}
+
+void Forest::computePredictionError() {
+
+// Predict trees in multiple threads
+#ifdef OLD_WIN_R_BUILD
+  // #nocov start
+  progress = 0;
+  clock_t start_time = clock();
+  clock_t lap_time = clock();
+  for (size_t i = 0; i < num_trees; ++i) {
+    trees[i]->predict(data.get(), true);
+    progress++;
+    showProgress("Predicting..", start_time, lap_time);
+  }
+  // #nocov end
+#else
+  std::vector<std::thread> threads;
+  threads.reserve(num_threads);
+  progress = 0;
+  for (uint i = 0; i < num_threads; ++i) {
+    threads.emplace_back(&Forest::predictTreesInThread, this, i, data.get(), true);
+  }
+  showProgress("Computing prediction error..", num_trees);
+  for (auto &thread : threads) {
+    thread.join();
+  }
+
+#ifdef R_BUILD
+  if (aborted_threads > 0) {
+    throw std::runtime_error("User interrupt.");
+  }
+#endif
+#endif
+
+  // Call special function for subclasses
+  computePredictionErrorInternal();
+}
+
+void Forest::computePermutationImportance() {
+
+// Compute tree permutation importance in multiple threads
+#ifdef OLD_WIN_R_BUILD
+  // #nocov start
+  progress = 0;
+  clock_t start_time = clock();
+  clock_t lap_time = clock();
+
+// Initialize importance and variance
+  variable_importance.resize(num_independent_variables, 0);
+  std::vector<double> variance;
+  if (importance_mode == IMP_PERM_BREIMAN || importance_mode == IMP_PERM_LIAW) {
+    variance.resize(num_independent_variables, 0);
+  }
+  if (importance_mode == IMP_PERM_CASEWISE) {
+    variable_importance_casewise.resize(num_independent_variables * num_samples, 0);
+  }
+
+// Compute importance
+  for (size_t i = 0; i < num_trees; ++i) {
+    trees[i]->computePermutationImportance(variable_importance, variance, variable_importance_casewise);
+    progress++;
+    showProgress("Computing permutation importance..", start_time, lap_time);
+  }
+
+#else
+  progress = 0;
+#ifdef R_BUILD
+  aborted = false;
+  aborted_threads = 0;
+#endif
+
+  std::vector<std::thread> threads;
+  threads.reserve(num_threads);
+
+// Initialize importance and variance
+  std::vector<std::vector<double>> variable_importance_threads(num_threads);
+  std::vector<std::vector<double>> variance_threads(num_threads);
+  std::vector<std::vector<double>> variable_importance_casewise_threads(num_threads);
+
+// Compute importance
+  for (uint i = 0; i < num_threads; ++i) {
+    variable_importance_threads[i].resize(num_independent_variables, 0);
+    if (importance_mode == IMP_PERM_BREIMAN || importance_mode == IMP_PERM_LIAW) {
+      variance_threads[i].resize(num_independent_variables, 0);
+    }
+    if (importance_mode == IMP_PERM_CASEWISE) {
+      variable_importance_casewise_threads[i].resize(num_independent_variables * num_samples, 0);
+    }
+    threads.emplace_back(
+          &Forest::computeTreePermutationImportanceInThread, this, i,
+          std::ref(variable_importance_threads[i]),
+          std::ref(variance_threads[i]),
+          std::ref(variable_importance_casewise_threads[i])
+        );
+  }
+  showProgress("Computing permutation importance..", num_trees);
+  for (auto &thread : threads) {
+    thread.join();
+  }
+
+#ifdef R_BUILD
+  if (aborted_threads > 0) {
+    throw std::runtime_error("User interrupt.");
+  }
+#endif
+
+// Sum thread importances
+  variable_importance.resize(num_independent_variables, 0);
+  for (size_t i = 0; i < num_independent_variables; ++i) {
+    for (uint j = 0; j < num_threads; ++j) {
+      variable_importance[i] += variable_importance_threads[j][i];
+    }
+  }
+  variable_importance_threads.clear();
+
+// Sum thread variances
+  std::vector<double> variance(num_independent_variables, 0);
+  if (importance_mode == IMP_PERM_BREIMAN || importance_mode == IMP_PERM_LIAW) {
+    for (size_t i = 0; i < num_independent_variables; ++i) {
+      for (uint j = 0; j < num_threads; ++j) {
+        variance[i] += variance_threads[j][i];
+      }
+    }
+    variance_threads.clear();
+  }
+
+// Sum thread casewise importances
+  if (importance_mode == IMP_PERM_CASEWISE) {
+    variable_importance_casewise.resize(num_independent_variables * num_samples, 0);
+    for (size_t i = 0; i < variable_importance_casewise.size(); ++i) {
+      for (uint j = 0; j < num_threads; ++j) {
+        variable_importance_casewise[i] += variable_importance_casewise_threads[j][i];
+      }
+    }
+    variable_importance_casewise_threads.clear();
+  }
+#endif
+
+  for (size_t i = 0; i < variable_importance.size(); ++i) {
+    variable_importance[i] /= num_trees;
+
+    // Normalize by variance for scaled permutation importance
+    if (importance_mode == IMP_PERM_BREIMAN || importance_mode == IMP_PERM_LIAW) {
+      if (variance[i] != 0) {
+        variance[i] = variance[i] / num_trees - variable_importance[i] * variable_importance[i];
+        variable_importance[i] /= sqrt(variance[i] / num_trees);
+      }
+    }
+  }
+
+  if (importance_mode == IMP_PERM_CASEWISE) {
+    for (size_t i = 0; i < variable_importance_casewise.size(); ++i) {
+      variable_importance_casewise[i] /= num_trees;
+    }
+  }
+}
+
+#ifndef OLD_WIN_R_BUILD
+void Forest::growTreesInThread(uint thread_idx, std::vector<double>* variable_importance) {
+  if (thread_ranges.size() > thread_idx + 1) {
+    for (size_t i = thread_ranges[thread_idx]; i < thread_ranges[thread_idx + 1]; ++i) {
+      trees[i]->grow(variable_importance);
+
+      // Check for user interrupt
+#ifdef R_BUILD
+      if (aborted) {
+        std::unique_lock<std::mutex> lock(mutex);
+        ++aborted_threads;
+        condition_variable.notify_one();
+        return;
+      }
+#endif
+
+      // Increase progress by 1 tree
+      std::unique_lock<std::mutex> lock(mutex);
+      ++progress;
+      condition_variable.notify_one();
+    }
+  }
+}
+
+void Forest::predictTreesInThread(uint thread_idx, const Data* prediction_data, bool oob_prediction) {
+  if (thread_ranges.size() > thread_idx + 1) {
+    for (size_t i = thread_ranges[thread_idx]; i < thread_ranges[thread_idx + 1]; ++i) {
+      trees[i]->predict(prediction_data, oob_prediction);
+
+      // Check for user interrupt
+#ifdef R_BUILD
+      if (aborted) {
+        std::unique_lock<std::mutex> lock(mutex);
+        ++aborted_threads;
+        condition_variable.notify_one();
+        return;
+      }
+#endif
+
+      // Increase progress by 1 tree
+      std::unique_lock<std::mutex> lock(mutex);
+      ++progress;
+      condition_variable.notify_one();
+    }
+  }
+}
+
+void Forest::predictInternalInThread(uint thread_idx) {
+  // Create thread ranges
+  std::vector<uint> predict_ranges;
+  equalSplit(predict_ranges, 0, num_samples - 1, num_threads);
+
+  if (predict_ranges.size() > thread_idx + 1) {
+    for (size_t i = predict_ranges[thread_idx]; i < predict_ranges[thread_idx + 1]; ++i) {
+      predictInternal(i);
+
+      // Check for user interrupt
+#ifdef R_BUILD
+      if (aborted) {
+        std::unique_lock<std::mutex> lock(mutex);
+        ++aborted_threads;
+        condition_variable.notify_one();
+        return;
+      }
+#endif
+
+      // Increase progress by 1 tree
+      std::unique_lock<std::mutex> lock(mutex);
+      ++progress;
+      condition_variable.notify_one();
+    }
+  }
+}
+
+void Forest::computeTreePermutationImportanceInThread(uint thread_idx, std::vector<double>& importance,
+    std::vector<double>& variance, std::vector<double>& importance_casewise) {
+  if (thread_ranges.size() > thread_idx + 1) {
+    for (size_t i = thread_ranges[thread_idx]; i < thread_ranges[thread_idx + 1]; ++i) {
+      trees[i]->computePermutationImportance(importance, variance, importance_casewise);
+
+      // Check for user interrupt
+#ifdef R_BUILD
+      if (aborted) {
+        std::unique_lock<std::mutex> lock(mutex);
+        ++aborted_threads;
+        condition_variable.notify_one();
+        return;
+      }
+#endif
+
+      // Increase progress by 1 tree
+      std::unique_lock<std::mutex> lock(mutex);
+      ++progress;
+      condition_variable.notify_one();
+    }
+  }
+}
+#endif
+
+// #nocov start
+void Forest::loadFromFile(std::string filename) {
+  if (verbose_out)
+    *verbose_out << "Loading forest from file " << filename << "." << std::endl;
+
+  // Open file for reading
+  std::ifstream infile;
+  infile.open(filename, std::ios::binary);
+  if (!infile.good()) {
+    throw std::runtime_error("Could not read from input file: " + filename + ".");
+  }
+
+  // Skip dependent variable names (already read)
+  uint num_dependent_variables;
+  infile.read((char*) &num_dependent_variables, sizeof(num_dependent_variables));
+  for (size_t i = 0; i < num_dependent_variables; ++i) {
+    size_t length;
+    infile.read((char*) &length, sizeof(size_t));
+    infile.ignore(length);
+  }
+
+  // Read num_trees
+  infile.read((char*) &num_trees, sizeof(num_trees));
+
+  // Read is_ordered_variable
+  readVector1D(data->getIsOrderedVariable(), infile);
+
+  // Read tree data. This is different for tree types -> virtual function
+  loadFromFileInternal(infile);
+
+  infile.close();
+
+  // Create thread ranges
+  equalSplit(thread_ranges, 0, num_trees - 1, num_threads);
+}
+
+void Forest::loadDependentVariableNamesFromFile(std::string filename) {
+
+  // Open file for reading
+  std::ifstream infile;
+  infile.open(filename, std::ios::binary);
+  if (!infile.good()) {
+    throw std::runtime_error("Could not read from input file: " + filename + ".");
+  }
+
+  // Read dependent variable names
+  uint num_dependent_variables = 0;
+  infile.read((char*) &num_dependent_variables, sizeof(num_dependent_variables));
+  for (size_t i = 0; i < num_dependent_variables; ++i) {
+    size_t length;
+    infile.read((char*) &length, sizeof(size_t));
+    char* temp = new char[length + 1];
+    infile.read((char*) temp, length * sizeof(char));
+    temp[length] = '\0';
+    dependent_variable_names.push_back(temp);
+    delete[] temp;
+  }
+
+  infile.close();
+}
+
+std::unique_ptr<Data> Forest::loadDataFromFile(const std::string& data_path) {
+  std::unique_ptr<Data> result { };
+  switch (memory_mode) {
+  case MEM_DOUBLE:
+    result = make_unique<DataDouble>();
+    break;
+  case MEM_FLOAT:
+    result = make_unique<DataFloat>();
+    break;
+  case MEM_CHAR:
+    result = make_unique<DataChar>();
+    break;
+  }
+
+  if (verbose_out)
+    *verbose_out << "Loading input file: " << data_path << "." << std::endl;
+  bool found_rounding_error = result->loadFromFile(data_path, dependent_variable_names);
+  if (found_rounding_error && verbose_out) {
+    *verbose_out << "Warning: Rounding or Integer overflow occurred. Use FLOAT or DOUBLE precision to avoid this."
+        << std::endl;
+  }
+  return result;
+}
+// #nocov end
+
+void Forest::setSplitWeightVector(std::vector<std::vector<double>>& split_select_weights) {
+
+// Size should be 1 x num_independent_variables or num_trees x num_independent_variables
+  if (split_select_weights.size() != 1 && split_select_weights.size() != num_trees) {
+    throw std::runtime_error("Size of split select weights not equal to 1 or number of trees.");
+  }
+
+// Reserve space
+  size_t num_weights = num_independent_variables;
+  if (importance_mode == IMP_GINI_CORRECTED) {
+    num_weights = 2 * num_independent_variables;
+  }
+  if (split_select_weights.size() == 1) {
+    this->split_select_weights[0].resize(num_weights);
+  } else {
+    this->split_select_weights.clear();
+    this->split_select_weights.resize(num_trees, std::vector<double>(num_weights));
+  }
+
+  // Split up in deterministic and weighted variables, ignore zero weights
+  for (size_t i = 0; i < split_select_weights.size(); ++i) {
+    size_t num_zero_weights = 0;
+
+    // Size should be 1 x num_independent_variables or num_trees x num_independent_variables
+    if (split_select_weights[i].size() != num_independent_variables) {
+      throw std::runtime_error("Number of split select weights not equal to number of independent variables.");
+    }
+
+    for (size_t j = 0; j < split_select_weights[i].size(); ++j) {
+      double weight = split_select_weights[i][j];
+
+      if (weight == 0) {
+        ++num_zero_weights;
+      } else if (weight < 0 || weight > 1) {
+        throw std::runtime_error("One or more split select weights not in range [0,1].");
+      } else {
+        this->split_select_weights[i][j] = weight;
+      }
+    }
+
+    // Copy weights for corrected impurity importance
+    if (importance_mode == IMP_GINI_CORRECTED) {
+      std::vector<double>* sw = &(this->split_select_weights[i]);
+      std::copy_n(sw->begin(), num_independent_variables, sw->begin() + num_independent_variables);
+    }
+
+    if (num_weights - num_zero_weights < mtry) {
+      throw std::runtime_error(
+          "Too many zeros in split select weights. Need at least mtry variables to split at.");
+    }
+  }
+}
+
+void Forest::setAlwaysSplitVariables(const std::vector<std::string>& always_split_variable_names) {
+
+  deterministic_varIDs.reserve(num_independent_variables);
+
+  for (auto& variable_name : always_split_variable_names) {
+    size_t varID = data->getVariableID(variable_name);
+    deterministic_varIDs.push_back(varID);
+  }
+
+  if (deterministic_varIDs.size() + this->mtry > num_independent_variables) {
+    throw std::runtime_error(
+        "Number of variables to be always considered for splitting plus mtry cannot be larger than number of independent variables.");
+  }
+
+  // Also add variables for corrected impurity importance
+  if (importance_mode == IMP_GINI_CORRECTED) {
+    size_t num_deterministic_varIDs = deterministic_varIDs.size();
+    for (size_t k = 0; k < num_deterministic_varIDs; ++k) {
+      deterministic_varIDs.push_back(k + num_independent_variables);
+    }
+  }
+}
+
+#ifdef OLD_WIN_R_BUILD
+// #nocov start
+void Forest::showProgress(std::string operation, clock_t start_time, clock_t& lap_time) {
+
+// Check for user interrupt
+  if (checkInterrupt()) {
+    throw std::runtime_error("User interrupt.");
+  }
+
+  double elapsed_time = (clock() - lap_time) / CLOCKS_PER_SEC;
+  if (elapsed_time > STATUS_INTERVAL) {
+    double relative_progress = (double) progress / (double) num_trees;
+    double time_from_start = (clock() - start_time) / CLOCKS_PER_SEC;
+    uint remaining_time = (1 / relative_progress - 1) * time_from_start;
+    if (verbose_out) {
+      *verbose_out << operation << " Progress: " << round(100 * relative_progress)
+      << "%. Estimated remaining time: " << beautifyTime(remaining_time) << "." << std::endl;
+    }
+    lap_time = clock();
+  }
+}
+// #nocov end
+#else
+void Forest::showProgress(std::string operation, size_t max_progress) {
+  using std::chrono::steady_clock;
+  using std::chrono::duration_cast;
+  using std::chrono::seconds;
+
+  steady_clock::time_point start_time = steady_clock::now();
+  steady_clock::time_point last_time = steady_clock::now();
+  std::unique_lock<std::mutex> lock(mutex);
+
+// Wait for message from threads and show output if enough time elapsed
+  while (progress < max_progress) {
+    condition_variable.wait(lock);
+    seconds elapsed_time = duration_cast<seconds>(steady_clock::now() - last_time);
+
+    // Check for user interrupt
+#ifdef R_BUILD
+    if (!aborted && checkInterrupt()) {
+      aborted = true;
+    }
+    if (aborted && aborted_threads >= num_threads) {
+      return;
+    }
+#endif
+
+    if (progress > 0 && elapsed_time.count() > STATUS_INTERVAL) {
+      double relative_progress = (double) progress / (double) max_progress;
+      seconds time_from_start = duration_cast<seconds>(steady_clock::now() - start_time);
+      uint remaining_time = (1 / relative_progress - 1) * time_from_start.count();
+      if (verbose_out) {
+        *verbose_out << operation << " Progress: " << round(100 * relative_progress) << "%. Estimated remaining time: "
+            << beautifyTime(remaining_time) << "." << std::endl;
+      }
+      last_time = steady_clock::now();
+    }
+  }
+}
+#endif
+
+} // namespace ranger
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/Forest.h
@@ -0,0 +1,264 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#ifndef FOREST_H_
+#define FOREST_H_
+
+#include <vector>
+#include <iostream>
+#include <random>
+#include <ctime>
+#include <memory>
+#ifndef OLD_WIN_R_BUILD
+#include <thread>
+#include <chrono>
+#include <mutex>
+#include <condition_variable>
+#endif
+
+#include "globals.h"
+#include "Tree.h"
+#include "Data.h"
+
+namespace ranger {
+
+class Forest {
+public:
+  Forest();
+
+  Forest(const Forest&) = delete;
+  Forest& operator=(const Forest&) = delete;
+
+  virtual ~Forest() = default;
+
+  // Init from c++ main or Rcpp from R
+  void initCpp(std::string dependent_variable_name, MemoryMode memory_mode, std::string input_file, uint mtry,
+      std::string output_prefix, uint num_trees, std::ostream* verbose_out, uint seed, uint num_threads,
+      std::string load_forest_filename, ImportanceMode importance_mode, uint min_node_size,
+      std::string split_select_weights_file, const std::vector<std::string>& always_split_variable_names,
+      std::string status_variable_name, bool sample_with_replacement,
+      const std::vector<std::string>& unordered_variable_names, bool memory_saving_splitting, SplitRule splitrule,
+      std::string case_weights_file, bool predict_all, double sample_fraction, double alpha, double minprop,
+      bool holdout, PredictionType prediction_type, uint num_random_splits, uint max_depth,
+      const std::vector<double>& regularization_factor, bool regularization_usedepth);
+  void initR(std::unique_ptr<Data> input_data, uint mtry, uint num_trees, std::ostream* verbose_out, uint seed,
+      uint num_threads, ImportanceMode importance_mode, uint min_node_size,
+      std::vector<std::vector<double>>& split_select_weights,
+      const std::vector<std::string>& always_split_variable_names, bool prediction_mode, bool sample_with_replacement,
+      const std::vector<std::string>& unordered_variable_names, bool memory_saving_splitting, SplitRule splitrule,
+      std::vector<double>& case_weights, std::vector<std::vector<size_t>>& manual_inbag, bool predict_all,
+      bool keep_inbag, std::vector<double>& sample_fraction, double alpha, double minprop, bool holdout,
+      PredictionType prediction_type, uint num_random_splits, bool order_snps, uint max_depth,
+      const std::vector<double>& regularization_factor, bool regularization_usedepth);
+  void init(MemoryMode memory_mode, std::unique_ptr<Data> input_data, uint mtry, std::string output_prefix,
+      uint num_trees, uint seed, uint num_threads, ImportanceMode importance_mode, uint min_node_size,
+      bool prediction_mode, bool sample_with_replacement, const std::vector<std::string>& unordered_variable_names,
+      bool memory_saving_splitting, SplitRule splitrule, bool predict_all, std::vector<double>& sample_fraction,
+      double alpha, double minprop, bool holdout, PredictionType prediction_type, uint num_random_splits,
+      bool order_snps, uint max_depth, const std::vector<double>& regularization_factor, bool regularization_usedepth);
+  virtual void initInternal() = 0;
+
+  // Grow or predict
+  void run(bool verbose, bool compute_oob_error);
+
+  // Write results to output files
+  void writeOutput();
+  virtual void writeOutputInternal() = 0;
+  virtual void writeConfusionFile() = 0;
+  virtual void writePredictionFile() = 0;
+  void writeImportanceFile();
+
+  // Save forest to file
+  void saveToFile();
+  virtual void saveToFileInternal(std::ofstream& outfile) = 0;
+
+  std::vector<std::vector<std::vector<size_t>>> getChildNodeIDs() {
+    std::vector<std::vector<std::vector<size_t>>> result;
+    for (auto& tree : trees) {
+      result.push_back(tree->getChildNodeIDs());
+    }
+    return result;
+  }
+  std::vector<std::vector<size_t>> getSplitVarIDs() {
+    std::vector<std::vector<size_t>> result;
+    for (auto& tree : trees) {
+      result.push_back(tree->getSplitVarIDs());
+    }
+    return result;
+  }
+  std::vector<std::vector<double>> getSplitValues() {
+    std::vector<std::vector<double>> result;
+    for (auto& tree : trees) {
+      result.push_back(tree->getSplitValues());
+    }
+    return result;
+  }
+  const std::vector<double>& getVariableImportance() const {
+    return variable_importance;
+  }
+  const std::vector<double>& getVariableImportanceCasewise() const {
+    return variable_importance_casewise;
+  }
+  double getOverallPredictionError() const {
+    return overall_prediction_error;
+  }
+  const std::vector<std::vector<std::vector<double>>>& getPredictions() const {
+    return predictions;
+  }
+  size_t getNumTrees() const {
+    return num_trees;
+  }
+  uint getMtry() const {
+    return mtry;
+  }
+  uint getMinNodeSize() const {
+    return min_node_size;
+  }
+  size_t getNumIndependentVariables() const {
+    return num_independent_variables;
+  }
+
+  const std::vector<bool>& getIsOrderedVariable() const {
+    return data->getIsOrderedVariable();
+  }
+
+  std::vector<std::vector<size_t>> getInbagCounts() const {
+    std::vector<std::vector<size_t>> result;
+    for (auto& tree : trees) {
+      result.push_back(tree->getInbagCounts());
+    }
+    return result;
+  }
+
+  const std::vector<std::vector<size_t>>& getSnpOrder() const {
+    return data->getSnpOrder();
+  }
+
+protected:
+  void grow();
+  virtual void growInternal() = 0;
+
+  // Predict using existing tree from file and data as prediction data
+  void predict();
+  virtual void allocatePredictMemory() = 0;
+  virtual void predictInternal(size_t sample_idx) = 0;
+
+  void computePredictionError();
+  virtual void computePredictionErrorInternal() = 0;
+
+  void computePermutationImportance();
+
+  // Multithreading methods for growing/prediction/importance, called by each thread
+  void growTreesInThread(uint thread_idx, std::vector<double>* variable_importance);
+  void predictTreesInThread(uint thread_idx, const Data* prediction_data, bool oob_prediction);
+  void predictInternalInThread(uint thread_idx);
+  void computeTreePermutationImportanceInThread(uint thread_idx, std::vector<double>& importance,
+      std::vector<double>& variance, std::vector<double>& importance_casewise);
+
+  // Load forest from file
+  void loadFromFile(std::string filename);
+  virtual void loadFromFileInternal(std::ifstream& infile) = 0;
+  void loadDependentVariableNamesFromFile(std::string filename);
+
+  // Load data from file
+  std::unique_ptr<Data> loadDataFromFile(const std::string& data_path);
+
+  // Set split select weights and variables to be always considered for splitting
+  void setSplitWeightVector(std::vector<std::vector<double>>& split_select_weights);
+  void setAlwaysSplitVariables(const std::vector<std::string>& always_split_variable_names);
+
+  // Show progress every few seconds
+#ifdef OLD_WIN_R_BUILD
+  void showProgress(std::string operation, clock_t start_time, clock_t& lap_time);
+#else
+  void showProgress(std::string operation, size_t max_progress);
+#endif
+
+  // Verbose output stream, cout if verbose==true, logfile if not
+  std::ostream* verbose_out;
+
+  std::vector<std::string> dependent_variable_names; // time,status for survival
+  size_t num_trees;
+  uint mtry;
+  uint min_node_size;
+  size_t num_independent_variables;
+  uint seed;
+  size_t num_samples;
+  bool prediction_mode;
+  MemoryMode memory_mode;
+  bool sample_with_replacement;
+  bool memory_saving_splitting;
+  SplitRule splitrule;
+  bool predict_all;
+  bool keep_inbag;
+  std::vector<double> sample_fraction;
+  bool holdout;
+  PredictionType prediction_type;
+  uint num_random_splits;
+  uint max_depth;
+
+  // MAXSTAT splitrule
+  double alpha;
+  double minprop;
+
+  // Multithreading
+  uint num_threads;
+  std::vector<uint> thread_ranges;
+#ifndef OLD_WIN_R_BUILD
+  std::mutex mutex;
+  std::condition_variable condition_variable;
+#endif
+
+  std::vector<std::unique_ptr<Tree>> trees;
+  std::unique_ptr<Data> data;
+
+  std::vector<std::vector<std::vector<double>>> predictions;
+  double overall_prediction_error;
+
+  // Weight vector for selecting possible split variables, one weight between 0 (never select) and 1 (always select) for each variable
+  // Deterministic variables are always selected
+  std::vector<size_t> deterministic_varIDs;
+  std::vector<std::vector<double>> split_select_weights;
+
+  // Bootstrap weights
+  std::vector<double> case_weights;
+
+  // Pre-selected bootstrap samples (per tree)
+  std::vector<std::vector<size_t>> manual_inbag;
+
+  // Random number generator
+  std::mt19937_64 random_number_generator;
+
+  std::string output_prefix;
+  ImportanceMode importance_mode;
+
+  // Regularization
+  std::vector<double> regularization_factor;
+  bool regularization_usedepth;
+  std::vector<bool> split_varIDs_used;
+  
+  // Variable importance for all variables in forest
+  std::vector<double> variable_importance;
+
+  // Casewise variable importance for all variables in forest
+  std::vector<double> variable_importance_casewise;
+
+  // Computation progress (finished trees)
+  size_t progress;
+#ifdef R_BUILD
+  size_t aborted_threads;
+  bool aborted;
+#endif
+};
+
+} // namespace ranger
+
+#endif /* FOREST_H_ */
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/ForestClassification.cpp
@@ -0,0 +1,333 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#include <unordered_map>
+#include <algorithm>
+#include <iterator>
+#include <random>
+#include <stdexcept>
+#include <cmath>
+#include <string>
+
+#include "utility.h"
+#include "ForestClassification.h"
+#include "TreeClassification.h"
+#include "Data.h"
+
+namespace ranger {
+
+void ForestClassification::loadForest(size_t num_trees,
+    std::vector<std::vector<std::vector<size_t>> >& forest_child_nodeIDs,
+    std::vector<std::vector<size_t>>& forest_split_varIDs, std::vector<std::vector<double>>& forest_split_values,
+    std::vector<double>& class_values, std::vector<bool>& is_ordered_variable) {
+
+  this->num_trees = num_trees;
+  this->class_values = class_values;
+  data->setIsOrderedVariable(is_ordered_variable);
+
+  // Create trees
+  trees.reserve(num_trees);
+  for (size_t i = 0; i < num_trees; ++i) {
+    trees.push_back(
+        make_unique<TreeClassification>(forest_child_nodeIDs[i], forest_split_varIDs[i], forest_split_values[i],
+            &this->class_values, &response_classIDs));
+  }
+
+  // Create thread ranges
+  equalSplit(thread_ranges, 0, num_trees - 1, num_threads);
+}
+
+void ForestClassification::initInternal() {
+
+  // If mtry not set, use floored square root of number of independent variables.
+  if (mtry == 0) {
+    unsigned long temp = sqrt((double) num_independent_variables);
+    mtry = std::max((unsigned long) 1, temp);
+  }
+
+  // Set minimal node size
+  if (min_node_size == 0) {
+    min_node_size = DEFAULT_MIN_NODE_SIZE_CLASSIFICATION;
+  }
+
+  // Create class_values and response_classIDs
+  if (!prediction_mode) {
+    for (size_t i = 0; i < num_samples; ++i) {
+      double value = data->get_y(i, 0);
+
+      // If classID is already in class_values, use ID. Else create a new one.
+      uint classID = find(class_values.begin(), class_values.end(), value) - class_values.begin();
+      if (classID == class_values.size()) {
+        class_values.push_back(value);
+      }
+      response_classIDs.push_back(classID);
+    }
+
+    if (splitrule == HELLINGER && class_values.size() != 2) {
+      throw std::runtime_error("Hellinger splitrule only implemented for binary classification.");
+    }
+  }
+
+  // Create sampleIDs_per_class if required
+  if (sample_fraction.size() > 1) {
+    sampleIDs_per_class.resize(sample_fraction.size());
+    for (auto& v : sampleIDs_per_class) {
+      v.reserve(num_samples);
+    }
+    for (size_t i = 0; i < num_samples; ++i) {
+      size_t classID = response_classIDs[i];
+      sampleIDs_per_class[classID].push_back(i);
+    }
+  }
+
+  // Set class weights all to 1
+  class_weights = std::vector<double>(class_values.size(), 1.0);
+
+  // Sort data if memory saving mode
+  if (!memory_saving_splitting) {
+    data->sort();
+  }
+}
+
+void ForestClassification::growInternal() {
+  trees.reserve(num_trees);
+  for (size_t i = 0; i < num_trees; ++i) {
+    trees.push_back(
+        make_unique<TreeClassification>(&class_values, &response_classIDs, &sampleIDs_per_class, &class_weights));
+  }
+}
+
+void ForestClassification::allocatePredictMemory() {
+  size_t num_prediction_samples = data->getNumRows();
+  if (predict_all || prediction_type == TERMINALNODES) {
+    predictions = std::vector<std::vector<std::vector<double>>>(1,
+        std::vector<std::vector<double>>(num_prediction_samples, std::vector<double>(num_trees)));
+  } else {
+    predictions = std::vector<std::vector<std::vector<double>>>(1,
+        std::vector<std::vector<double>>(1, std::vector<double>(num_prediction_samples)));
+  }
+}
+
+void ForestClassification::predictInternal(size_t sample_idx) {
+  if (predict_all || prediction_type == TERMINALNODES) {
+    // Get all tree predictions
+    for (size_t tree_idx = 0; tree_idx < num_trees; ++tree_idx) {
+      if (prediction_type == TERMINALNODES) {
+        predictions[0][sample_idx][tree_idx] = getTreePredictionTerminalNodeID(tree_idx, sample_idx);
+      } else {
+        predictions[0][sample_idx][tree_idx] = getTreePrediction(tree_idx, sample_idx);
+      }
+    }
+  } else {
+    // Count classes over trees and save class with maximum count
+    std::unordered_map<double, size_t> class_count;
+    for (size_t tree_idx = 0; tree_idx < num_trees; ++tree_idx) {
+      ++class_count[getTreePrediction(tree_idx, sample_idx)];
+    }
+    predictions[0][0][sample_idx] = mostFrequentValue(class_count, random_number_generator);
+  }
+}
+
+void ForestClassification::computePredictionErrorInternal() {
+
+  // Class counts for samples
+  std::vector<std::unordered_map<double, size_t>> class_counts;
+  class_counts.reserve(num_samples);
+  for (size_t i = 0; i < num_samples; ++i) {
+    class_counts.push_back(std::unordered_map<double, size_t>());
+  }
+
+  // For each tree loop over OOB samples and count classes
+  for (size_t tree_idx = 0; tree_idx < num_trees; ++tree_idx) {
+    for (size_t sample_idx = 0; sample_idx < trees[tree_idx]->getNumSamplesOob(); ++sample_idx) {
+      size_t sampleID = trees[tree_idx]->getOobSampleIDs()[sample_idx];
+      ++class_counts[sampleID][getTreePrediction(tree_idx, sample_idx)];
+    }
+  }
+
+  // Compute majority vote for each sample
+  predictions = std::vector<std::vector<std::vector<double>>>(1,
+      std::vector<std::vector<double>>(1, std::vector<double>(num_samples)));
+  for (size_t i = 0; i < num_samples; ++i) {
+    if (!class_counts[i].empty()) {
+      predictions[0][0][i] = mostFrequentValue(class_counts[i], random_number_generator);
+    } else {
+      predictions[0][0][i] = NAN;
+    }
+  }
+
+  // Compare predictions with true data
+  size_t num_missclassifications = 0;
+  size_t num_predictions = 0;
+  for (size_t i = 0; i < predictions[0][0].size(); ++i) {
+    double predicted_value = predictions[0][0][i];
+    if (!std::isnan(predicted_value)) {
+      ++num_predictions;
+      double real_value = data->get_y(i, 0);
+      if (predicted_value != real_value) {
+        ++num_missclassifications;
+      }
+      ++classification_table[std::make_pair(real_value, predicted_value)];
+    }
+  }
+  overall_prediction_error = (double) num_missclassifications / (double) num_predictions;
+}
+
+// #nocov start
+void ForestClassification::writeOutputInternal() {
+  if (verbose_out) {
+    *verbose_out << "Tree type:                         " << "Classification" << std::endl;
+  }
+}
+
+void ForestClassification::writeConfusionFile() {
+
+  // Open confusion file for writing
+  std::string filename = output_prefix + ".confusion";
+  std::ofstream outfile;
+  outfile.open(filename, std::ios::out);
+  if (!outfile.good()) {
+    throw std::runtime_error("Could not write to confusion file: " + filename + ".");
+  }
+
+  // Write confusion to file
+  outfile << "Overall OOB prediction error (Fraction missclassified): " << overall_prediction_error << std::endl;
+  outfile << std::endl;
+  outfile << "Class specific prediction errors:" << std::endl;
+  outfile << "           ";
+  for (auto& class_value : class_values) {
+    outfile << "     " << class_value;
+  }
+  outfile << std::endl;
+  for (auto& predicted_value : class_values) {
+    outfile << "predicted " << predicted_value << "     ";
+    for (auto& real_value : class_values) {
+      size_t value = classification_table[std::make_pair(real_value, predicted_value)];
+      outfile << value;
+      if (value < 10) {
+        outfile << "     ";
+      } else if (value < 100) {
+        outfile << "    ";
+      } else if (value < 1000) {
+        outfile << "   ";
+      } else if (value < 10000) {
+        outfile << "  ";
+      } else if (value < 100000) {
+        outfile << " ";
+      }
+    }
+    outfile << std::endl;
+  }
+
+  outfile.close();
+  if (verbose_out)
+    *verbose_out << "Saved confusion matrix to file " << filename << "." << std::endl;
+}
+
+void ForestClassification::writePredictionFile() {
+
+  // Open prediction file for writing
+  std::string filename = output_prefix + ".prediction";
+  std::ofstream outfile;
+  outfile.open(filename, std::ios::out);
+  if (!outfile.good()) {
+    throw std::runtime_error("Could not write to prediction file: " + filename + ".");
+  }
+
+  // Write
+  outfile << "Predictions: " << std::endl;
+  if (predict_all) {
+    for (size_t k = 0; k < num_trees; ++k) {
+      outfile << "Tree " << k << ":" << std::endl;
+      for (size_t i = 0; i < predictions.size(); ++i) {
+        for (size_t j = 0; j < predictions[i].size(); ++j) {
+          outfile << predictions[i][j][k] << std::endl;
+        }
+      }
+      outfile << std::endl;
+    }
+  } else {
+    for (size_t i = 0; i < predictions.size(); ++i) {
+      for (size_t j = 0; j < predictions[i].size(); ++j) {
+        for (size_t k = 0; k < predictions[i][j].size(); ++k) {
+          outfile << predictions[i][j][k] << std::endl;
+        }
+      }
+    }
+  }
+
+  if (verbose_out)
+    *verbose_out << "Saved predictions to file " << filename << "." << std::endl;
+}
+
+void ForestClassification::saveToFileInternal(std::ofstream& outfile) {
+
+  // Write num_variables
+  outfile.write((char*) &num_independent_variables, sizeof(num_independent_variables));
+
+  // Write treetype
+  TreeType treetype = TREE_CLASSIFICATION;
+  outfile.write((char*) &treetype, sizeof(treetype));
+
+  // Write class_values
+  saveVector1D(class_values, outfile);
+}
+
+void ForestClassification::loadFromFileInternal(std::ifstream& infile) {
+
+  // Read number of variables
+  size_t num_variables_saved;
+  infile.read((char*) &num_variables_saved, sizeof(num_variables_saved));
+
+  // Read treetype
+  TreeType treetype;
+  infile.read((char*) &treetype, sizeof(treetype));
+  if (treetype != TREE_CLASSIFICATION) {
+    throw std::runtime_error("Wrong treetype. Loaded file is not a classification forest.");
+  }
+
+  // Read class_values
+  readVector1D(class_values, infile);
+
+  for (size_t i = 0; i < num_trees; ++i) {
+
+    // Read data
+    std::vector<std::vector<size_t>> child_nodeIDs;
+    readVector2D(child_nodeIDs, infile);
+    std::vector<size_t> split_varIDs;
+    readVector1D(split_varIDs, infile);
+    std::vector<double> split_values;
+    readVector1D(split_values, infile);
+
+    // If dependent variable not in test data, throw error
+    if (num_variables_saved != num_independent_variables) {
+      throw std::runtime_error("Number of independent variables in data does not match with the loaded forest.");
+    }
+
+    // Create tree
+    trees.push_back(
+        make_unique<TreeClassification>(child_nodeIDs, split_varIDs, split_values, &class_values, &response_classIDs));
+  }
+}
+
+double ForestClassification::getTreePrediction(size_t tree_idx, size_t sample_idx) const {
+  const auto& tree = dynamic_cast<const TreeClassification&>(*trees[tree_idx]);
+  return tree.getPrediction(sample_idx);
+}
+
+size_t ForestClassification::getTreePredictionTerminalNodeID(size_t tree_idx, size_t sample_idx) const {
+  const auto& tree = dynamic_cast<const TreeClassification&>(*trees[tree_idx]);
+  return tree.getPredictionTerminalNodeID(sample_idx);
+}
+
+// #nocov end
+
+}// namespace ranger
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/ForestClassification.h
@@ -0,0 +1,76 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#ifndef FORESTCLASSIFICATION_H_
+#define FORESTCLASSIFICATION_H_
+
+#include <iostream>
+#include <map>
+#include <utility>
+#include <vector>
+
+#include "globals.h"
+#include "Forest.h"
+
+namespace ranger {
+
+class ForestClassification: public Forest {
+public:
+  ForestClassification() = default;
+
+  ForestClassification(const ForestClassification&) = delete;
+  ForestClassification& operator=(const ForestClassification&) = delete;
+
+  virtual ~ForestClassification() override = default;
+
+  void loadForest(size_t num_trees, std::vector<std::vector<std::vector<size_t>> >& forest_child_nodeIDs,
+      std::vector<std::vector<size_t>>& forest_split_varIDs, std::vector<std::vector<double>>& forest_split_values,
+      std::vector<double>& class_values, std::vector<bool>& is_ordered_variable);
+
+  const std::vector<double>& getClassValues() const {
+    return class_values;
+  }
+
+  void setClassWeights(std::vector<double>& class_weights) {
+    this->class_weights = class_weights;
+  }
+
+protected:
+  void initInternal() override;
+  void growInternal() override;
+  void allocatePredictMemory() override;
+  void predictInternal(size_t sample_idx) override;
+  void computePredictionErrorInternal() override;
+  void writeOutputInternal() override;
+  void writeConfusionFile() override;
+  void writePredictionFile() override;
+  void saveToFileInternal(std::ofstream& outfile) override;
+  void loadFromFileInternal(std::ifstream& infile) override;
+
+  // Classes of the dependent variable and classIDs for responses
+  std::vector<double> class_values;
+  std::vector<uint> response_classIDs;
+  std::vector<std::vector<size_t>> sampleIDs_per_class;
+
+  // Splitting weights
+  std::vector<double> class_weights;
+
+  // Table with classifications and true classes
+  std::map<std::pair<double, double>, size_t> classification_table;
+
+private:
+  double getTreePrediction(size_t tree_idx, size_t sample_idx) const;
+  size_t getTreePredictionTerminalNodeID(size_t tree_idx, size_t sample_idx) const;
+};
+
+} // namespace ranger
+
+#endif /* FORESTCLASSIFICATION_H_ */
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/ForestProbability.cpp
@@ -0,0 +1,341 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#include <stdexcept>
+
+#include "utility.h"
+#include "ForestProbability.h"
+#include "TreeProbability.h"
+#include "Data.h"
+
+namespace ranger {
+
+void ForestProbability::loadForest(size_t num_trees,
+    std::vector<std::vector<std::vector<size_t>> >& forest_child_nodeIDs,
+    std::vector<std::vector<size_t>>& forest_split_varIDs, std::vector<std::vector<double>>& forest_split_values,
+    std::vector<double>& class_values, std::vector<std::vector<std::vector<double>>>& forest_terminal_class_counts,
+    std::vector<bool>& is_ordered_variable) {
+
+  this->num_trees = num_trees;
+  this->class_values = class_values;
+  data->setIsOrderedVariable(is_ordered_variable);
+
+  // Create trees
+  trees.reserve(num_trees);
+  for (size_t i = 0; i < num_trees; ++i) {
+    trees.push_back(
+        make_unique<TreeProbability>(forest_child_nodeIDs[i], forest_split_varIDs[i], forest_split_values[i],
+            &this->class_values, &response_classIDs, forest_terminal_class_counts[i]));
+  }
+
+  // Create thread ranges
+  equalSplit(thread_ranges, 0, num_trees - 1, num_threads);
+}
+
+std::vector<std::vector<std::vector<double>>> ForestProbability::getTerminalClassCounts() const {
+  std::vector<std::vector<std::vector<double>>> result;
+  result.reserve(num_trees);
+  for (const auto& tree : trees) {
+    const auto& temp = dynamic_cast<const TreeProbability&>(*tree);
+    result.push_back(temp.getTerminalClassCounts());
+  }
+  return result;
+}
+
+void ForestProbability::initInternal() {
+
+  // If mtry not set, use floored square root of number of independent variables.
+  if (mtry == 0) {
+    unsigned long temp = sqrt((double) num_independent_variables);
+    mtry = std::max((unsigned long) 1, temp);
+  }
+
+  // Set minimal node size
+  if (min_node_size == 0) {
+    min_node_size = DEFAULT_MIN_NODE_SIZE_PROBABILITY;
+  }
+
+  // Create class_values and response_classIDs
+  if (!prediction_mode) {
+    for (size_t i = 0; i < num_samples; ++i) {
+      double value = data->get_y(i, 0);
+
+      // If classID is already in class_values, use ID. Else create a new one.
+      uint classID = find(class_values.begin(), class_values.end(), value) - class_values.begin();
+      if (classID == class_values.size()) {
+        class_values.push_back(value);
+      }
+      response_classIDs.push_back(classID);
+    }
+
+    if (splitrule == HELLINGER && class_values.size() != 2) {
+      throw std::runtime_error("Hellinger splitrule only implemented for binary classification.");
+    }
+  }
+
+  // Create sampleIDs_per_class if required
+  if (sample_fraction.size() > 1) {
+    sampleIDs_per_class.resize(sample_fraction.size());
+    for (auto& v : sampleIDs_per_class) {
+      v.reserve(num_samples);
+    }
+    for (size_t i = 0; i < num_samples; ++i) {
+      size_t classID = response_classIDs[i];
+      sampleIDs_per_class[classID].push_back(i);
+    }
+  }
+
+  // Set class weights all to 1
+  class_weights = std::vector<double>(class_values.size(), 1.0);
+
+  // Sort data if memory saving mode
+  if (!memory_saving_splitting) {
+    data->sort();
+  }
+}
+
+void ForestProbability::growInternal() {
+  trees.reserve(num_trees);
+  for (size_t i = 0; i < num_trees; ++i) {
+    trees.push_back(
+        make_unique<TreeProbability>(&class_values, &response_classIDs, &sampleIDs_per_class, &class_weights));
+  }
+}
+
+void ForestProbability::allocatePredictMemory() {
+  size_t num_prediction_samples = data->getNumRows();
+  if (predict_all) {
+    predictions = std::vector<std::vector<std::vector<double>>>(num_prediction_samples,
+        std::vector<std::vector<double>>(class_values.size(), std::vector<double>(num_trees, 0)));
+  } else if (prediction_type == TERMINALNODES) {
+    predictions = std::vector<std::vector<std::vector<double>>>(1,
+        std::vector<std::vector<double>>(num_prediction_samples, std::vector<double>(num_trees, 0)));
+  } else {
+    predictions = std::vector<std::vector<std::vector<double>>>(1,
+        std::vector<std::vector<double>>(num_prediction_samples, std::vector<double>(class_values.size(), 0)));
+  }
+}
+
+void ForestProbability::predictInternal(size_t sample_idx) {
+  // For each sample compute proportions in each tree
+  for (size_t tree_idx = 0; tree_idx < num_trees; ++tree_idx) {
+    if (predict_all) {
+      std::vector<double> counts = getTreePrediction(tree_idx, sample_idx);
+
+      for (size_t class_idx = 0; class_idx < counts.size(); ++class_idx) {
+        predictions[sample_idx][class_idx][tree_idx] += counts[class_idx];
+      }
+    } else if (prediction_type == TERMINALNODES) {
+      predictions[0][sample_idx][tree_idx] = getTreePredictionTerminalNodeID(tree_idx, sample_idx);
+    } else {
+      std::vector<double> counts = getTreePrediction(tree_idx, sample_idx);
+
+      for (size_t class_idx = 0; class_idx < counts.size(); ++class_idx) {
+        predictions[0][sample_idx][class_idx] += counts[class_idx];
+      }
+    }
+  }
+
+  // Average over trees
+  if (!predict_all && prediction_type != TERMINALNODES) {
+    for (size_t class_idx = 0; class_idx < predictions[0][sample_idx].size(); ++class_idx) {
+      predictions[0][sample_idx][class_idx] /= num_trees;
+    }
+  }
+}
+
+void ForestProbability::computePredictionErrorInternal() {
+
+// For each sample sum over trees where sample is OOB
+  std::vector<size_t> samples_oob_count;
+  samples_oob_count.resize(num_samples, 0);
+  predictions = std::vector<std::vector<std::vector<double>>>(1,
+      std::vector<std::vector<double>>(num_samples, std::vector<double>(class_values.size(), 0)));
+
+  for (size_t tree_idx = 0; tree_idx < num_trees; ++tree_idx) {
+    for (size_t sample_idx = 0; sample_idx < trees[tree_idx]->getNumSamplesOob(); ++sample_idx) {
+      size_t sampleID = trees[tree_idx]->getOobSampleIDs()[sample_idx];
+      std::vector<double> counts = getTreePrediction(tree_idx, sample_idx);
+
+      for (size_t class_idx = 0; class_idx < counts.size(); ++class_idx) {
+        predictions[0][sampleID][class_idx] += counts[class_idx];
+      }
+      ++samples_oob_count[sampleID];
+    }
+  }
+
+// MSE with predicted probability and true data
+  size_t num_predictions = 0;
+  overall_prediction_error = 0;
+  for (size_t i = 0; i < predictions[0].size(); ++i) {
+    if (samples_oob_count[i] > 0) {
+      ++num_predictions;
+      for (size_t j = 0; j < predictions[0][i].size(); ++j) {
+        predictions[0][i][j] /= (double) samples_oob_count[i];
+      }
+      size_t real_classID = response_classIDs[i];
+      double predicted_value = predictions[0][i][real_classID];
+      overall_prediction_error += (1 - predicted_value) * (1 - predicted_value);
+    } else {
+      for (size_t j = 0; j < predictions[0][i].size(); ++j) {
+        predictions[0][i][j] = NAN;
+      }
+    }
+  }
+
+  overall_prediction_error /= (double) num_predictions;
+}
+
+// #nocov start
+void ForestProbability::writeOutputInternal() {
+  if (verbose_out) {
+    *verbose_out << "Tree type:                         " << "Probability estimation" << std::endl;
+  }
+}
+
+void ForestProbability::writeConfusionFile() {
+
+// Open confusion file for writing
+  std::string filename = output_prefix + ".confusion";
+  std::ofstream outfile;
+  outfile.open(filename, std::ios::out);
+  if (!outfile.good()) {
+    throw std::runtime_error("Could not write to confusion file: " + filename + ".");
+  }
+
+// Write confusion to file
+  outfile << "Overall OOB prediction error (MSE): " << overall_prediction_error << std::endl;
+
+  outfile.close();
+  if (verbose_out)
+    *verbose_out << "Saved prediction error to file " << filename << "." << std::endl;
+}
+
+void ForestProbability::writePredictionFile() {
+
+  // Open prediction file for writing
+  std::string filename = output_prefix + ".prediction";
+  std::ofstream outfile;
+  outfile.open(filename, std::ios::out);
+  if (!outfile.good()) {
+    throw std::runtime_error("Could not write to prediction file: " + filename + ".");
+  }
+
+  // Write
+  outfile << "Class predictions, one sample per row." << std::endl;
+  for (auto& class_value : class_values) {
+    outfile << class_value << " ";
+  }
+  outfile << std::endl << std::endl;
+
+  if (predict_all) {
+    for (size_t k = 0; k < num_trees; ++k) {
+      outfile << "Tree " << k << ":" << std::endl;
+      for (size_t i = 0; i < predictions.size(); ++i) {
+        for (size_t j = 0; j < predictions[i].size(); ++j) {
+          outfile << predictions[i][j][k] << " ";
+        }
+        outfile << std::endl;
+      }
+      outfile << std::endl;
+    }
+  } else {
+    for (size_t i = 0; i < predictions.size(); ++i) {
+      for (size_t j = 0; j < predictions[i].size(); ++j) {
+        for (size_t k = 0; k < predictions[i][j].size(); ++k) {
+          outfile << predictions[i][j][k] << " ";
+        }
+        outfile << std::endl;
+      }
+    }
+  }
+
+  if (verbose_out)
+    *verbose_out << "Saved predictions to file " << filename << "." << std::endl;
+}
+
+void ForestProbability::saveToFileInternal(std::ofstream& outfile) {
+
+// Write num_variables
+  outfile.write((char*) &num_independent_variables, sizeof(num_independent_variables));
+
+// Write treetype
+  TreeType treetype = TREE_PROBABILITY;
+  outfile.write((char*) &treetype, sizeof(treetype));
+
+// Write class_values
+  saveVector1D(class_values, outfile);
+}
+
+void ForestProbability::loadFromFileInternal(std::ifstream& infile) {
+
+// Read number of variables
+  size_t num_variables_saved;
+  infile.read((char*) &num_variables_saved, sizeof(num_variables_saved));
+
+// Read treetype
+  TreeType treetype;
+  infile.read((char*) &treetype, sizeof(treetype));
+  if (treetype != TREE_PROBABILITY) {
+    throw std::runtime_error("Wrong treetype. Loaded file is not a probability estimation forest.");
+  }
+
+// Read class_values
+  readVector1D(class_values, infile);
+
+  for (size_t i = 0; i < num_trees; ++i) {
+
+    // Read data
+    std::vector<std::vector<size_t>> child_nodeIDs;
+    readVector2D(child_nodeIDs, infile);
+    std::vector<size_t> split_varIDs;
+    readVector1D(split_varIDs, infile);
+    std::vector<double> split_values;
+    readVector1D(split_values, infile);
+
+    // Read Terminal node class counts
+    std::vector<size_t> terminal_nodes;
+    readVector1D(terminal_nodes, infile);
+    std::vector<std::vector<double>> terminal_class_counts_vector;
+    readVector2D(terminal_class_counts_vector, infile);
+
+    // Convert Terminal node class counts to vector with empty elemtents for non-terminal nodes
+    std::vector<std::vector<double>> terminal_class_counts;
+    terminal_class_counts.resize(child_nodeIDs[0].size(), std::vector<double>());
+    for (size_t j = 0; j < terminal_nodes.size(); ++j) {
+      terminal_class_counts[terminal_nodes[j]] = terminal_class_counts_vector[j];
+    }
+
+    // If dependent variable not in test data, throw error
+    if (num_variables_saved != num_independent_variables) {
+      throw std::runtime_error("Number of independent variables in data does not match with the loaded forest.");
+    }
+
+    // Create tree
+    trees.push_back(
+        make_unique<TreeProbability>(child_nodeIDs, split_varIDs, split_values, &class_values, &response_classIDs,
+            terminal_class_counts));
+  }
+}
+
+const std::vector<double>& ForestProbability::getTreePrediction(size_t tree_idx, size_t sample_idx) const {
+  const auto& tree = dynamic_cast<const TreeProbability&>(*trees[tree_idx]);
+  return tree.getPrediction(sample_idx);
+}
+
+size_t ForestProbability::getTreePredictionTerminalNodeID(size_t tree_idx, size_t sample_idx) const {
+  const auto& tree = dynamic_cast<const TreeProbability&>(*trees[tree_idx]);
+  return tree.getPredictionTerminalNodeID(sample_idx);
+}
+
+// #nocov end
+
+}// namespace ranger
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/ForestProbability.h
@@ -0,0 +1,76 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#ifndef FORESTPROBABILITY_H_
+#define FORESTPROBABILITY_H_
+
+#include <map>
+#include <utility>
+#include <vector>
+
+#include "globals.h"
+#include "Forest.h"
+#include "TreeProbability.h"
+
+namespace ranger {
+
+class ForestProbability: public Forest {
+public:
+  ForestProbability() = default;
+
+  ForestProbability(const ForestProbability&) = delete;
+  ForestProbability& operator=(const ForestProbability&) = delete;
+
+  virtual ~ForestProbability() override = default;
+
+  void loadForest(size_t num_trees, std::vector<std::vector<std::vector<size_t>> >& forest_child_nodeIDs,
+      std::vector<std::vector<size_t>>& forest_split_varIDs, std::vector<std::vector<double>>& forest_split_values,
+      std::vector<double>& class_values, std::vector<std::vector<std::vector<double>>>& forest_terminal_class_counts,
+      std::vector<bool>& is_ordered_variable);
+
+  std::vector<std::vector<std::vector<double>>> getTerminalClassCounts() const;
+
+  const std::vector<double>& getClassValues() const {
+    return class_values;
+  }
+
+  void setClassWeights(std::vector<double>& class_weights) {
+    this->class_weights = class_weights;
+  }
+
+protected:
+  void initInternal() override;
+  void growInternal() override;
+  void allocatePredictMemory() override;
+  void predictInternal(size_t sample_idx) override;
+  void computePredictionErrorInternal() override;
+  void writeOutputInternal() override;
+  void writeConfusionFile() override;
+  void writePredictionFile() override;
+  void saveToFileInternal(std::ofstream& outfile) override;
+  void loadFromFileInternal(std::ifstream& infile) override;
+
+  // Classes of the dependent variable and classIDs for responses
+  std::vector<double> class_values;
+  std::vector<uint> response_classIDs;
+  std::vector<std::vector<size_t>> sampleIDs_per_class;
+
+  // Splitting weights
+  std::vector<double> class_weights;
+
+private:
+  const std::vector<double>& getTreePrediction(size_t tree_idx, size_t sample_idx) const;
+  size_t getTreePredictionTerminalNodeID(size_t tree_idx, size_t sample_idx) const;
+};
+
+} // namespace ranger
+
+#endif /* FORESTPROBABILITY_H_ */
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/ForestRegression.cpp
@@ -0,0 +1,260 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#include <algorithm>
+#include <stdexcept>
+#include <string>
+
+#include "utility.h"
+#include "ForestRegression.h"
+#include "TreeRegression.h"
+#include "Data.h"
+
+namespace ranger {
+
+void ForestRegression::loadForest(size_t num_trees,
+    std::vector<std::vector<std::vector<size_t>> >& forest_child_nodeIDs,
+    std::vector<std::vector<size_t>>& forest_split_varIDs, std::vector<std::vector<double>>& forest_split_values,
+    std::vector<bool>& is_ordered_variable) {
+
+  this->num_trees = num_trees;
+  data->setIsOrderedVariable(is_ordered_variable);
+
+  // Create trees
+  trees.reserve(num_trees);
+  for (size_t i = 0; i < num_trees; ++i) {
+    trees.push_back(
+        make_unique<TreeRegression>(forest_child_nodeIDs[i], forest_split_varIDs[i], forest_split_values[i]));
+  }
+
+  // Create thread ranges
+  equalSplit(thread_ranges, 0, num_trees - 1, num_threads);
+}
+
+void ForestRegression::initInternal() {
+
+  // If mtry not set, use floored square root of number of independent variables
+  if (mtry == 0) {
+    unsigned long temp = sqrt((double) num_independent_variables);
+    mtry = std::max((unsigned long) 1, temp);
+  }
+
+  // Set minimal node size
+  if (min_node_size == 0) {
+    min_node_size = DEFAULT_MIN_NODE_SIZE_REGRESSION;
+  }
+
+  // Error if beta splitrule used with data outside of [0,1]
+  if (splitrule == BETA && !prediction_mode) {
+    for (size_t i = 0; i < num_samples; ++i) {
+      double y = data->get_y(i, 0);
+      if (y < 0 || y > 1) {
+        throw std::runtime_error("Beta splitrule applicable to regression data with outcome between 0 and 1 only.");
+      }
+    }
+  }
+
+  // Sort data if memory saving mode
+  if (!memory_saving_splitting) {
+    data->sort();
+  }
+}
+
+void ForestRegression::growInternal() {
+  trees.reserve(num_trees);
+  for (size_t i = 0; i < num_trees; ++i) {
+    trees.push_back(make_unique<TreeRegression>());
+  }
+}
+
+void ForestRegression::allocatePredictMemory() {
+  size_t num_prediction_samples = data->getNumRows();
+  if (predict_all || prediction_type == TERMINALNODES) {
+    predictions = std::vector<std::vector<std::vector<double>>>(1,
+        std::vector<std::vector<double>>(num_prediction_samples, std::vector<double>(num_trees)));
+  } else {
+    predictions = std::vector<std::vector<std::vector<double>>>(1,
+        std::vector<std::vector<double>>(1, std::vector<double>(num_prediction_samples)));
+  }
+}
+
+void ForestRegression::predictInternal(size_t sample_idx) {
+  if (predict_all || prediction_type == TERMINALNODES) {
+    // Get all tree predictions
+    for (size_t tree_idx = 0; tree_idx < num_trees; ++tree_idx) {
+      if (prediction_type == TERMINALNODES) {
+        predictions[0][sample_idx][tree_idx] = getTreePredictionTerminalNodeID(tree_idx, sample_idx);
+      } else {
+        predictions[0][sample_idx][tree_idx] = getTreePrediction(tree_idx, sample_idx);
+      }
+    }
+  } else {
+    // Mean over trees
+    double prediction_sum = 0;
+    for (size_t tree_idx = 0; tree_idx < num_trees; ++tree_idx) {
+      prediction_sum += getTreePrediction(tree_idx, sample_idx);
+    }
+    predictions[0][0][sample_idx] = prediction_sum / num_trees;
+  }
+}
+
+void ForestRegression::computePredictionErrorInternal() {
+
+// For each sample sum over trees where sample is OOB
+  std::vector<size_t> samples_oob_count;
+  predictions = std::vector<std::vector<std::vector<double>>>(1,
+      std::vector<std::vector<double>>(1, std::vector<double>(num_samples, 0)));
+  samples_oob_count.resize(num_samples, 0);
+  for (size_t tree_idx = 0; tree_idx < num_trees; ++tree_idx) {
+    for (size_t sample_idx = 0; sample_idx < trees[tree_idx]->getNumSamplesOob(); ++sample_idx) {
+      size_t sampleID = trees[tree_idx]->getOobSampleIDs()[sample_idx];
+      double value = getTreePrediction(tree_idx, sample_idx);
+
+      predictions[0][0][sampleID] += value;
+      ++samples_oob_count[sampleID];
+    }
+  }
+
+// MSE with predictions and true data
+  size_t num_predictions = 0;
+  overall_prediction_error = 0;
+  for (size_t i = 0; i < predictions[0][0].size(); ++i) {
+    if (samples_oob_count[i] > 0) {
+      ++num_predictions;
+      predictions[0][0][i] /= (double) samples_oob_count[i];
+      double predicted_value = predictions[0][0][i];
+      double real_value = data->get_y(i, 0);
+      overall_prediction_error += (predicted_value - real_value) * (predicted_value - real_value);
+    } else {
+      predictions[0][0][i] = NAN;
+    }
+  }
+
+  overall_prediction_error /= (double) num_predictions;
+}
+
+// #nocov start
+void ForestRegression::writeOutputInternal() {
+  if (verbose_out) {
+    *verbose_out << "Tree type:                         " << "Regression" << std::endl;
+  }
+}
+
+void ForestRegression::writeConfusionFile() {
+
+// Open confusion file for writing
+  std::string filename = output_prefix + ".confusion";
+  std::ofstream outfile;
+  outfile.open(filename, std::ios::out);
+  if (!outfile.good()) {
+    throw std::runtime_error("Could not write to confusion file: " + filename + ".");
+  }
+
+// Write confusion to file
+  outfile << "Overall OOB prediction error (MSE): " << overall_prediction_error << std::endl;
+
+  outfile.close();
+  if (verbose_out)
+    *verbose_out << "Saved prediction error to file " << filename << "." << std::endl;
+}
+
+void ForestRegression::writePredictionFile() {
+
+// Open prediction file for writing
+  std::string filename = output_prefix + ".prediction";
+  std::ofstream outfile;
+  outfile.open(filename, std::ios::out);
+  if (!outfile.good()) {
+    throw std::runtime_error("Could not write to prediction file: " + filename + ".");
+  }
+
+  // Write
+  outfile << "Predictions: " << std::endl;
+  if (predict_all) {
+    for (size_t k = 0; k < num_trees; ++k) {
+      outfile << "Tree " << k << ":" << std::endl;
+      for (size_t i = 0; i < predictions.size(); ++i) {
+        for (size_t j = 0; j < predictions[i].size(); ++j) {
+          outfile << predictions[i][j][k] << std::endl;
+        }
+      }
+      outfile << std::endl;
+    }
+  } else {
+    for (size_t i = 0; i < predictions.size(); ++i) {
+      for (size_t j = 0; j < predictions[i].size(); ++j) {
+        for (size_t k = 0; k < predictions[i][j].size(); ++k) {
+          outfile << predictions[i][j][k] << std::endl;
+        }
+      }
+    }
+  }
+
+  if (verbose_out)
+    *verbose_out << "Saved predictions to file " << filename << "." << std::endl;
+}
+
+void ForestRegression::saveToFileInternal(std::ofstream& outfile) {
+
+// Write num_variables
+  outfile.write((char*) &num_independent_variables, sizeof(num_independent_variables));
+
+// Write treetype
+  TreeType treetype = TREE_REGRESSION;
+  outfile.write((char*) &treetype, sizeof(treetype));
+}
+
+void ForestRegression::loadFromFileInternal(std::ifstream& infile) {
+
+// Read number of variables
+  size_t num_variables_saved;
+  infile.read((char*) &num_variables_saved, sizeof(num_variables_saved));
+
+// Read treetype
+  TreeType treetype;
+  infile.read((char*) &treetype, sizeof(treetype));
+  if (treetype != TREE_REGRESSION) {
+    throw std::runtime_error("Wrong treetype. Loaded file is not a regression forest.");
+  }
+
+  for (size_t i = 0; i < num_trees; ++i) {
+
+    // Read data
+    std::vector<std::vector<size_t>> child_nodeIDs;
+    readVector2D(child_nodeIDs, infile);
+    std::vector<size_t> split_varIDs;
+    readVector1D(split_varIDs, infile);
+    std::vector<double> split_values;
+    readVector1D(split_values, infile);
+
+    // If dependent variable not in test data, throw error
+    if (num_variables_saved != num_independent_variables) {
+      throw std::runtime_error("Number of independent variables in data does not match with the loaded forest.");
+    }
+
+    // Create tree
+    trees.push_back(make_unique<TreeRegression>(child_nodeIDs, split_varIDs, split_values));
+  }
+}
+
+double ForestRegression::getTreePrediction(size_t tree_idx, size_t sample_idx) const {
+  const auto& tree = dynamic_cast<const TreeRegression&>(*trees[tree_idx]);
+  return tree.getPrediction(sample_idx);
+}
+
+size_t ForestRegression::getTreePredictionTerminalNodeID(size_t tree_idx, size_t sample_idx) const {
+  const auto& tree = dynamic_cast<const TreeRegression&>(*trees[tree_idx]);
+  return tree.getPredictionTerminalNodeID(sample_idx);
+}
+
+// #nocov end
+
+}// namespace ranger
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/ForestRegression.h
@@ -0,0 +1,55 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#ifndef FORESTREGRESSION_H_
+#define FORESTREGRESSION_H_
+
+#include <iostream>
+#include <vector>
+
+#include "globals.h"
+#include "Forest.h"
+
+namespace ranger {
+
+class ForestRegression: public Forest {
+public:
+  ForestRegression() = default;
+
+  ForestRegression(const ForestRegression&) = delete;
+  ForestRegression& operator=(const ForestRegression&) = delete;
+
+  virtual ~ForestRegression() override = default;
+
+  void loadForest(size_t num_trees, std::vector<std::vector<std::vector<size_t>> >& forest_child_nodeIDs,
+      std::vector<std::vector<size_t>>& forest_split_varIDs, std::vector<std::vector<double>>& forest_split_values,
+      std::vector<bool>& is_ordered_variable);
+
+private:
+  void initInternal() override;
+  void growInternal() override;
+  void allocatePredictMemory() override;
+  void predictInternal(size_t sample_idx) override;
+  void computePredictionErrorInternal() override;
+  void writeOutputInternal() override;
+  void writeConfusionFile() override;
+  void writePredictionFile() override;
+  void saveToFileInternal(std::ofstream& outfile) override;
+  void loadFromFileInternal(std::ifstream& infile) override;
+
+private:
+  double getTreePrediction(size_t tree_idx, size_t sample_idx) const;
+  size_t getTreePredictionTerminalNodeID(size_t tree_idx, size_t sample_idx) const;
+};
+
+} // namespace ranger
+
+#endif /* FORESTREGRESSION_H_ */
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/ForestSurvival.cpp
@@ -0,0 +1,336 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#include <set>
+#include <algorithm>
+#include <cmath>
+#include <stdexcept>
+#include <string>
+
+#include "utility.h"
+#include "ForestSurvival.h"
+#include "Data.h"
+
+namespace ranger {
+
+void ForestSurvival::loadForest(size_t num_trees, std::vector<std::vector<std::vector<size_t>> >& forest_child_nodeIDs,
+    std::vector<std::vector<size_t>>& forest_split_varIDs, std::vector<std::vector<double>>& forest_split_values,
+    std::vector<std::vector<std::vector<double>> >& forest_chf, std::vector<double>& unique_timepoints,
+    std::vector<bool>& is_ordered_variable) {
+
+  this->num_trees = num_trees;
+  this->unique_timepoints = unique_timepoints;
+  data->setIsOrderedVariable(is_ordered_variable);
+
+  // Create trees
+  trees.reserve(num_trees);
+  for (size_t i = 0; i < num_trees; ++i) {
+    trees.push_back(
+        make_unique<TreeSurvival>(forest_child_nodeIDs[i], forest_split_varIDs[i], forest_split_values[i],
+            forest_chf[i], &this->unique_timepoints, &response_timepointIDs));
+  }
+
+  // Create thread ranges
+  equalSplit(thread_ranges, 0, num_trees - 1, num_threads);
+}
+
+std::vector<std::vector<std::vector<double>>> ForestSurvival::getChf() const {
+  std::vector<std::vector<std::vector<double>>> result;
+  result.reserve(num_trees);
+  for (const auto& tree : trees) {
+    const auto& temp = dynamic_cast<const TreeSurvival&>(*tree);
+    result.push_back(temp.getChf());
+  }
+  return result;
+}
+
+void ForestSurvival::initInternal() {
+
+  // If mtry not set, use floored square root of number of independent variables.
+  if (mtry == 0) {
+    unsigned long temp = ceil(sqrt((double) num_independent_variables));
+    mtry = std::max((unsigned long) 1, temp);
+  }
+
+  // Set minimal node size
+  if (min_node_size == 0) {
+    min_node_size = DEFAULT_MIN_NODE_SIZE_SURVIVAL;
+  }
+
+  // Create unique timepoints
+  if (!prediction_mode) {
+    std::set<double> unique_timepoint_set;
+    for (size_t i = 0; i < num_samples; ++i) {
+      unique_timepoint_set.insert(data->get_y(i, 0));
+    }
+    unique_timepoints.reserve(unique_timepoint_set.size());
+    for (auto& t : unique_timepoint_set) {
+      unique_timepoints.push_back(t);
+    }
+
+    // Create response_timepointIDs
+    for (size_t i = 0; i < num_samples; ++i) {
+      double value = data->get_y(i, 0);
+
+      // If timepoint is already in unique_timepoints, use ID. Else create a new one.
+      uint timepointID = find(unique_timepoints.begin(), unique_timepoints.end(), value) - unique_timepoints.begin();
+      response_timepointIDs.push_back(timepointID);
+    }
+  }
+
+  // Sort data if extratrees and not memory saving mode
+  if (splitrule == EXTRATREES && !memory_saving_splitting) {
+    data->sort();
+  }
+}
+
+void ForestSurvival::growInternal() {
+  trees.reserve(num_trees);
+  for (size_t i = 0; i < num_trees; ++i) {
+    trees.push_back(make_unique<TreeSurvival>(&unique_timepoints, &response_timepointIDs));
+  }
+}
+
+void ForestSurvival::allocatePredictMemory() {
+  size_t num_prediction_samples = data->getNumRows();
+  size_t num_timepoints = unique_timepoints.size();
+  if (predict_all) {
+    predictions = std::vector<std::vector<std::vector<double>>>(num_prediction_samples,
+        std::vector<std::vector<double>>(num_timepoints, std::vector<double>(num_trees, 0)));
+  } else if (prediction_type == TERMINALNODES) {
+    predictions = std::vector<std::vector<std::vector<double>>>(1,
+        std::vector<std::vector<double>>(num_prediction_samples, std::vector<double>(num_trees, 0)));
+  } else {
+    predictions = std::vector<std::vector<std::vector<double>>>(1,
+        std::vector<std::vector<double>>(num_prediction_samples, std::vector<double>(num_timepoints, 0)));
+  }
+}
+
+void ForestSurvival::predictInternal(size_t sample_idx) {
+  // For each timepoint sum over trees
+  if (predict_all) {
+    for (size_t j = 0; j < unique_timepoints.size(); ++j) {
+      for (size_t k = 0; k < num_trees; ++k) {
+        predictions[sample_idx][j][k] = getTreePrediction(k, sample_idx)[j];
+      }
+    }
+  } else if (prediction_type == TERMINALNODES) {
+    for (size_t k = 0; k < num_trees; ++k) {
+      predictions[0][sample_idx][k] = getTreePredictionTerminalNodeID(k, sample_idx);
+    }
+  } else {
+    for (size_t j = 0; j < unique_timepoints.size(); ++j) {
+      double sample_time_prediction = 0;
+      for (size_t k = 0; k < num_trees; ++k) {
+        sample_time_prediction += getTreePrediction(k, sample_idx)[j];
+      }
+      predictions[0][sample_idx][j] = sample_time_prediction / num_trees;
+    }
+  }
+}
+
+void ForestSurvival::computePredictionErrorInternal() {
+
+  size_t num_timepoints = unique_timepoints.size();
+
+  // For each sample sum over trees where sample is OOB
+  std::vector<size_t> samples_oob_count;
+  samples_oob_count.resize(num_samples, 0);
+  predictions = std::vector<std::vector<std::vector<double>>>(1,
+      std::vector<std::vector<double>>(num_samples, std::vector<double>(num_timepoints, 0)));
+
+  for (size_t tree_idx = 0; tree_idx < num_trees; ++tree_idx) {
+    for (size_t sample_idx = 0; sample_idx < trees[tree_idx]->getNumSamplesOob(); ++sample_idx) {
+      size_t sampleID = trees[tree_idx]->getOobSampleIDs()[sample_idx];
+      std::vector<double> tree_sample_chf = getTreePrediction(tree_idx, sample_idx);
+
+      for (size_t time_idx = 0; time_idx < tree_sample_chf.size(); ++time_idx) {
+        predictions[0][sampleID][time_idx] += tree_sample_chf[time_idx];
+      }
+      ++samples_oob_count[sampleID];
+    }
+  }
+
+  // Divide sample predictions by number of trees where sample is oob and compute summed chf for samples
+  std::vector<double> sum_chf;
+  sum_chf.reserve(predictions[0].size());
+  std::vector<size_t> oob_sampleIDs;
+  oob_sampleIDs.reserve(predictions[0].size());
+  for (size_t i = 0; i < predictions[0].size(); ++i) {
+    if (samples_oob_count[i] > 0) {
+      double sum = 0;
+      for (size_t j = 0; j < predictions[0][i].size(); ++j) {
+        predictions[0][i][j] /= samples_oob_count[i];
+        sum += predictions[0][i][j];
+      }
+      sum_chf.push_back(sum);
+      oob_sampleIDs.push_back(i);
+    }
+  }
+
+  // Use all samples which are OOB at least once
+  overall_prediction_error = 1 - computeConcordanceIndex(*data, sum_chf, oob_sampleIDs, NULL);
+}
+
+// #nocov start
+void ForestSurvival::writeOutputInternal() {
+  if (verbose_out) {
+    *verbose_out << "Tree type:                         " << "Survival" << std::endl;
+    if (dependent_variable_names.size() >= 2) {
+      *verbose_out << "Status variable name:              " << dependent_variable_names[1] << std::endl;
+    }
+  }
+}
+
+void ForestSurvival::writeConfusionFile() {
+
+  // Open confusion file for writing
+  std::string filename = output_prefix + ".confusion";
+  std::ofstream outfile;
+  outfile.open(filename, std::ios::out);
+  if (!outfile.good()) {
+    throw std::runtime_error("Could not write to confusion file: " + filename + ".");
+  }
+
+  // Write confusion to file
+  outfile << "Overall OOB prediction error (1 - C): " << overall_prediction_error << std::endl;
+
+  outfile.close();
+  if (verbose_out)
+    *verbose_out << "Saved prediction error to file " << filename << "." << std::endl;
+
+}
+
+void ForestSurvival::writePredictionFile() {
+
+  // Open prediction file for writing
+  std::string filename = output_prefix + ".prediction";
+  std::ofstream outfile;
+  outfile.open(filename, std::ios::out);
+  if (!outfile.good()) {
+    throw std::runtime_error("Could not write to prediction file: " + filename + ".");
+  }
+
+  // Write
+  outfile << "Unique timepoints: " << std::endl;
+  for (auto& timepoint : unique_timepoints) {
+    outfile << timepoint << " ";
+  }
+  outfile << std::endl << std::endl;
+
+  outfile << "Cumulative hazard function, one row per sample: " << std::endl;
+  if (predict_all) {
+    for (size_t k = 0; k < num_trees; ++k) {
+      outfile << "Tree " << k << ":" << std::endl;
+      for (size_t i = 0; i < predictions.size(); ++i) {
+        for (size_t j = 0; j < predictions[i].size(); ++j) {
+          outfile << predictions[i][j][k] << " ";
+        }
+        outfile << std::endl;
+      }
+      outfile << std::endl;
+    }
+  } else {
+    for (size_t i = 0; i < predictions.size(); ++i) {
+      for (size_t j = 0; j < predictions[i].size(); ++j) {
+        for (size_t k = 0; k < predictions[i][j].size(); ++k) {
+          outfile << predictions[i][j][k] << " ";
+        }
+        outfile << std::endl;
+      }
+    }
+  }
+
+  if (verbose_out)
+    *verbose_out << "Saved predictions to file " << filename << "." << std::endl;
+}
+
+void ForestSurvival::saveToFileInternal(std::ofstream& outfile) {
+
+  // Write num_variables
+  outfile.write((char*) &num_independent_variables, sizeof(num_independent_variables));
+
+  // Write treetype
+  TreeType treetype = TREE_SURVIVAL;
+  outfile.write((char*) &treetype, sizeof(treetype));
+
+  // Write unique timepoints
+  saveVector1D(unique_timepoints, outfile);
+}
+
+void ForestSurvival::loadFromFileInternal(std::ifstream& infile) {
+
+  // Read number of variables
+  size_t num_variables_saved;
+  infile.read((char*) &num_variables_saved, sizeof(num_variables_saved));
+
+  // Read treetype
+  TreeType treetype;
+  infile.read((char*) &treetype, sizeof(treetype));
+  if (treetype != TREE_SURVIVAL) {
+    throw std::runtime_error("Wrong treetype. Loaded file is not a survival forest.");
+  }
+
+  // Read unique timepoints
+  unique_timepoints.clear();
+  readVector1D(unique_timepoints, infile);
+
+  for (size_t i = 0; i < num_trees; ++i) {
+
+    // Read data
+    std::vector<std::vector<size_t>> child_nodeIDs;
+    readVector2D(child_nodeIDs, infile);
+    std::vector<size_t> split_varIDs;
+    readVector1D(split_varIDs, infile);
+    std::vector<double> split_values;
+    readVector1D(split_values, infile);
+
+    // Read chf
+    std::vector<size_t> terminal_nodes;
+    readVector1D(terminal_nodes, infile);
+    std::vector<std::vector<double>> chf_vector;
+    readVector2D(chf_vector, infile);
+
+    // Convert chf to vector with empty elements for non-terminal nodes
+    std::vector<std::vector<double>> chf;
+    chf.resize(child_nodeIDs[0].size(), std::vector<double>());
+//    for (size_t i = 0; i < child_nodeIDs.size(); ++i) {
+//      chf.push_back(std::vector<double>());
+//    }
+    for (size_t j = 0; j < terminal_nodes.size(); ++j) {
+      chf[terminal_nodes[j]] = chf_vector[j];
+    }
+
+    // If dependent variable not in test data, throw error
+    if (num_variables_saved != num_independent_variables) {
+      throw std::runtime_error("Number of independent variables in data does not match with the loaded forest.");
+    }
+
+    // Create tree
+    trees.push_back(
+        make_unique<TreeSurvival>(child_nodeIDs, split_varIDs, split_values, chf, &unique_timepoints,
+            &response_timepointIDs));
+  }
+}
+
+const std::vector<double>& ForestSurvival::getTreePrediction(size_t tree_idx, size_t sample_idx) const {
+  const auto& tree = dynamic_cast<const TreeSurvival&>(*trees[tree_idx]);
+  return tree.getPrediction(sample_idx);
+}
+
+size_t ForestSurvival::getTreePredictionTerminalNodeID(size_t tree_idx, size_t sample_idx) const {
+  const auto& tree = dynamic_cast<const TreeSurvival&>(*trees[tree_idx]);
+  return tree.getPredictionTerminalNodeID(sample_idx);
+}
+
+// #nocov end
+
+}// namespace ranger
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/ForestSurvival.h
@@ -0,0 +1,66 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#ifndef FORESTSURVIVAL_H_
+#define FORESTSURVIVAL_H_
+
+#include <iostream>
+#include <vector>
+
+#include "globals.h"
+#include "Forest.h"
+#include "TreeSurvival.h"
+
+namespace ranger {
+
+class ForestSurvival: public Forest {
+public:
+  ForestSurvival() = default;
+
+  ForestSurvival(const ForestSurvival&) = delete;
+  ForestSurvival& operator=(const ForestSurvival&) = delete;
+
+  virtual ~ForestSurvival() override = default;
+
+  void loadForest(size_t num_trees, std::vector<std::vector<std::vector<size_t>> >& forest_child_nodeIDs,
+      std::vector<std::vector<size_t>>& forest_split_varIDs, std::vector<std::vector<double>>& forest_split_values,
+      std::vector<std::vector<std::vector<double>> >& forest_chf, std::vector<double>& unique_timepoints,
+      std::vector<bool>& is_ordered_variable);
+
+  std::vector<std::vector<std::vector<double>>> getChf() const;
+
+  const std::vector<double>& getUniqueTimepoints() const {
+    return unique_timepoints;
+  }
+
+private:
+  void initInternal() override;
+  void growInternal() override;
+  void allocatePredictMemory() override;
+  void predictInternal(size_t sample_idx) override;
+  void computePredictionErrorInternal() override;
+  void writeOutputInternal() override;
+  void writeConfusionFile() override;
+  void writePredictionFile() override;
+  void saveToFileInternal(std::ofstream& outfile) override;
+  void loadFromFileInternal(std::ifstream& infile) override;
+
+  std::vector<double> unique_timepoints;
+  std::vector<size_t> response_timepointIDs;
+
+private:
+  const std::vector<double>& getTreePrediction(size_t tree_idx, size_t sample_idx) const;
+  size_t getTreePredictionTerminalNodeID(size_t tree_idx, size_t sample_idx) const;
+};
+
+} // namespace ranger
+
+#endif /* FORESTSURVIVAL_H_ */
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/Tree.cpp
@@ -0,0 +1,593 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#include <iterator>
+
+#include "Tree.h"
+#include "utility.h"
+
+namespace ranger {
+
+Tree::Tree() :
+    mtry(0), num_samples(0), num_samples_oob(0), min_node_size(0), deterministic_varIDs(0), split_select_weights(0), case_weights(
+        0), manual_inbag(0), oob_sampleIDs(0), holdout(false), keep_inbag(false), data(0), regularization_factor(0), regularization_usedepth(
+        false), split_varIDs_used(0), variable_importance(0), importance_mode(DEFAULT_IMPORTANCE_MODE), sample_with_replacement(
+        true), sample_fraction(0), memory_saving_splitting(false), splitrule(DEFAULT_SPLITRULE), alpha(DEFAULT_ALPHA), minprop(
+        DEFAULT_MINPROP), num_random_splits(DEFAULT_NUM_RANDOM_SPLITS), max_depth(DEFAULT_MAXDEPTH), depth(0), last_left_nodeID(
+        0) {
+}
+
+Tree::Tree(std::vector<std::vector<size_t>>& child_nodeIDs, std::vector<size_t>& split_varIDs,
+    std::vector<double>& split_values) :
+    mtry(0), num_samples(0), num_samples_oob(0), min_node_size(0), deterministic_varIDs(0), split_select_weights(0), case_weights(
+        0), manual_inbag(0), split_varIDs(split_varIDs), split_values(split_values), child_nodeIDs(child_nodeIDs), oob_sampleIDs(
+        0), holdout(false), keep_inbag(false), data(0), regularization_factor(0), regularization_usedepth(false), split_varIDs_used(
+        0), variable_importance(0), importance_mode(DEFAULT_IMPORTANCE_MODE), sample_with_replacement(true), sample_fraction(
+        0), memory_saving_splitting(false), splitrule(DEFAULT_SPLITRULE), alpha(DEFAULT_ALPHA), minprop(
+        DEFAULT_MINPROP), num_random_splits(DEFAULT_NUM_RANDOM_SPLITS), max_depth(DEFAULT_MAXDEPTH), depth(0), last_left_nodeID(
+        0) {
+}
+
+void Tree::init(const Data* data, uint mtry, size_t num_samples, uint seed, std::vector<size_t>* deterministic_varIDs,
+    std::vector<double>* split_select_weights, ImportanceMode importance_mode, uint min_node_size,
+    bool sample_with_replacement, bool memory_saving_splitting, SplitRule splitrule, std::vector<double>* case_weights,
+    std::vector<size_t>* manual_inbag, bool keep_inbag, std::vector<double>* sample_fraction, double alpha,
+    double minprop, bool holdout, uint num_random_splits, uint max_depth, std::vector<double>* regularization_factor,
+    bool regularization_usedepth, std::vector<bool>* split_varIDs_used) {
+
+  this->data = data;
+  this->mtry = mtry;
+  this->num_samples = num_samples;
+  this->memory_saving_splitting = memory_saving_splitting;
+
+  // Create root node, assign bootstrap sample and oob samples
+  child_nodeIDs.push_back(std::vector<size_t>());
+  child_nodeIDs.push_back(std::vector<size_t>());
+  createEmptyNode();
+
+  // Initialize random number generator and set seed
+  random_number_generator.seed(seed);
+
+  this->deterministic_varIDs = deterministic_varIDs;
+  this->split_select_weights = split_select_weights;
+  this->importance_mode = importance_mode;
+  this->min_node_size = min_node_size;
+  this->sample_with_replacement = sample_with_replacement;
+  this->splitrule = splitrule;
+  this->case_weights = case_weights;
+  this->manual_inbag = manual_inbag;
+  this->keep_inbag = keep_inbag;
+  this->sample_fraction = sample_fraction;
+  this->holdout = holdout;
+  this->alpha = alpha;
+  this->minprop = minprop;
+  this->num_random_splits = num_random_splits;
+  this->max_depth = max_depth;
+  this->regularization_factor = regularization_factor;
+  this->regularization_usedepth = regularization_usedepth;
+  this->split_varIDs_used = split_varIDs_used;
+
+  // Regularization
+  if (regularization_factor->size() > 0) {
+    regularization = true;
+  } else {
+    regularization = false;
+  }
+}
+
+void Tree::grow(std::vector<double>* variable_importance) {
+  // Allocate memory for tree growing
+  allocateMemory();
+
+  this->variable_importance = variable_importance;
+
+// Bootstrap, dependent if weighted or not and with or without replacement
+  if (!case_weights->empty()) {
+    if (sample_with_replacement) {
+      bootstrapWeighted();
+    } else {
+      bootstrapWithoutReplacementWeighted();
+    }
+  } else if (sample_fraction->size() > 1) {
+    if (sample_with_replacement) {
+      bootstrapClassWise();
+    } else {
+      bootstrapWithoutReplacementClassWise();
+    }
+  } else if (!manual_inbag->empty()) {
+    setManualInbag();
+  } else {
+    if (sample_with_replacement) {
+      bootstrap();
+    } else {
+      bootstrapWithoutReplacement();
+    }
+  }
+
+  // Init start and end positions
+  start_pos[0] = 0;
+  end_pos[0] = sampleIDs.size();
+
+  // While not all nodes terminal, split next node
+  size_t num_open_nodes = 1;
+  size_t i = 0;
+  depth = 0;
+  while (num_open_nodes > 0) {
+    // Split node
+    bool is_terminal_node = splitNode(i);
+    if (is_terminal_node) {
+      --num_open_nodes;
+    } else {
+      ++num_open_nodes;
+      if (i >= last_left_nodeID) {
+        // If new level, increase depth
+        // (left_node saves left-most node in current level, new level reached if that node is splitted)
+        last_left_nodeID = split_varIDs.size() - 2;
+        ++depth;
+      }
+    }
+    ++i;
+  }
+
+  // Delete sampleID vector to save memory
+  sampleIDs.clear();
+  sampleIDs.shrink_to_fit();
+  cleanUpInternal();
+}
+
+void Tree::predict(const Data* prediction_data, bool oob_prediction) {
+
+  size_t num_samples_predict;
+  if (oob_prediction) {
+    num_samples_predict = num_samples_oob;
+  } else {
+    num_samples_predict = prediction_data->getNumRows();
+  }
+
+  prediction_terminal_nodeIDs.resize(num_samples_predict, 0);
+
+// For each sample start in root, drop down the tree and return final value
+  for (size_t i = 0; i < num_samples_predict; ++i) {
+    size_t sample_idx;
+    if (oob_prediction) {
+      sample_idx = oob_sampleIDs[i];
+    } else {
+      sample_idx = i;
+    }
+    size_t nodeID = 0;
+    while (1) {
+
+      // Break if terminal node
+      if (child_nodeIDs[0][nodeID] == 0 && child_nodeIDs[1][nodeID] == 0) {
+        break;
+      }
+
+      // Move to child
+      size_t split_varID = split_varIDs[nodeID];
+
+      double value = prediction_data->get_x(sample_idx, split_varID);
+      if (prediction_data->isOrderedVariable(split_varID)) {
+        if (value <= split_values[nodeID]) {
+          // Move to left child
+          nodeID = child_nodeIDs[0][nodeID];
+        } else {
+          // Move to right child
+          nodeID = child_nodeIDs[1][nodeID];
+        }
+      } else {
+        size_t factorID = floor(value) - 1;
+        size_t splitID = floor(split_values[nodeID]);
+
+        // Left if 0 found at position factorID
+        if (!(splitID & (1ULL << factorID))) {
+          // Move to left child
+          nodeID = child_nodeIDs[0][nodeID];
+        } else {
+          // Move to right child
+          nodeID = child_nodeIDs[1][nodeID];
+        }
+      }
+    }
+
+    prediction_terminal_nodeIDs[i] = nodeID;
+  }
+}
+
+void Tree::computePermutationImportance(std::vector<double>& forest_importance, std::vector<double>& forest_variance,
+    std::vector<double>& forest_importance_casewise) {
+
+  size_t num_independent_variables = data->getNumCols();
+
+// Compute normal prediction accuracy for each tree. Predictions already computed..
+  double accuracy_normal;
+  std::vector<double> prederr_normal_casewise;
+  std::vector<double> prederr_shuf_casewise;
+  if (importance_mode == IMP_PERM_CASEWISE) {
+    prederr_normal_casewise.resize(num_samples_oob, 0);
+    prederr_shuf_casewise.resize(num_samples_oob, 0);
+    accuracy_normal = computePredictionAccuracyInternal(&prederr_normal_casewise);
+  } else {
+    accuracy_normal = computePredictionAccuracyInternal(NULL);
+  }
+
+  prediction_terminal_nodeIDs.clear();
+  prediction_terminal_nodeIDs.resize(num_samples_oob, 0);
+
+// Reserve space for permutations, initialize with oob_sampleIDs
+  std::vector<size_t> permutations(oob_sampleIDs);
+
+// Randomly permute for all independent variables
+  for (size_t i = 0; i < num_independent_variables; ++i) {
+
+    // Permute and compute prediction accuracy again for this permutation and save difference
+    permuteAndPredictOobSamples(i, permutations);
+    double accuracy_permuted;
+    if (importance_mode == IMP_PERM_CASEWISE) {
+      accuracy_permuted = computePredictionAccuracyInternal(&prederr_shuf_casewise);
+      for (size_t j = 0; j < num_samples_oob; ++j) {
+        size_t pos = i * num_samples + oob_sampleIDs[j];
+        forest_importance_casewise[pos] += prederr_shuf_casewise[j] - prederr_normal_casewise[j];
+      }
+    } else {
+      accuracy_permuted = computePredictionAccuracyInternal(NULL);
+    }
+
+    double accuracy_difference = accuracy_normal - accuracy_permuted;
+    forest_importance[i] += accuracy_difference;
+
+    // Compute variance
+    if (importance_mode == IMP_PERM_BREIMAN) {
+      forest_variance[i] += accuracy_difference * accuracy_difference;
+    } else if (importance_mode == IMP_PERM_LIAW) {
+      forest_variance[i] += accuracy_difference * accuracy_difference * num_samples_oob;
+    }
+  }
+}
+
+// #nocov start
+void Tree::appendToFile(std::ofstream& file) {
+
+// Save general fields
+  saveVector2D(child_nodeIDs, file);
+  saveVector1D(split_varIDs, file);
+  saveVector1D(split_values, file);
+
+// Call special functions for subclasses to save special fields.
+  appendToFileInternal(file);
+}
+// #nocov end
+
+void Tree::createPossibleSplitVarSubset(std::vector<size_t>& result) {
+
+  size_t num_vars = data->getNumCols();
+
+  // For corrected Gini importance add dummy variables
+  if (importance_mode == IMP_GINI_CORRECTED) {
+    num_vars += data->getNumCols();
+  }
+
+  // Randomly add non-deterministic variables (according to weights if needed)
+  if (split_select_weights->empty()) {
+    if (deterministic_varIDs->empty()) {
+      drawWithoutReplacement(result, random_number_generator, num_vars, mtry);
+    } else {
+      drawWithoutReplacementSkip(result, random_number_generator, num_vars, (*deterministic_varIDs), mtry);
+    }
+  } else {
+    drawWithoutReplacementWeighted(result, random_number_generator, num_vars, mtry, *split_select_weights);
+  }
+
+  // Always use deterministic variables
+  std::copy(deterministic_varIDs->begin(), deterministic_varIDs->end(), std::inserter(result, result.end()));
+}
+
+bool Tree::splitNode(size_t nodeID) {
+
+  // Select random subset of variables to possibly split at
+  std::vector<size_t> possible_split_varIDs;
+  createPossibleSplitVarSubset(possible_split_varIDs);
+
+  // Call subclass method, sets split_varIDs and split_values
+  bool stop = splitNodeInternal(nodeID, possible_split_varIDs);
+  if (stop) {
+    // Terminal node
+    return true;
+  }
+
+  size_t split_varID = split_varIDs[nodeID];
+  double split_value = split_values[nodeID];
+
+  // Save non-permuted variable for prediction
+  split_varIDs[nodeID] = data->getUnpermutedVarID(split_varID);
+
+  // Create child nodes
+  size_t left_child_nodeID = split_varIDs.size();
+  child_nodeIDs[0][nodeID] = left_child_nodeID;
+  createEmptyNode();
+  start_pos[left_child_nodeID] = start_pos[nodeID];
+
+  size_t right_child_nodeID = split_varIDs.size();
+  child_nodeIDs[1][nodeID] = right_child_nodeID;
+  createEmptyNode();
+  start_pos[right_child_nodeID] = end_pos[nodeID];
+
+  // For each sample in node, assign to left or right child
+  if (data->isOrderedVariable(split_varID)) {
+    // Ordered: left is <= splitval and right is > splitval
+    size_t pos = start_pos[nodeID];
+    while (pos < start_pos[right_child_nodeID]) {
+      size_t sampleID = sampleIDs[pos];
+      if (data->get_x(sampleID, split_varID) <= split_value) {
+        // If going to left, do nothing
+        ++pos;
+      } else {
+        // If going to right, move to right end
+        --start_pos[right_child_nodeID];
+        std::swap(sampleIDs[pos], sampleIDs[start_pos[right_child_nodeID]]);
+      }
+    }
+  } else {
+    // Unordered: If bit at position is 1 -> right, 0 -> left
+    size_t pos = start_pos[nodeID];
+    while (pos < start_pos[right_child_nodeID]) {
+      size_t sampleID = sampleIDs[pos];
+      double level = data->get_x(sampleID, split_varID);
+      size_t factorID = floor(level) - 1;
+      size_t splitID = floor(split_value);
+
+      // Left if 0 found at position factorID
+      if (!(splitID & (1ULL << factorID))) {
+        // If going to left, do nothing
+        ++pos;
+      } else {
+        // If going to right, move to right end
+        --start_pos[right_child_nodeID];
+        std::swap(sampleIDs[pos], sampleIDs[start_pos[right_child_nodeID]]);
+      }
+    }
+  }
+
+  // End position of left child is start position of right child
+  end_pos[left_child_nodeID] = start_pos[right_child_nodeID];
+  end_pos[right_child_nodeID] = end_pos[nodeID];
+
+  // No terminal node
+  return false;
+}
+
+void Tree::createEmptyNode() {
+  split_varIDs.push_back(0);
+  split_values.push_back(0);
+  child_nodeIDs[0].push_back(0);
+  child_nodeIDs[1].push_back(0);
+  start_pos.push_back(0);
+  end_pos.push_back(0);
+
+  createEmptyNodeInternal();
+}
+
+size_t Tree::dropDownSamplePermuted(size_t permuted_varID, size_t sampleID, size_t permuted_sampleID) {
+
+// Start in root and drop down
+  size_t nodeID = 0;
+  while (child_nodeIDs[0][nodeID] != 0 || child_nodeIDs[1][nodeID] != 0) {
+
+    // Permute if variable is permutation variable
+    size_t split_varID = split_varIDs[nodeID];
+    size_t sampleID_final = sampleID;
+    if (split_varID == permuted_varID) {
+      sampleID_final = permuted_sampleID;
+    }
+
+    // Move to child
+    double value = data->get_x(sampleID_final, split_varID);
+    if (data->isOrderedVariable(split_varID)) {
+      if (value <= split_values[nodeID]) {
+        // Move to left child
+        nodeID = child_nodeIDs[0][nodeID];
+      } else {
+        // Move to right child
+        nodeID = child_nodeIDs[1][nodeID];
+      }
+    } else {
+      size_t factorID = floor(value) - 1;
+      size_t splitID = floor(split_values[nodeID]);
+
+      // Left if 0 found at position factorID
+      if (!(splitID & (1ULL << factorID))) {
+        // Move to left child
+        nodeID = child_nodeIDs[0][nodeID];
+      } else {
+        // Move to right child
+        nodeID = child_nodeIDs[1][nodeID];
+      }
+    }
+
+  }
+  return nodeID;
+}
+
+void Tree::permuteAndPredictOobSamples(size_t permuted_varID, std::vector<size_t>& permutations) {
+
+// Permute OOB sample
+//std::vector<size_t> permutations(oob_sampleIDs);
+  std::shuffle(permutations.begin(), permutations.end(), random_number_generator);
+
+// For each sample, drop down the tree and add prediction
+  for (size_t i = 0; i < num_samples_oob; ++i) {
+    size_t nodeID = dropDownSamplePermuted(permuted_varID, oob_sampleIDs[i], permutations[i]);
+    prediction_terminal_nodeIDs[i] = nodeID;
+  }
+}
+
+void Tree::bootstrap() {
+
+// Use fraction (default 63.21%) of the samples
+  size_t num_samples_inbag = (size_t) num_samples * (*sample_fraction)[0];
+
+// Reserve space, reserve a little more to be save)
+  sampleIDs.reserve(num_samples_inbag);
+  oob_sampleIDs.reserve(num_samples * (exp(-(*sample_fraction)[0]) + 0.1));
+
+  std::uniform_int_distribution<size_t> unif_dist(0, num_samples - 1);
+
+// Start with all samples OOB
+  inbag_counts.resize(num_samples, 0);
+
+// Draw num_samples samples with replacement (num_samples_inbag out of n) as inbag and mark as not OOB
+  for (size_t s = 0; s < num_samples_inbag; ++s) {
+    size_t draw = unif_dist(random_number_generator);
+    sampleIDs.push_back(draw);
+    ++inbag_counts[draw];
+  }
+
+// Save OOB samples
+  for (size_t s = 0; s < inbag_counts.size(); ++s) {
+    if (inbag_counts[s] == 0) {
+      oob_sampleIDs.push_back(s);
+    }
+  }
+  num_samples_oob = oob_sampleIDs.size();
+
+  if (!keep_inbag) {
+    inbag_counts.clear();
+    inbag_counts.shrink_to_fit();
+  }
+}
+
+void Tree::bootstrapWeighted() {
+
+// Use fraction (default 63.21%) of the samples
+  size_t num_samples_inbag = (size_t) num_samples * (*sample_fraction)[0];
+
+// Reserve space, reserve a little more to be save)
+  sampleIDs.reserve(num_samples_inbag);
+  oob_sampleIDs.reserve(num_samples * (exp(-(*sample_fraction)[0]) + 0.1));
+
+  std::discrete_distribution<> weighted_dist(case_weights->begin(), case_weights->end());
+
+// Start with all samples OOB
+  inbag_counts.resize(num_samples, 0);
+
+// Draw num_samples samples with replacement (n out of n) as inbag and mark as not OOB
+  for (size_t s = 0; s < num_samples_inbag; ++s) {
+    size_t draw = weighted_dist(random_number_generator);
+    sampleIDs.push_back(draw);
+    ++inbag_counts[draw];
+  }
+
+  // Save OOB samples. In holdout mode these are the cases with 0 weight.
+  if (holdout) {
+    for (size_t s = 0; s < (*case_weights).size(); ++s) {
+      if ((*case_weights)[s] == 0) {
+        oob_sampleIDs.push_back(s);
+      }
+    }
+  } else {
+    for (size_t s = 0; s < inbag_counts.size(); ++s) {
+      if (inbag_counts[s] == 0) {
+        oob_sampleIDs.push_back(s);
+      }
+    }
+  }
+  num_samples_oob = oob_sampleIDs.size();
+
+  if (!keep_inbag) {
+    inbag_counts.clear();
+    inbag_counts.shrink_to_fit();
+  }
+}
+
+void Tree::bootstrapWithoutReplacement() {
+
+// Use fraction (default 63.21%) of the samples
+  size_t num_samples_inbag = (size_t) num_samples * (*sample_fraction)[0];
+  shuffleAndSplit(sampleIDs, oob_sampleIDs, num_samples, num_samples_inbag, random_number_generator);
+  num_samples_oob = oob_sampleIDs.size();
+
+  if (keep_inbag) {
+    // All observation are 0 or 1 times inbag
+    inbag_counts.resize(num_samples, 1);
+    for (size_t i = 0; i < oob_sampleIDs.size(); i++) {
+      inbag_counts[oob_sampleIDs[i]] = 0;
+    }
+  }
+}
+
+void Tree::bootstrapWithoutReplacementWeighted() {
+
+// Use fraction (default 63.21%) of the samples
+  size_t num_samples_inbag = (size_t) num_samples * (*sample_fraction)[0];
+  drawWithoutReplacementWeighted(sampleIDs, random_number_generator, num_samples - 1, num_samples_inbag, *case_weights);
+
+// All observation are 0 or 1 times inbag
+  inbag_counts.resize(num_samples, 0);
+  for (auto& sampleID : sampleIDs) {
+    inbag_counts[sampleID] = 1;
+  }
+
+// Save OOB samples. In holdout mode these are the cases with 0 weight.
+  if (holdout) {
+    for (size_t s = 0; s < (*case_weights).size(); ++s) {
+      if ((*case_weights)[s] == 0) {
+        oob_sampleIDs.push_back(s);
+      }
+    }
+  } else {
+    for (size_t s = 0; s < inbag_counts.size(); ++s) {
+      if (inbag_counts[s] == 0) {
+        oob_sampleIDs.push_back(s);
+      }
+    }
+  }
+  num_samples_oob = oob_sampleIDs.size();
+
+  if (!keep_inbag) {
+    inbag_counts.clear();
+    inbag_counts.shrink_to_fit();
+  }
+}
+
+void Tree::bootstrapClassWise() {
+  // Empty on purpose (virtual function only implemented in classification and probability)
+}
+
+void Tree::bootstrapWithoutReplacementClassWise() {
+  // Empty on purpose (virtual function only implemented in classification and probability)
+}
+
+void Tree::setManualInbag() {
+  // Select observation as specified in manual_inbag vector
+  sampleIDs.reserve(manual_inbag->size());
+  inbag_counts.resize(num_samples, 0);
+  for (size_t i = 0; i < manual_inbag->size(); ++i) {
+    size_t inbag_count = (*manual_inbag)[i];
+    if ((*manual_inbag)[i] > 0) {
+      for (size_t j = 0; j < inbag_count; ++j) {
+        sampleIDs.push_back(i);
+      }
+      inbag_counts[i] = inbag_count;
+    } else {
+      oob_sampleIDs.push_back(i);
+    }
+  }
+  num_samples_oob = oob_sampleIDs.size();
+
+  // Shuffle samples
+  std::shuffle(sampleIDs.begin(), sampleIDs.end(), random_number_generator);
+
+  if (!keep_inbag) {
+    inbag_counts.clear();
+    inbag_counts.shrink_to_fit();
+  }
+}
+
+} // namespace ranger
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/Tree.h
@@ -0,0 +1,234 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#ifndef TREE_H_
+#define TREE_H_
+
+#include <vector>
+#include <random>
+#include <iostream>
+#include <stdexcept>
+
+#include "globals.h"
+#include "Data.h"
+
+namespace ranger {
+
+class Tree {
+public:
+  Tree();
+
+  // Create from loaded forest
+  Tree(std::vector<std::vector<size_t>>& child_nodeIDs, std::vector<size_t>& split_varIDs,
+      std::vector<double>& split_values);
+
+  virtual ~Tree() = default;
+
+  Tree(const Tree&) = delete;
+  Tree& operator=(const Tree&) = delete;
+
+  void init(const Data* data, uint mtry, size_t num_samples, uint seed, std::vector<size_t>* deterministic_varIDs,
+      std::vector<double>* split_select_weights, ImportanceMode importance_mode, uint min_node_size,
+      bool sample_with_replacement, bool memory_saving_splitting, SplitRule splitrule,
+      std::vector<double>* case_weights, std::vector<size_t>* manual_inbag, bool keep_inbag,
+      std::vector<double>* sample_fraction, double alpha, double minprop, bool holdout, uint num_random_splits,
+      uint max_depth, std::vector<double>* regularization_factor, bool regularization_usedepth,
+      std::vector<bool>* split_varIDs_used);
+
+  virtual void allocateMemory() = 0;
+
+  void grow(std::vector<double>* variable_importance);
+
+  void predict(const Data* prediction_data, bool oob_prediction);
+
+  void computePermutationImportance(std::vector<double>& forest_importance, std::vector<double>& forest_variance,
+      std::vector<double>& forest_importance_casewise);
+
+  void appendToFile(std::ofstream& file);
+  virtual void appendToFileInternal(std::ofstream& file) = 0;
+
+  const std::vector<std::vector<size_t>>& getChildNodeIDs() const {
+    return child_nodeIDs;
+  }
+  const std::vector<double>& getSplitValues() const {
+    return split_values;
+  }
+  const std::vector<size_t>& getSplitVarIDs() const {
+    return split_varIDs;
+  }
+
+  const std::vector<size_t>& getOobSampleIDs() const {
+    return oob_sampleIDs;
+  }
+  size_t getNumSamplesOob() const {
+    return num_samples_oob;
+  }
+
+  const std::vector<size_t>& getInbagCounts() const {
+    return inbag_counts;
+  }
+
+protected:
+  void createPossibleSplitVarSubset(std::vector<size_t>& result);
+
+  bool splitNode(size_t nodeID);
+  virtual bool splitNodeInternal(size_t nodeID, std::vector<size_t>& possible_split_varIDs) = 0;
+
+  void createEmptyNode();
+  virtual void createEmptyNodeInternal() = 0;
+
+  size_t dropDownSamplePermuted(size_t permuted_varID, size_t sampleID, size_t permuted_sampleID);
+  void permuteAndPredictOobSamples(size_t permuted_varID, std::vector<size_t>& permutations);
+
+  virtual double computePredictionAccuracyInternal(std::vector<double>* prediction_error_casewise) = 0;
+  
+  void bootstrap();
+  void bootstrapWithoutReplacement();
+
+  void bootstrapWeighted();
+  void bootstrapWithoutReplacementWeighted();
+
+  virtual void bootstrapClassWise();
+  virtual void bootstrapWithoutReplacementClassWise();
+
+  void setManualInbag();
+
+  virtual void cleanUpInternal() = 0;
+
+  void regularize(double& decrease, size_t varID) {
+    if (regularization) {
+      if (importance_mode == IMP_GINI_CORRECTED) {
+        varID = data->getUnpermutedVarID(varID);
+      }
+      if ((*regularization_factor)[varID] != 1) {
+        if (!(*split_varIDs_used)[varID]) {
+          if (regularization_usedepth) {
+            decrease *= std::pow((*regularization_factor)[varID], depth + 1);
+          } else {
+            decrease *= (*regularization_factor)[varID];
+          }
+        }
+      }
+    }
+  }
+
+  void regularizeNegative(double& decrease, size_t varID) {
+      if (regularization) {
+        if (importance_mode == IMP_GINI_CORRECTED) {
+          varID = data->getUnpermutedVarID(varID);
+        }
+        if ((*regularization_factor)[varID] != 1) {
+          if (!(*split_varIDs_used)[varID]) {
+            if (regularization_usedepth) {
+              decrease /= std::pow((*regularization_factor)[varID], depth + 1);
+            } else {
+              decrease /= (*regularization_factor)[varID];
+            }
+          }
+        }
+      }
+    }
+
+  void saveSplitVarID(size_t varID) {
+    if (regularization) {
+      if (importance_mode == IMP_GINI_CORRECTED) {
+        (*split_varIDs_used)[data->getUnpermutedVarID(varID)] = true;
+      } else {
+        (*split_varIDs_used)[varID] = true;
+      }
+    }
+  }
+
+  uint mtry;
+
+  // Number of samples (all samples, not only inbag for this tree)
+  size_t num_samples;
+
+  // Number of OOB samples
+  size_t num_samples_oob;
+
+  // Minimum node size to split, like in original RF nodes of smaller size can be produced
+  uint min_node_size;
+
+  // Weight vector for selecting possible split variables, one weight between 0 (never select) and 1 (always select) for each variable
+  // Deterministic variables are always selected
+  const std::vector<size_t>* deterministic_varIDs;
+  const std::vector<double>* split_select_weights;
+
+  // Bootstrap weights
+  const std::vector<double>* case_weights;
+
+  // Pre-selected bootstrap samples
+  const std::vector<size_t>* manual_inbag;
+
+  // Splitting variable for each node
+  std::vector<size_t> split_varIDs;
+
+  // Value to split at for each node, for now only binary split
+  // For terminal nodes the prediction value is saved here
+  std::vector<double> split_values;
+
+  // Vector of left and right child node IDs, 0 for no child
+  std::vector<std::vector<size_t>> child_nodeIDs;
+
+  // All sampleIDs in the tree, will be re-ordered while splitting
+  std::vector<size_t> sampleIDs;
+
+  // For each node a vector with start and end positions
+  std::vector<size_t> start_pos;
+  std::vector<size_t> end_pos;
+
+  // IDs of OOB individuals, sorted
+  std::vector<size_t> oob_sampleIDs;
+
+  // Holdout mode
+  bool holdout;
+
+  // Inbag counts
+  bool keep_inbag;
+  std::vector<size_t> inbag_counts;
+
+  // Random number generator
+  std::mt19937_64 random_number_generator;
+
+  // Pointer to original data
+  const Data* data;
+
+  // Regularization
+  bool regularization;
+  std::vector<double>* regularization_factor;
+  bool regularization_usedepth;
+  std::vector<bool>* split_varIDs_used;
+  
+  // Variable importance for all variables
+  std::vector<double>* variable_importance;
+  ImportanceMode importance_mode;
+
+  // When growing here the OOB set is used
+  // Terminal nodeIDs for prediction samples
+  std::vector<size_t> prediction_terminal_nodeIDs;
+
+  bool sample_with_replacement;
+  const std::vector<double>* sample_fraction;
+
+  bool memory_saving_splitting;
+  SplitRule splitrule;
+  double alpha;
+  double minprop;
+  uint num_random_splits;
+  uint max_depth;
+  uint depth;
+  size_t last_left_nodeID;
+};
+
+} // namespace ranger
+
+#endif /* TREE_H_ */
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/TreeClassification.cpp
@@ -0,0 +1,848 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#include <unordered_map>
+#include <random>
+#include <algorithm>
+#include <iostream>
+#include <iterator>
+
+#include "TreeClassification.h"
+#include "utility.h"
+#include "Data.h"
+
+namespace ranger {
+
+TreeClassification::TreeClassification(std::vector<double>* class_values, std::vector<uint>* response_classIDs,
+    std::vector<std::vector<size_t>>* sampleIDs_per_class, std::vector<double>* class_weights) :
+    class_values(class_values), response_classIDs(response_classIDs), sampleIDs_per_class(sampleIDs_per_class), class_weights(
+        class_weights), counter(0), counter_per_class(0) {
+}
+
+TreeClassification::TreeClassification(std::vector<std::vector<size_t>>& child_nodeIDs,
+    std::vector<size_t>& split_varIDs, std::vector<double>& split_values, std::vector<double>* class_values,
+    std::vector<uint>* response_classIDs) :
+    Tree(child_nodeIDs, split_varIDs, split_values), class_values(class_values), response_classIDs(response_classIDs), sampleIDs_per_class(
+        0), class_weights(0), counter { }, counter_per_class { } {
+}
+
+void TreeClassification::allocateMemory() {
+  // Init counters if not in memory efficient mode
+  if (!memory_saving_splitting) {
+    size_t num_classes = class_values->size();
+    size_t max_num_splits = data->getMaxNumUniqueValues();
+
+    // Use number of random splits for extratrees
+    if (splitrule == EXTRATREES && num_random_splits > max_num_splits) {
+      max_num_splits = num_random_splits;
+    }
+
+    counter.resize(max_num_splits);
+    counter_per_class.resize(num_classes * max_num_splits);
+  }
+}
+
+double TreeClassification::estimate(size_t nodeID) {
+
+  // Count classes over samples in node and return class with maximum count
+  std::vector<double> class_count = std::vector<double>(class_values->size(), 0.0);
+
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    size_t value = (*response_classIDs)[sampleID];
+    class_count[value] += (*class_weights)[value];
+  }
+
+  if (end_pos[nodeID] > start_pos[nodeID]) {
+    size_t result_classID = mostFrequentClass(class_count, random_number_generator);
+    return ((*class_values)[result_classID]);
+  } else {
+    throw std::runtime_error("Error: Empty node.");
+  }
+
+}
+
+void TreeClassification::appendToFileInternal(std::ofstream& file) { // #nocov start
+  // Empty on purpose
+} // #nocov end
+
+bool TreeClassification::splitNodeInternal(size_t nodeID, std::vector<size_t>& possible_split_varIDs) {
+
+  // Stop if maximum node size or depth reached
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+  if (num_samples_node <= min_node_size || (nodeID >= last_left_nodeID && max_depth > 0 && depth >= max_depth)) {
+    split_values[nodeID] = estimate(nodeID);
+    return true;
+  }
+
+  // Check if node is pure and set split_value to estimate and stop if pure
+  bool pure = true;
+  double pure_value = 0;
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    double value = data->get_y(sampleID, 0);
+    if (pos != start_pos[nodeID] && value != pure_value) {
+      pure = false;
+      break;
+    }
+    pure_value = value;
+  }
+  if (pure) {
+    split_values[nodeID] = pure_value;
+    return true;
+  }
+
+  // Find best split, stop if no decrease of impurity
+  bool stop;
+  if (splitrule == EXTRATREES) {
+    stop = findBestSplitExtraTrees(nodeID, possible_split_varIDs);
+  } else {
+    stop = findBestSplit(nodeID, possible_split_varIDs);
+  }
+
+  if (stop) {
+    split_values[nodeID] = estimate(nodeID);
+    return true;
+  }
+
+  return false;
+}
+
+void TreeClassification::createEmptyNodeInternal() {
+  // Empty on purpose
+}
+
+double TreeClassification::computePredictionAccuracyInternal(std::vector<double>* prediction_error_casewise) {
+
+  size_t num_predictions = prediction_terminal_nodeIDs.size();
+  size_t num_missclassifications = 0;
+  for (size_t i = 0; i < num_predictions; ++i) {
+    size_t terminal_nodeID = prediction_terminal_nodeIDs[i];
+    double predicted_value = split_values[terminal_nodeID];
+    double real_value = data->get_y(oob_sampleIDs[i], 0);
+    if (predicted_value != real_value) {
+      ++num_missclassifications;
+      if (prediction_error_casewise) {
+        (*prediction_error_casewise)[i] = 1;
+      }
+    } else {
+      if (prediction_error_casewise) {
+        (*prediction_error_casewise)[i] = 0;
+      }
+    }
+  }
+  return (1.0 - (double) num_missclassifications / (double) num_predictions);
+}
+
+bool TreeClassification::findBestSplit(size_t nodeID, std::vector<size_t>& possible_split_varIDs) {
+
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+  size_t num_classes = class_values->size();
+  double best_decrease = -1;
+  size_t best_varID = 0;
+  double best_value = 0;
+
+  std::vector<size_t> class_counts(num_classes);
+  // Compute overall class counts
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    uint sample_classID = (*response_classIDs)[sampleID];
+    ++class_counts[sample_classID];
+  }
+
+  // For all possible split variables
+  for (auto& varID : possible_split_varIDs) {
+    // Find best split value, if ordered consider all values as split values, else all 2-partitions
+    if (data->isOrderedVariable(varID)) {
+
+      // Use memory saving method if option set
+      if (memory_saving_splitting) {
+        findBestSplitValueSmallQ(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+            best_decrease);
+      } else {
+        // Use faster method for both cases
+        double q = (double) num_samples_node / (double) data->getNumUniqueDataValues(varID);
+        if (q < Q_THRESHOLD) {
+          findBestSplitValueSmallQ(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+              best_decrease);
+        } else {
+          findBestSplitValueLargeQ(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+              best_decrease);
+        }
+      }
+    } else {
+      findBestSplitValueUnordered(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+          best_decrease);
+    }
+  }
+
+  // Stop if no good split found
+  if (best_decrease < 0) {
+    return true;
+  }
+
+  // Save best values
+  split_varIDs[nodeID] = best_varID;
+  split_values[nodeID] = best_value;
+
+  // Compute gini index for this node and to variable importance if needed
+  if (importance_mode == IMP_GINI || importance_mode == IMP_GINI_CORRECTED) {
+    addGiniImportance(nodeID, best_varID, best_decrease);
+  }
+
+  // Regularization
+  saveSplitVarID(best_varID);
+
+  return false;
+}
+
+void TreeClassification::findBestSplitValueSmallQ(size_t nodeID, size_t varID, size_t num_classes,
+    const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+    double& best_decrease) {
+
+// Create possible split values
+  std::vector<double> possible_split_values;
+  data->getAllValues(possible_split_values, sampleIDs, varID, start_pos[nodeID], end_pos[nodeID]);
+
+  // Try next variable if all equal for this
+  if (possible_split_values.size() < 2) {
+    return;
+  }
+
+  const size_t num_splits = possible_split_values.size();
+  if (memory_saving_splitting) {
+    std::vector<size_t> class_counts_right(num_splits * num_classes), n_right(num_splits);
+    findBestSplitValueSmallQ(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+        best_decrease, possible_split_values, class_counts_right, n_right);
+  } else {
+    std::fill_n(counter_per_class.begin(), num_splits * num_classes, 0);
+    std::fill_n(counter.begin(), num_splits, 0);
+    findBestSplitValueSmallQ(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+        best_decrease, possible_split_values, counter_per_class, counter);
+  }
+}
+
+void TreeClassification::findBestSplitValueSmallQ(size_t nodeID, size_t varID, size_t num_classes,
+    const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+    double& best_decrease, const std::vector<double>& possible_split_values, std::vector<size_t>& counter_per_class,
+    std::vector<size_t>& counter) {
+
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    uint sample_classID = (*response_classIDs)[sampleID];
+    size_t idx = std::lower_bound(possible_split_values.begin(), possible_split_values.end(),
+        data->get_x(sampleID, varID)) - possible_split_values.begin();
+
+    ++counter_per_class[idx * num_classes + sample_classID];
+    ++counter[idx];
+  }
+
+  size_t n_left = 0;
+  std::vector<size_t> class_counts_left(num_classes);
+
+  // Compute decrease of impurity for each split
+  for (size_t i = 0; i < possible_split_values.size() - 1; ++i) {
+
+    // Stop if nothing here
+    if (counter[i] == 0) {
+      continue;
+    }
+
+    n_left += counter[i];
+
+    // Stop if right child empty
+    size_t n_right = num_samples_node - n_left;
+    if (n_right == 0) {
+      break;
+    }
+
+    double decrease;
+    if (splitrule == HELLINGER) {
+      for (size_t j = 0; j < num_classes; ++j) {
+        class_counts_left[j] += counter_per_class[i * num_classes + j];
+      }
+
+      // TPR is number of outcome 1s in one node / total number of 1s
+      // FPR is number of outcome 0s in one node / total number of 0s
+      double tpr = (double) (class_counts[1] - class_counts_left[1]) / (double) class_counts[1];
+      double fpr = (double) (class_counts[0] - class_counts_left[0]) / (double) class_counts[0];
+
+      // Decrease of impurity
+      double a1 = sqrt(tpr) - sqrt(fpr);
+      double a2 = sqrt(1 - tpr) - sqrt(1 - fpr);
+      decrease = sqrt(a1 * a1 + a2 * a2);
+    } else {
+      // Sum of squares
+      double sum_left = 0;
+      double sum_right = 0;
+      for (size_t j = 0; j < num_classes; ++j) {
+        class_counts_left[j] += counter_per_class[i * num_classes + j];
+        size_t class_count_right = class_counts[j] - class_counts_left[j];
+
+        sum_left += (*class_weights)[j] * class_counts_left[j] * class_counts_left[j];
+        sum_right += (*class_weights)[j] * class_count_right * class_count_right;
+      }
+
+      // Decrease of impurity
+      decrease = sum_right / (double) n_right + sum_left / (double) n_left;
+    }
+
+    // Regularization
+    regularize(decrease, varID);
+
+    // If better than before, use this
+    if (decrease > best_decrease) {
+      // Use mid-point split
+      best_value = (possible_split_values[i] + possible_split_values[i + 1]) / 2;
+      best_varID = varID;
+      best_decrease = decrease;
+
+      // Use smaller value if average is numerically the same as the larger value
+      if (best_value == possible_split_values[i + 1]) {
+        best_value = possible_split_values[i];
+      }
+    }
+  }
+}
+
+void TreeClassification::findBestSplitValueLargeQ(size_t nodeID, size_t varID, size_t num_classes,
+    const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+    double& best_decrease) {
+
+  // Set counters to 0
+  size_t num_unique = data->getNumUniqueDataValues(varID);
+  std::fill_n(counter_per_class.begin(), num_unique * num_classes, 0);
+  std::fill_n(counter.begin(), num_unique, 0);
+
+  // Count values
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    size_t index = data->getIndex(sampleID, varID);
+    size_t classID = (*response_classIDs)[sampleID];
+
+    ++counter[index];
+    ++counter_per_class[index * num_classes + classID];
+  }
+
+  size_t n_left = 0;
+  std::vector<size_t> class_counts_left(num_classes);
+
+  // Compute decrease of impurity for each split
+  for (size_t i = 0; i < num_unique - 1; ++i) {
+
+    // Stop if nothing here
+    if (counter[i] == 0) {
+      continue;
+    }
+
+    n_left += counter[i];
+
+    // Stop if right child empty
+    size_t n_right = num_samples_node - n_left;
+    if (n_right == 0) {
+      break;
+    }
+
+    double decrease;
+    if (splitrule == HELLINGER) {
+      for (size_t j = 0; j < num_classes; ++j) {
+        class_counts_left[j] += counter_per_class[i * num_classes + j];
+      }
+
+      // TPR is number of outcome 1s in one node / total number of 1s
+      // FPR is number of outcome 0s in one node / total number of 0s
+      double tpr = (double) (class_counts[1] - class_counts_left[1]) / (double) class_counts[1];
+      double fpr = (double) (class_counts[0] - class_counts_left[0]) / (double) class_counts[0];
+
+      // Decrease of impurity
+      double a1 = sqrt(tpr) - sqrt(fpr);
+      double a2 = sqrt(1 - tpr) - sqrt(1 - fpr);
+      decrease = sqrt(a1 * a1 + a2 * a2);
+    } else {
+      // Sum of squares
+      double sum_left = 0;
+      double sum_right = 0;
+      for (size_t j = 0; j < num_classes; ++j) {
+        class_counts_left[j] += counter_per_class[i * num_classes + j];
+        size_t class_count_right = class_counts[j] - class_counts_left[j];
+
+        sum_left += (*class_weights)[j] * class_counts_left[j] * class_counts_left[j];
+        sum_right += (*class_weights)[j] * class_count_right * class_count_right;
+      }
+
+      // Decrease of impurity
+      decrease = sum_right / (double) n_right + sum_left / (double) n_left;
+    }
+
+    // Regularization
+    regularize(decrease, varID);
+
+    // If better than before, use this
+    if (decrease > best_decrease) {
+      // Find next value in this node
+      size_t j = i + 1;
+      while (j < num_unique && counter[j] == 0) {
+        ++j;
+      }
+
+      // Use mid-point split
+      best_value = (data->getUniqueDataValue(varID, i) + data->getUniqueDataValue(varID, j)) / 2;
+      best_varID = varID;
+      best_decrease = decrease;
+
+      // Use smaller value if average is numerically the same as the larger value
+      if (best_value == data->getUniqueDataValue(varID, j)) {
+        best_value = data->getUniqueDataValue(varID, i);
+      }
+    }
+  }
+}
+
+void TreeClassification::findBestSplitValueUnordered(size_t nodeID, size_t varID, size_t num_classes,
+    const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+    double& best_decrease) {
+
+  // Create possible split values
+  std::vector<double> factor_levels;
+  data->getAllValues(factor_levels, sampleIDs, varID, start_pos[nodeID], end_pos[nodeID]);
+
+  // Try next variable if all equal for this
+  if (factor_levels.size() < 2) {
+    return;
+  }
+
+  // Number of possible splits is 2^num_levels
+  size_t num_splits = (1ULL << factor_levels.size());
+
+  // Compute decrease of impurity for each possible split
+  // Split where all left (0) or all right (1) are excluded
+  // The second half of numbers is just left/right switched the first half -> Exclude second half
+  for (size_t local_splitID = 1; local_splitID < num_splits / 2; ++local_splitID) {
+
+    // Compute overall splitID by shifting local factorIDs to global positions
+    size_t splitID = 0;
+    for (size_t j = 0; j < factor_levels.size(); ++j) {
+      if ((local_splitID & (1ULL << j))) {
+        double level = factor_levels[j];
+        size_t factorID = floor(level) - 1;
+        splitID = splitID | (1ULL << factorID);
+      }
+    }
+
+    // Initialize
+    std::vector<size_t> class_counts_right(num_classes);
+    size_t n_right = 0;
+
+    // Count classes in left and right child
+    for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+      size_t sampleID = sampleIDs[pos];
+      uint sample_classID = (*response_classIDs)[sampleID];
+      double value = data->get_x(sampleID, varID);
+      size_t factorID = floor(value) - 1;
+
+      // If in right child, count
+      // In right child, if bitwise splitID at position factorID is 1
+      if ((splitID & (1ULL << factorID))) {
+        ++n_right;
+        ++class_counts_right[sample_classID];
+      }
+    }
+    size_t n_left = num_samples_node - n_right;
+
+    double decrease;
+    if (splitrule == HELLINGER) {
+      // TPR is number of outcome 1s in one node / total number of 1s
+      // FPR is number of outcome 0s in one node / total number of 0s
+      double tpr = (double) class_counts_right[1] / (double) class_counts[1];
+      double fpr = (double) class_counts_right[0] / (double) class_counts[0];
+
+      // Decrease of impurity
+      double a1 = sqrt(tpr) - sqrt(fpr);
+      double a2 = sqrt(1 - tpr) - sqrt(1 - fpr);
+      decrease = sqrt(a1 * a1 + a2 * a2);
+    } else {
+      // Sum of squares
+      double sum_left = 0;
+      double sum_right = 0;
+      for (size_t j = 0; j < num_classes; ++j) {
+        size_t class_count_right = class_counts_right[j];
+        size_t class_count_left = class_counts[j] - class_count_right;
+
+        sum_right += (*class_weights)[j] * class_count_right * class_count_right;
+        sum_left += (*class_weights)[j] * class_count_left * class_count_left;
+      }
+
+      // Decrease of impurity
+      decrease = sum_left / (double) n_left + sum_right / (double) n_right;
+    }
+
+    // Regularization
+    regularize(decrease, varID);
+
+    // If better than before, use this
+    if (decrease > best_decrease) {
+      best_value = splitID;
+      best_varID = varID;
+      best_decrease = decrease;
+    }
+  }
+}
+
+bool TreeClassification::findBestSplitExtraTrees(size_t nodeID, std::vector<size_t>& possible_split_varIDs) {
+
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+  size_t num_classes = class_values->size();
+  double best_decrease = -1;
+  size_t best_varID = 0;
+  double best_value = 0;
+
+  std::vector<size_t> class_counts(num_classes);
+  // Compute overall class counts
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    uint sample_classID = (*response_classIDs)[sampleID];
+    ++class_counts[sample_classID];
+  }
+
+  // For all possible split variables
+  for (auto& varID : possible_split_varIDs) {
+    // Find best split value, if ordered consider all values as split values, else all 2-partitions
+    if (data->isOrderedVariable(varID)) {
+      findBestSplitValueExtraTrees(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+          best_decrease);
+    } else {
+      findBestSplitValueExtraTreesUnordered(nodeID, varID, num_classes, class_counts, num_samples_node, best_value,
+          best_varID, best_decrease);
+    }
+  }
+
+  // Stop if no good split found
+  if (best_decrease < 0) {
+    return true;
+  }
+
+  // Save best values
+  split_varIDs[nodeID] = best_varID;
+  split_values[nodeID] = best_value;
+
+  // Compute gini index for this node and to variable importance if needed
+  if (importance_mode == IMP_GINI || importance_mode == IMP_GINI_CORRECTED) {
+    addGiniImportance(nodeID, best_varID, best_decrease);
+  }
+
+  // Regularization
+  saveSplitVarID(best_varID);
+
+  return false;
+}
+
+void TreeClassification::findBestSplitValueExtraTrees(size_t nodeID, size_t varID, size_t num_classes,
+    const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+    double& best_decrease) {
+
+  // Get min/max values of covariate in node
+  double min;
+  double max;
+  data->getMinMaxValues(min, max, sampleIDs, varID, start_pos[nodeID], end_pos[nodeID]);
+
+  // Try next variable if all equal for this
+  if (min == max) {
+    return;
+  }
+
+  // Create possible split values: Draw randomly between min and max
+  std::vector<double> possible_split_values;
+  std::uniform_real_distribution<double> udist(min, max);
+  possible_split_values.reserve(num_random_splits);
+  for (size_t i = 0; i < num_random_splits; ++i) {
+    possible_split_values.push_back(udist(random_number_generator));
+  }
+  if (num_random_splits > 1) {
+    std::sort(possible_split_values.begin(), possible_split_values.end());
+  }
+
+  const size_t num_splits = possible_split_values.size();
+  if (memory_saving_splitting) {
+    std::vector<size_t> class_counts_right(num_splits * num_classes), n_right(num_splits);
+    findBestSplitValueExtraTrees(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+        best_decrease, possible_split_values, class_counts_right, n_right);
+  } else {
+    std::fill_n(counter_per_class.begin(), num_splits * num_classes, 0);
+    std::fill_n(counter.begin(), num_splits, 0);
+    findBestSplitValueExtraTrees(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+        best_decrease, possible_split_values, counter_per_class, counter);
+  }
+}
+
+void TreeClassification::findBestSplitValueExtraTrees(size_t nodeID, size_t varID, size_t num_classes,
+    const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+    double& best_decrease, const std::vector<double>& possible_split_values, std::vector<size_t>& class_counts_right,
+    std::vector<size_t>& n_right) {
+  const size_t num_splits = possible_split_values.size();
+
+  // Count samples in right child per class and possbile split
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    double value = data->get_x(sampleID, varID);
+    uint sample_classID = (*response_classIDs)[sampleID];
+
+    // Count samples until split_value reached
+    for (size_t i = 0; i < num_splits; ++i) {
+      if (value > possible_split_values[i]) {
+        ++n_right[i];
+        ++class_counts_right[i * num_classes + sample_classID];
+      } else {
+        break;
+      }
+    }
+  }
+
+  // Compute decrease of impurity for each possible split
+  for (size_t i = 0; i < num_splits; ++i) {
+
+    // Stop if one child empty
+    size_t n_left = num_samples_node - n_right[i];
+    if (n_left == 0 || n_right[i] == 0) {
+      continue;
+    }
+
+    // Sum of squares
+    double sum_left = 0;
+    double sum_right = 0;
+    for (size_t j = 0; j < num_classes; ++j) {
+      size_t class_count_right = class_counts_right[i * num_classes + j];
+      size_t class_count_left = class_counts[j] - class_count_right;
+
+      sum_right += (*class_weights)[j] * class_count_right * class_count_right;
+      sum_left += (*class_weights)[j] * class_count_left * class_count_left;
+    }
+
+    // Decrease of impurity
+    double decrease = sum_left / (double) n_left + sum_right / (double) n_right[i];
+
+    // Regularization
+    regularize(decrease, varID);
+
+    // If better than before, use this
+    if (decrease > best_decrease) {
+      best_value = possible_split_values[i];
+      best_varID = varID;
+      best_decrease = decrease;
+    }
+  }
+}
+
+void TreeClassification::findBestSplitValueExtraTreesUnordered(size_t nodeID, size_t varID, size_t num_classes,
+    const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+    double& best_decrease) {
+
+  size_t num_unique_values = data->getNumUniqueDataValues(varID);
+
+  // Get all factor indices in node
+  std::vector<bool> factor_in_node(num_unique_values, false);
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    size_t index = data->getIndex(sampleID, varID);
+    factor_in_node[index] = true;
+  }
+
+  // Vector of indices in and out of node
+  std::vector<size_t> indices_in_node;
+  std::vector<size_t> indices_out_node;
+  indices_in_node.reserve(num_unique_values);
+  indices_out_node.reserve(num_unique_values);
+  for (size_t i = 0; i < num_unique_values; ++i) {
+    if (factor_in_node[i]) {
+      indices_in_node.push_back(i);
+    } else {
+      indices_out_node.push_back(i);
+    }
+  }
+
+  // Generate num_random_splits splits
+  for (size_t i = 0; i < num_random_splits; ++i) {
+    std::vector<size_t> split_subset;
+    split_subset.reserve(num_unique_values);
+
+    // Draw random subsets, sample all partitions with equal probability
+    if (indices_in_node.size() > 1) {
+      size_t num_partitions = (2ULL << (indices_in_node.size() - 1ULL)) - 2ULL; // 2^n-2 (don't allow full or empty)
+      std::uniform_int_distribution<size_t> udist(1, num_partitions);
+      size_t splitID_in_node = udist(random_number_generator);
+      for (size_t j = 0; j < indices_in_node.size(); ++j) {
+        if ((splitID_in_node & (1ULL << j)) > 0) {
+          split_subset.push_back(indices_in_node[j]);
+        }
+      }
+    }
+    if (indices_out_node.size() > 1) {
+      size_t num_partitions = (2ULL << (indices_out_node.size() - 1ULL)) - 1ULL; // 2^n-1 (allow full or empty)
+      std::uniform_int_distribution<size_t> udist(0, num_partitions);
+      size_t splitID_out_node = udist(random_number_generator);
+      for (size_t j = 0; j < indices_out_node.size(); ++j) {
+        if ((splitID_out_node & (1ULL << j)) > 0) {
+          split_subset.push_back(indices_out_node[j]);
+        }
+      }
+    }
+
+    // Assign union of the two subsets to right child
+    size_t splitID = 0;
+    for (auto& idx : split_subset) {
+      splitID |= 1ULL << idx;
+    }
+
+    // Initialize
+    std::vector<size_t> class_counts_right(num_classes);
+    size_t n_right = 0;
+
+    // Count classes in left and right child
+    for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+      size_t sampleID = sampleIDs[pos];
+      uint sample_classID = (*response_classIDs)[sampleID];
+      double value = data->get_x(sampleID, varID);
+      size_t factorID = floor(value) - 1;
+
+      // If in right child, count
+      // In right child, if bitwise splitID at position factorID is 1
+      if ((splitID & (1ULL << factorID))) {
+        ++n_right;
+        ++class_counts_right[sample_classID];
+      }
+    }
+    size_t n_left = num_samples_node - n_right;
+
+    // Sum of squares
+    double sum_left = 0;
+    double sum_right = 0;
+    for (size_t j = 0; j < num_classes; ++j) {
+      size_t class_count_right = class_counts_right[j];
+      size_t class_count_left = class_counts[j] - class_count_right;
+
+      sum_right += (*class_weights)[j] * class_count_right * class_count_right;
+      sum_left += (*class_weights)[j] * class_count_left * class_count_left;
+    }
+
+    // Decrease of impurity
+    double decrease = sum_left / (double) n_left + sum_right / (double) n_right;
+
+    // Regularization
+    regularize(decrease, varID);
+
+    // If better than before, use this
+    if (decrease > best_decrease) {
+      best_value = splitID;
+      best_varID = varID;
+      best_decrease = decrease;
+    }
+  }
+}
+
+void TreeClassification::addGiniImportance(size_t nodeID, size_t varID, double decrease) {
+
+  double best_decrease = decrease;
+  if (splitrule != HELLINGER) {
+    size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+    std::vector<size_t> class_counts;
+    class_counts.resize(class_values->size(), 0);
+
+    for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+        size_t sampleID = sampleIDs[pos];
+        uint sample_classID = (*response_classIDs)[sampleID];
+        class_counts[sample_classID]++;
+    }
+    double sum_node = 0;
+    for (size_t i = 0; i < class_counts.size(); ++i) {
+      sum_node += (*class_weights)[i] * class_counts[i] * class_counts[i];
+    }
+
+    double impurity_node = (sum_node / (double) num_samples_node);
+
+    // Account for the regularization
+    regularize(impurity_node, varID);
+
+    best_decrease = decrease - impurity_node;
+  }
+
+  // No variable importance for no split variables
+  size_t tempvarID = data->getUnpermutedVarID(varID);
+
+  // Subtract if corrected importance and permuted variable, else add
+  if (importance_mode == IMP_GINI_CORRECTED && varID >= data->getNumCols()) {
+    (*variable_importance)[tempvarID] -= best_decrease;
+  } else {
+    (*variable_importance)[tempvarID] += best_decrease;
+  }
+}
+
+void TreeClassification::bootstrapClassWise() {
+  // Number of samples is sum of sample fraction * number of samples
+  size_t num_samples_inbag = 0;
+  double sum_sample_fraction = 0;
+  for (auto& s : *sample_fraction) {
+    num_samples_inbag += (size_t) num_samples * s;
+    sum_sample_fraction += s;
+  }
+
+  // Reserve space, reserve a little more to be save)
+  sampleIDs.reserve(num_samples_inbag);
+  oob_sampleIDs.reserve(num_samples * (exp(-sum_sample_fraction) + 0.1));
+
+  // Start with all samples OOB
+  inbag_counts.resize(num_samples, 0);
+
+  // Draw samples for each class
+  for (size_t i = 0; i < sample_fraction->size(); ++i) {
+    // Draw samples of class with replacement as inbag and mark as not OOB
+    size_t num_samples_class = (*sampleIDs_per_class)[i].size();
+    size_t num_samples_inbag_class = round(num_samples * (*sample_fraction)[i]);
+    std::uniform_int_distribution<size_t> unif_dist(0, num_samples_class - 1);
+    for (size_t s = 0; s < num_samples_inbag_class; ++s) {
+      size_t draw = (*sampleIDs_per_class)[i][unif_dist(random_number_generator)];
+      sampleIDs.push_back(draw);
+      ++inbag_counts[draw];
+    }
+  }
+
+  // Save OOB samples
+  for (size_t s = 0; s < inbag_counts.size(); ++s) {
+    if (inbag_counts[s] == 0) {
+      oob_sampleIDs.push_back(s);
+    }
+  }
+  num_samples_oob = oob_sampleIDs.size();
+
+  if (!keep_inbag) {
+    inbag_counts.clear();
+    inbag_counts.shrink_to_fit();
+  }
+}
+
+void TreeClassification::bootstrapWithoutReplacementClassWise() {
+  // Draw samples for each class
+  for (size_t i = 0; i < sample_fraction->size(); ++i) {
+    size_t num_samples_class = (*sampleIDs_per_class)[i].size();
+    size_t num_samples_inbag_class = round(num_samples * (*sample_fraction)[i]);
+
+    shuffleAndSplitAppend(sampleIDs, oob_sampleIDs, num_samples_class, num_samples_inbag_class,
+        (*sampleIDs_per_class)[i], random_number_generator);
+  }
+
+  if (keep_inbag) {
+    // All observation are 0 or 1 times inbag
+    inbag_counts.resize(num_samples, 1);
+    for (size_t i = 0; i < oob_sampleIDs.size(); i++) {
+      inbag_counts[oob_sampleIDs[i]] = 0;
+    }
+  }
+}
+
+} // namespace ranger
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/TreeClassification.h
@@ -0,0 +1,111 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#ifndef TREECLASSIFICATION_H_
+#define TREECLASSIFICATION_H_
+
+#include <vector>
+
+#include "globals.h"
+#include "Tree.h"
+
+namespace ranger {
+
+class TreeClassification: public Tree {
+public:
+  TreeClassification(std::vector<double>* class_values, std::vector<uint>* response_classIDs,
+      std::vector<std::vector<size_t>>* sampleIDs_per_class, std::vector<double>* class_weights);
+
+  // Create from loaded forest
+  TreeClassification(std::vector<std::vector<size_t>>& child_nodeIDs, std::vector<size_t>& split_varIDs,
+      std::vector<double>& split_values, std::vector<double>* class_values, std::vector<uint>* response_classIDs);
+
+  TreeClassification(const TreeClassification&) = delete;
+  TreeClassification& operator=(const TreeClassification&) = delete;
+
+  virtual ~TreeClassification() override = default;
+
+  void allocateMemory() override;
+
+  double estimate(size_t nodeID);
+  void computePermutationImportanceInternal(std::vector<std::vector<size_t>>* permutations);
+  void appendToFileInternal(std::ofstream& file) override;
+
+  double getPrediction(size_t sampleID) const {
+    size_t terminal_nodeID = prediction_terminal_nodeIDs[sampleID];
+    return split_values[terminal_nodeID];
+  }
+
+  size_t getPredictionTerminalNodeID(size_t sampleID) const {
+    return prediction_terminal_nodeIDs[sampleID];
+  }
+
+private:
+  bool splitNodeInternal(size_t nodeID, std::vector<size_t>& possible_split_varIDs) override;
+  void createEmptyNodeInternal() override;
+
+  double computePredictionAccuracyInternal(std::vector<double>* prediction_error_casewise) override;
+
+  // Called by splitNodeInternal(). Sets split_varIDs and split_values.
+  bool findBestSplit(size_t nodeID, std::vector<size_t>& possible_split_varIDs);
+  void findBestSplitValueSmallQ(size_t nodeID, size_t varID, size_t num_classes,
+      const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+      double& best_decrease);
+  void findBestSplitValueSmallQ(size_t nodeID, size_t varID, size_t num_classes,
+      const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+      double& best_decrease, const std::vector<double>& possible_split_values, std::vector<size_t>& counter_per_class,
+      std::vector<size_t>& counter);
+  void findBestSplitValueLargeQ(size_t nodeID, size_t varID, size_t num_classes,
+      const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+      double& best_decrease);
+  void findBestSplitValueUnordered(size_t nodeID, size_t varID, size_t num_classes,
+      const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+      double& best_decrease);
+
+  bool findBestSplitExtraTrees(size_t nodeID, std::vector<size_t>& possible_split_varIDs);
+  void findBestSplitValueExtraTrees(size_t nodeID, size_t varID, size_t num_classes,
+      const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+      double& best_decrease);
+  void findBestSplitValueExtraTrees(size_t nodeID, size_t varID, size_t num_classes,
+      const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+      double& best_decrease, const std::vector<double>& possible_split_values, std::vector<size_t>& class_counts_right,
+      std::vector<size_t>& n_right);
+  void findBestSplitValueExtraTreesUnordered(size_t nodeID, size_t varID, size_t num_classes,
+      const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+      double& best_decrease);
+
+  void addGiniImportance(size_t nodeID, size_t varID, double decrease);
+
+  void bootstrapClassWise() override;
+  void bootstrapWithoutReplacementClassWise() override;
+
+  void cleanUpInternal() override {
+    counter.clear();
+    counter.shrink_to_fit();
+    counter_per_class.clear();
+    counter_per_class.shrink_to_fit();
+  }
+
+  // Classes of the dependent variable and classIDs for responses
+  const std::vector<double>* class_values;
+  const std::vector<uint>* response_classIDs;
+  const std::vector<std::vector<size_t>>* sampleIDs_per_class;
+
+  // Splitting weights
+  const std::vector<double>* class_weights;
+
+  std::vector<size_t> counter;
+  std::vector<size_t> counter_per_class;
+};
+
+} // namespace ranger
+
+#endif /* TREECLASSIFICATION_H_ */
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/TreeProbability.cpp
@@ -0,0 +1,842 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#include "TreeProbability.h"
+#include "utility.h"
+#include "Data.h"
+
+namespace ranger {
+
+TreeProbability::TreeProbability(std::vector<double>* class_values, std::vector<uint>* response_classIDs,
+    std::vector<std::vector<size_t>>* sampleIDs_per_class, std::vector<double>* class_weights) :
+    class_values(class_values), response_classIDs(response_classIDs), sampleIDs_per_class(sampleIDs_per_class), class_weights(
+        class_weights), counter(0), counter_per_class(0) {
+}
+
+TreeProbability::TreeProbability(std::vector<std::vector<size_t>>& child_nodeIDs, std::vector<size_t>& split_varIDs,
+    std::vector<double>& split_values, std::vector<double>* class_values, std::vector<uint>* response_classIDs,
+    std::vector<std::vector<double>>& terminal_class_counts) :
+    Tree(child_nodeIDs, split_varIDs, split_values), class_values(class_values), response_classIDs(response_classIDs), sampleIDs_per_class(
+        0), terminal_class_counts(terminal_class_counts), class_weights(0), counter(0), counter_per_class(0) {
+}
+
+void TreeProbability::allocateMemory() {
+  // Init counters if not in memory efficient mode
+  if (!memory_saving_splitting) {
+    size_t num_classes = class_values->size();
+    size_t max_num_splits = data->getMaxNumUniqueValues();
+
+    // Use number of random splits for extratrees
+    if (splitrule == EXTRATREES && num_random_splits > max_num_splits) {
+      max_num_splits = num_random_splits;
+    }
+
+    counter.resize(max_num_splits);
+    counter_per_class.resize(num_classes * max_num_splits);
+  }
+}
+
+void TreeProbability::addToTerminalNodes(size_t nodeID) {
+
+  size_t num_samples_in_node = end_pos[nodeID] - start_pos[nodeID];
+  terminal_class_counts[nodeID].resize(class_values->size(), 0);
+
+  // Compute counts
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    size_t classID = (*response_classIDs)[sampleID];
+    ++terminal_class_counts[nodeID][classID];
+  }
+
+  // Compute fractions
+  for (size_t i = 0; i < terminal_class_counts[nodeID].size(); ++i) {
+    terminal_class_counts[nodeID][i] /= num_samples_in_node;
+  }
+}
+
+void TreeProbability::appendToFileInternal(std::ofstream& file) { // #nocov start
+
+  // Add Terminal node class counts
+  // Convert to vector without empty elements and save
+  std::vector<size_t> terminal_nodes;
+  std::vector<std::vector<double>> terminal_class_counts_vector;
+  for (size_t i = 0; i < terminal_class_counts.size(); ++i) {
+    if (!terminal_class_counts[i].empty()) {
+      terminal_nodes.push_back(i);
+      terminal_class_counts_vector.push_back(terminal_class_counts[i]);
+    }
+  }
+  saveVector1D(terminal_nodes, file);
+  saveVector2D(terminal_class_counts_vector, file);
+} // #nocov end
+
+bool TreeProbability::splitNodeInternal(size_t nodeID, std::vector<size_t>& possible_split_varIDs) {
+
+  // Stop if maximum node size or depth reached
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+  if (num_samples_node <= min_node_size || (nodeID >= last_left_nodeID && max_depth > 0 && depth >= max_depth)) {
+    addToTerminalNodes(nodeID);
+    return true;
+  }
+
+  // Check if node is pure and set split_value to estimate and stop if pure
+  bool pure = true;
+  double pure_value = 0;
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    double value = data->get_y(sampleID, 0);
+    if (pos != start_pos[nodeID] && value != pure_value) {
+      pure = false;
+      break;
+    }
+    pure_value = value;
+  }
+  if (pure) {
+    addToTerminalNodes(nodeID);
+    return true;
+  }
+
+  // Find best split, stop if no decrease of impurity
+  bool stop;
+  if (splitrule == EXTRATREES) {
+    stop = findBestSplitExtraTrees(nodeID, possible_split_varIDs);
+  } else {
+    stop = findBestSplit(nodeID, possible_split_varIDs);
+  }
+
+  if (stop) {
+    addToTerminalNodes(nodeID);
+    return true;
+  }
+
+  return false;
+}
+
+void TreeProbability::createEmptyNodeInternal() {
+  terminal_class_counts.push_back(std::vector<double>());
+}
+
+double TreeProbability::computePredictionAccuracyInternal(std::vector<double>* prediction_error_casewise) {
+
+  size_t num_predictions = prediction_terminal_nodeIDs.size();
+  double sum_of_squares = 0;
+  for (size_t i = 0; i < num_predictions; ++i) {
+    size_t sampleID = oob_sampleIDs[i];
+    size_t real_classID = (*response_classIDs)[sampleID];
+    size_t terminal_nodeID = prediction_terminal_nodeIDs[i];
+    double predicted_value = terminal_class_counts[terminal_nodeID][real_classID];
+    double err = (1 - predicted_value) * (1 - predicted_value);
+    if (prediction_error_casewise) {
+      (*prediction_error_casewise)[i] = err;
+    }
+    sum_of_squares += err;
+  }
+  return (1.0 - sum_of_squares / (double) num_predictions);
+}
+
+bool TreeProbability::findBestSplit(size_t nodeID, std::vector<size_t>& possible_split_varIDs) {
+
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+  size_t num_classes = class_values->size();
+  double best_decrease = -1;
+  size_t best_varID = 0;
+  double best_value = 0;
+
+  std::vector<size_t> class_counts(num_classes);
+  // Compute overall class counts
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    uint sample_classID = (*response_classIDs)[sampleID];
+    ++class_counts[sample_classID];
+  }
+
+  // For all possible split variables
+  for (auto& varID : possible_split_varIDs) {
+    // Find best split value, if ordered consider all values as split values, else all 2-partitions
+    if (data->isOrderedVariable(varID)) {
+
+      // Use memory saving method if option set
+      if (memory_saving_splitting) {
+        findBestSplitValueSmallQ(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+            best_decrease);
+      } else {
+        // Use faster method for both cases
+        double q = (double) num_samples_node / (double) data->getNumUniqueDataValues(varID);
+        if (q < Q_THRESHOLD) {
+          findBestSplitValueSmallQ(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+              best_decrease);
+        } else {
+          findBestSplitValueLargeQ(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+              best_decrease);
+        }
+      }
+    } else {
+      findBestSplitValueUnordered(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+          best_decrease);
+    }
+  }
+
+  // Stop if no good split found
+  if (best_decrease < 0) {
+    return true;
+  }
+
+  // Save best values
+  split_varIDs[nodeID] = best_varID;
+  split_values[nodeID] = best_value;
+
+  // Compute decrease of impurity for this node and add to variable importance if needed
+  if (importance_mode == IMP_GINI || importance_mode == IMP_GINI_CORRECTED) {
+    addImpurityImportance(nodeID, best_varID, best_decrease);
+  }
+
+  // Regularization
+  saveSplitVarID(best_varID);
+
+  return false;
+}
+
+void TreeProbability::findBestSplitValueSmallQ(size_t nodeID, size_t varID, size_t num_classes,
+    const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+    double& best_decrease) {
+
+  // Create possible split values
+  std::vector<double> possible_split_values;
+  data->getAllValues(possible_split_values, sampleIDs, varID, start_pos[nodeID], end_pos[nodeID]);
+
+  // Try next variable if all equal for this
+  if (possible_split_values.size() < 2) {
+    return;
+  }
+
+  const size_t num_splits = possible_split_values.size();
+  if (memory_saving_splitting) {
+    std::vector<size_t> class_counts_right(num_splits * num_classes), n_right(num_splits);
+    findBestSplitValueSmallQ(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+        best_decrease, possible_split_values, class_counts_right, n_right);
+  } else {
+    std::fill_n(counter_per_class.begin(), num_splits * num_classes, 0);
+    std::fill_n(counter.begin(), num_splits, 0);
+    findBestSplitValueSmallQ(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+        best_decrease, possible_split_values, counter_per_class, counter);
+  }
+}
+
+void TreeProbability::findBestSplitValueSmallQ(size_t nodeID, size_t varID, size_t num_classes,
+    const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+    double& best_decrease, const std::vector<double>& possible_split_values, std::vector<size_t>& counter_per_class,
+    std::vector<size_t>& counter) {
+
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    uint sample_classID = (*response_classIDs)[sampleID];
+    size_t idx = std::lower_bound(possible_split_values.begin(), possible_split_values.end(),
+        data->get_x(sampleID, varID)) - possible_split_values.begin();
+
+    ++counter_per_class[idx * num_classes + sample_classID];
+    ++counter[idx];
+  }
+
+  size_t n_left = 0;
+  std::vector<size_t> class_counts_left(num_classes);
+
+  // Compute decrease of impurity for each split
+  for (size_t i = 0; i < possible_split_values.size() - 1; ++i) {
+
+    // Stop if nothing here
+    if (counter[i] == 0) {
+      continue;
+    }
+
+    n_left += counter[i];
+
+    // Stop if right child empty
+    size_t n_right = num_samples_node - n_left;
+    if (n_right == 0) {
+      break;
+    }
+
+    double decrease;
+    if (splitrule == HELLINGER) {
+      for (size_t j = 0; j < num_classes; ++j) {
+        class_counts_left[j] += counter_per_class[i * num_classes + j];
+      }
+
+      // TPR is number of outcome 1s in one node / total number of 1s
+      // FPR is number of outcome 0s in one node / total number of 0s
+      double tpr = (double) (class_counts[1] - class_counts_left[1]) / (double) class_counts[1];
+      double fpr = (double) (class_counts[0] - class_counts_left[0]) / (double) class_counts[0];
+
+      // Decrease of impurity
+      double a1 = sqrt(tpr) - sqrt(fpr);
+      double a2 = sqrt(1 - tpr) - sqrt(1 - fpr);
+      decrease = sqrt(a1 * a1 + a2 * a2);
+    } else {
+      // Sum of squares
+      double sum_left = 0;
+      double sum_right = 0;
+      for (size_t j = 0; j < num_classes; ++j) {
+        class_counts_left[j] += counter_per_class[i * num_classes + j];
+        size_t class_count_right = class_counts[j] - class_counts_left[j];
+
+        sum_left += (*class_weights)[j] * class_counts_left[j] * class_counts_left[j];
+        sum_right += (*class_weights)[j] * class_count_right * class_count_right;
+      }
+
+      // Decrease of impurity
+      decrease = sum_right / (double) n_right + sum_left / (double) n_left;
+    }
+
+    // Regularization
+    regularize(decrease, varID);
+
+    // If better than before, use this
+    if (decrease > best_decrease) {
+      // Use mid-point split
+      best_value = (possible_split_values[i] + possible_split_values[i + 1]) / 2;
+      best_varID = varID;
+      best_decrease = decrease;
+
+      // Use smaller value if average is numerically the same as the larger value
+      if (best_value == possible_split_values[i + 1]) {
+        best_value = possible_split_values[i];
+      }
+    }
+  }
+}
+
+void TreeProbability::findBestSplitValueLargeQ(size_t nodeID, size_t varID, size_t num_classes,
+    const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+    double& best_decrease) {
+
+  // Set counters to 0
+  size_t num_unique = data->getNumUniqueDataValues(varID);
+  std::fill_n(counter_per_class.begin(), num_unique * num_classes, 0);
+  std::fill_n(counter.begin(), num_unique, 0);
+
+  // Count values
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    size_t index = data->getIndex(sampleID, varID);
+    size_t classID = (*response_classIDs)[sampleID];
+
+    ++counter[index];
+    ++counter_per_class[index * num_classes + classID];
+  }
+
+  size_t n_left = 0;
+  std::vector<size_t> class_counts_left(num_classes);
+
+  // Compute decrease of impurity for each split
+  for (size_t i = 0; i < num_unique - 1; ++i) {
+
+    // Stop if nothing here
+    if (counter[i] == 0) {
+      continue;
+    }
+
+    n_left += counter[i];
+
+    // Stop if right child empty
+    size_t n_right = num_samples_node - n_left;
+    if (n_right == 0) {
+      break;
+    }
+
+    double decrease;
+    if (splitrule == HELLINGER) {
+      for (size_t j = 0; j < num_classes; ++j) {
+        class_counts_left[j] += counter_per_class[i * num_classes + j];
+      }
+
+      // TPR is number of outcome 1s in one node / total number of 1s
+      // FPR is number of outcome 0s in one node / total number of 0s
+      double tpr = (double) (class_counts[1] - class_counts_left[1]) / (double) class_counts[1];
+      double fpr = (double) (class_counts[0] - class_counts_left[0]) / (double) class_counts[0];
+
+      // Decrease of impurity
+      double a1 = sqrt(tpr) - sqrt(fpr);
+      double a2 = sqrt(1 - tpr) - sqrt(1 - fpr);
+      decrease = sqrt(a1 * a1 + a2 * a2);
+    } else {
+      // Sum of squares
+      double sum_left = 0;
+      double sum_right = 0;
+      for (size_t j = 0; j < num_classes; ++j) {
+        class_counts_left[j] += counter_per_class[i * num_classes + j];
+        size_t class_count_right = class_counts[j] - class_counts_left[j];
+
+        sum_left += (*class_weights)[j] * class_counts_left[j] * class_counts_left[j];
+        sum_right += (*class_weights)[j] * class_count_right * class_count_right;
+      }
+
+      // Decrease of impurity
+      decrease = sum_right / (double) n_right + sum_left / (double) n_left;
+    }
+
+    // Regularization
+    regularize(decrease, varID);
+
+    // If better than before, use this
+    if (decrease > best_decrease) {
+      // Find next value in this node
+      size_t j = i + 1;
+      while (j < num_unique && counter[j] == 0) {
+        ++j;
+      }
+
+      // Use mid-point split
+      best_value = (data->getUniqueDataValue(varID, i) + data->getUniqueDataValue(varID, j)) / 2;
+      best_varID = varID;
+      best_decrease = decrease;
+
+      // Use smaller value if average is numerically the same as the larger value
+      if (best_value == data->getUniqueDataValue(varID, j)) {
+        best_value = data->getUniqueDataValue(varID, i);
+      }
+    }
+  }
+}
+
+void TreeProbability::findBestSplitValueUnordered(size_t nodeID, size_t varID, size_t num_classes,
+    const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+    double& best_decrease) {
+
+  // Create possible split values
+  std::vector<double> factor_levels;
+  data->getAllValues(factor_levels, sampleIDs, varID, start_pos[nodeID], end_pos[nodeID]);
+
+  // Try next variable if all equal for this
+  if (factor_levels.size() < 2) {
+    return;
+  }
+
+  // Number of possible splits is 2^num_levels
+  size_t num_splits = (1ULL << factor_levels.size());
+
+  // Compute decrease of impurity for each possible split
+  // Split where all left (0) or all right (1) are excluded
+  // The second half of numbers is just left/right switched the first half -> Exclude second half
+  for (size_t local_splitID = 1; local_splitID < num_splits / 2; ++local_splitID) {
+
+    // Compute overall splitID by shifting local factorIDs to global positions
+    size_t splitID = 0;
+    for (size_t j = 0; j < factor_levels.size(); ++j) {
+      if ((local_splitID & (1ULL << j))) {
+        double level = factor_levels[j];
+        size_t factorID = floor(level) - 1;
+        splitID = splitID | (1ULL << factorID);
+      }
+    }
+
+    // Initialize
+    std::vector<size_t> class_counts_right(num_classes);
+    size_t n_right = 0;
+
+    // Count classes in left and right child
+    for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+      size_t sampleID = sampleIDs[pos];
+      uint sample_classID = (*response_classIDs)[sampleID];
+      double value = data->get_x(sampleID, varID);
+      size_t factorID = floor(value) - 1;
+
+      // If in right child, count
+      // In right child, if bitwise splitID at position factorID is 1
+      if ((splitID & (1ULL << factorID))) {
+        ++n_right;
+        ++class_counts_right[sample_classID];
+      }
+    }
+    size_t n_left = num_samples_node - n_right;
+
+    double decrease;
+    if (splitrule == HELLINGER) {
+      // TPR is number of outcome 1s in one node / total number of 1s
+      // FPR is number of outcome 0s in one node / total number of 0s
+      double tpr = (double) class_counts_right[1] / (double) class_counts[1];
+      double fpr = (double) class_counts_right[0] / (double) class_counts[0];
+
+      // Decrease of impurity
+      double a1 = sqrt(tpr) - sqrt(fpr);
+      double a2 = sqrt(1 - tpr) - sqrt(1 - fpr);
+      decrease = sqrt(a1 * a1 + a2 * a2);
+    } else {
+      // Sum of squares
+      double sum_left = 0;
+      double sum_right = 0;
+      for (size_t j = 0; j < num_classes; ++j) {
+        size_t class_count_right = class_counts_right[j];
+        size_t class_count_left = class_counts[j] - class_count_right;
+
+        sum_right += (*class_weights)[j] * class_count_right * class_count_right;
+        sum_left += (*class_weights)[j] * class_count_left * class_count_left;
+      }
+
+      // Decrease of impurity
+      decrease = sum_left / (double) n_left + sum_right / (double) n_right;
+    }
+
+    // Regularization
+    regularize(decrease, varID);
+
+    // If better than before, use this
+    if (decrease > best_decrease) {
+      best_value = splitID;
+      best_varID = varID;
+      best_decrease = decrease;
+    }
+  }
+}
+
+bool TreeProbability::findBestSplitExtraTrees(size_t nodeID, std::vector<size_t>& possible_split_varIDs) {
+
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+  size_t num_classes = class_values->size();
+  double best_decrease = -1;
+  size_t best_varID = 0;
+  double best_value = 0;
+
+  std::vector<size_t> class_counts(num_classes);
+  // Compute overall class counts
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    uint sample_classID = (*response_classIDs)[sampleID];
+    ++class_counts[sample_classID];
+  }
+
+  // For all possible split variables
+  for (auto& varID : possible_split_varIDs) {
+    // Find best split value, if ordered consider all values as split values, else all 2-partitions
+    if (data->isOrderedVariable(varID)) {
+      findBestSplitValueExtraTrees(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+          best_decrease);
+    } else {
+      findBestSplitValueExtraTreesUnordered(nodeID, varID, num_classes, class_counts, num_samples_node, best_value,
+          best_varID, best_decrease);
+    }
+  }
+
+  // Stop if no good split found
+  if (best_decrease < 0) {
+    return true;
+  }
+
+  // Save best values
+  split_varIDs[nodeID] = best_varID;
+  split_values[nodeID] = best_value;
+
+  // Compute decrease of impurity for this node and add to variable importance if needed
+  if (importance_mode == IMP_GINI || importance_mode == IMP_GINI_CORRECTED) {
+    addImpurityImportance(nodeID, best_varID, best_decrease);
+  }
+
+  // Regularization
+  saveSplitVarID(best_varID);
+
+  return false;
+}
+
+void TreeProbability::findBestSplitValueExtraTrees(size_t nodeID, size_t varID, size_t num_classes,
+    const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+    double& best_decrease) {
+
+  // Get min/max values of covariate in node
+  double min;
+  double max;
+  data->getMinMaxValues(min, max, sampleIDs, varID, start_pos[nodeID], end_pos[nodeID]);
+
+  // Try next variable if all equal for this
+  if (min == max) {
+    return;
+  }
+
+  // Create possible split values: Draw randomly between min and max
+  std::vector<double> possible_split_values;
+  std::uniform_real_distribution<double> udist(min, max);
+  possible_split_values.reserve(num_random_splits);
+  for (size_t i = 0; i < num_random_splits; ++i) {
+    possible_split_values.push_back(udist(random_number_generator));
+  }
+  if (num_random_splits > 1) {
+    std::sort(possible_split_values.begin(), possible_split_values.end());
+  }
+
+  const size_t num_splits = possible_split_values.size();
+  if (memory_saving_splitting) {
+    std::vector<size_t> class_counts_right(num_splits * num_classes), n_right(num_splits);
+    findBestSplitValueExtraTrees(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+        best_decrease, possible_split_values, class_counts_right, n_right);
+  } else {
+    std::fill_n(counter_per_class.begin(), num_splits * num_classes, 0);
+    std::fill_n(counter.begin(), num_splits, 0);
+    findBestSplitValueExtraTrees(nodeID, varID, num_classes, class_counts, num_samples_node, best_value, best_varID,
+        best_decrease, possible_split_values, counter_per_class, counter);
+  }
+}
+
+void TreeProbability::findBestSplitValueExtraTrees(size_t nodeID, size_t varID, size_t num_classes,
+    const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+    double& best_decrease, const std::vector<double>& possible_split_values, std::vector<size_t>& class_counts_right,
+    std::vector<size_t>& n_right) {
+  const size_t num_splits = possible_split_values.size();
+
+  // Count samples in right child per class and possbile split
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    double value = data->get_x(sampleID, varID);
+    uint sample_classID = (*response_classIDs)[sampleID];
+
+    // Count samples until split_value reached
+    for (size_t i = 0; i < num_splits; ++i) {
+      if (value > possible_split_values[i]) {
+        ++n_right[i];
+        ++class_counts_right[i * num_classes + sample_classID];
+      } else {
+        break;
+      }
+    }
+  }
+
+  // Compute decrease of impurity for each possible split
+  for (size_t i = 0; i < num_splits; ++i) {
+
+    // Stop if one child empty
+    size_t n_left = num_samples_node - n_right[i];
+    if (n_left == 0 || n_right[i] == 0) {
+      continue;
+    }
+
+    // Sum of squares
+    double sum_left = 0;
+    double sum_right = 0;
+    for (size_t j = 0; j < num_classes; ++j) {
+      size_t class_count_right = class_counts_right[i * num_classes + j];
+      size_t class_count_left = class_counts[j] - class_count_right;
+
+      sum_right += (*class_weights)[j] * class_count_right * class_count_right;
+      sum_left += (*class_weights)[j] * class_count_left * class_count_left;
+    }
+
+    // Decrease of impurity
+    double decrease = sum_left / (double) n_left + sum_right / (double) n_right[i];
+
+    // Regularization
+    regularize(decrease, varID);
+
+    // If better than before, use this
+    if (decrease > best_decrease) {
+      best_value = possible_split_values[i];
+      best_varID = varID;
+      best_decrease = decrease;
+    }
+  }
+}
+
+void TreeProbability::findBestSplitValueExtraTreesUnordered(size_t nodeID, size_t varID, size_t num_classes,
+    const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+    double& best_decrease) {
+
+  size_t num_unique_values = data->getNumUniqueDataValues(varID);
+
+  // Get all factor indices in node
+  std::vector<bool> factor_in_node(num_unique_values, false);
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    size_t index = data->getIndex(sampleID, varID);
+    factor_in_node[index] = true;
+  }
+
+  // Vector of indices in and out of node
+  std::vector<size_t> indices_in_node;
+  std::vector<size_t> indices_out_node;
+  indices_in_node.reserve(num_unique_values);
+  indices_out_node.reserve(num_unique_values);
+  for (size_t i = 0; i < num_unique_values; ++i) {
+    if (factor_in_node[i]) {
+      indices_in_node.push_back(i);
+    } else {
+      indices_out_node.push_back(i);
+    }
+  }
+
+  // Generate num_random_splits splits
+  for (size_t i = 0; i < num_random_splits; ++i) {
+    std::vector<size_t> split_subset;
+    split_subset.reserve(num_unique_values);
+
+    // Draw random subsets, sample all partitions with equal probability
+    if (indices_in_node.size() > 1) {
+      size_t num_partitions = (2ULL << (indices_in_node.size() - 1ULL)) - 2ULL; // 2^n-2 (don't allow full or empty)
+      std::uniform_int_distribution<size_t> udist(1, num_partitions);
+      size_t splitID_in_node = udist(random_number_generator);
+      for (size_t j = 0; j < indices_in_node.size(); ++j) {
+        if ((splitID_in_node & (1ULL << j)) > 0) {
+          split_subset.push_back(indices_in_node[j]);
+        }
+      }
+    }
+    if (indices_out_node.size() > 1) {
+      size_t num_partitions = (2ULL << (indices_out_node.size() - 1ULL)) - 1ULL; // 2^n-1 (allow full or empty)
+      std::uniform_int_distribution<size_t> udist(0, num_partitions);
+      size_t splitID_out_node = udist(random_number_generator);
+      for (size_t j = 0; j < indices_out_node.size(); ++j) {
+        if ((splitID_out_node & (1ULL << j)) > 0) {
+          split_subset.push_back(indices_out_node[j]);
+        }
+      }
+    }
+
+    // Assign union of the two subsets to right child
+    size_t splitID = 0;
+    for (auto& idx : split_subset) {
+      splitID |= 1ULL << idx;
+    }
+
+    // Initialize
+    std::vector<size_t> class_counts_right(num_classes);
+    size_t n_right = 0;
+
+    // Count classes in left and right child
+    for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+      size_t sampleID = sampleIDs[pos];
+      uint sample_classID = (*response_classIDs)[sampleID];
+      double value = data->get_x(sampleID, varID);
+      size_t factorID = floor(value) - 1;
+
+      // If in right child, count
+      // In right child, if bitwise splitID at position factorID is 1
+      if ((splitID & (1ULL << factorID))) {
+        ++n_right;
+        ++class_counts_right[sample_classID];
+      }
+    }
+    size_t n_left = num_samples_node - n_right;
+
+    // Sum of squares
+    double sum_left = 0;
+    double sum_right = 0;
+    for (size_t j = 0; j < num_classes; ++j) {
+      size_t class_count_right = class_counts_right[j];
+      size_t class_count_left = class_counts[j] - class_count_right;
+
+      sum_right += (*class_weights)[j] * class_count_right * class_count_right;
+      sum_left += (*class_weights)[j] * class_count_left * class_count_left;
+    }
+
+    // Decrease of impurity
+    double decrease = sum_left / (double) n_left + sum_right / (double) n_right;
+
+    // Regularization
+    regularize(decrease, varID);
+
+    // If better than before, use this
+    if (decrease > best_decrease) {
+      best_value = splitID;
+      best_varID = varID;
+      best_decrease = decrease;
+    }
+  }
+}
+
+void TreeProbability::addImpurityImportance(size_t nodeID, size_t varID, double decrease) {
+
+  double best_decrease = decrease;
+  if (splitrule != HELLINGER) {
+    size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+    std::vector<size_t> class_counts;
+    class_counts.resize(class_values->size(), 0);
+
+    for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+      size_t sampleID = sampleIDs[pos];
+      uint sample_classID = (*response_classIDs)[sampleID];
+      class_counts[sample_classID]++;
+    }
+    double sum_node = 0;
+    for (size_t i = 0; i < class_counts.size(); ++i) {
+      sum_node += (*class_weights)[i] * class_counts[i] * class_counts[i];
+    }
+    best_decrease = decrease - sum_node / (double) num_samples_node;
+  }
+
+  // No variable importance for no split variables
+  size_t tempvarID = data->getUnpermutedVarID(varID);
+
+  // Subtract if corrected importance and permuted variable, else add
+  if (importance_mode == IMP_GINI_CORRECTED && varID >= data->getNumCols()) {
+    (*variable_importance)[tempvarID] -= best_decrease;
+  } else {
+    (*variable_importance)[tempvarID] += best_decrease;
+  }
+}
+
+void TreeProbability::bootstrapClassWise() {
+  // Number of samples is sum of sample fraction * number of samples
+  size_t num_samples_inbag = 0;
+  double sum_sample_fraction = 0;
+  for (auto& s : *sample_fraction) {
+    num_samples_inbag += (size_t) num_samples * s;
+    sum_sample_fraction += s;
+  }
+
+  // Reserve space, reserve a little more to be save)
+  sampleIDs.reserve(num_samples_inbag);
+  oob_sampleIDs.reserve(num_samples * (exp(-sum_sample_fraction) + 0.1));
+
+  // Start with all samples OOB
+  inbag_counts.resize(num_samples, 0);
+
+  // Draw samples for each class
+  for (size_t i = 0; i < sample_fraction->size(); ++i) {
+    // Draw samples of class with replacement as inbag and mark as not OOB
+    size_t num_samples_class = (*sampleIDs_per_class)[i].size();
+    size_t num_samples_inbag_class = round(num_samples * (*sample_fraction)[i]);
+    std::uniform_int_distribution<size_t> unif_dist(0, num_samples_class - 1);
+    for (size_t s = 0; s < num_samples_inbag_class; ++s) {
+      size_t draw = (*sampleIDs_per_class)[i][unif_dist(random_number_generator)];
+      sampleIDs.push_back(draw);
+      ++inbag_counts[draw];
+    }
+  }
+
+  // Save OOB samples
+  for (size_t s = 0; s < inbag_counts.size(); ++s) {
+    if (inbag_counts[s] == 0) {
+      oob_sampleIDs.push_back(s);
+    }
+  }
+  num_samples_oob = oob_sampleIDs.size();
+
+  if (!keep_inbag) {
+    inbag_counts.clear();
+    inbag_counts.shrink_to_fit();
+  }
+}
+
+void TreeProbability::bootstrapWithoutReplacementClassWise() {
+  // Draw samples for each class
+  for (size_t i = 0; i < sample_fraction->size(); ++i) {
+    size_t num_samples_class = (*sampleIDs_per_class)[i].size();
+    size_t num_samples_inbag_class = round(num_samples * (*sample_fraction)[i]);
+
+    shuffleAndSplitAppend(sampleIDs, oob_sampleIDs, num_samples_class, num_samples_inbag_class,
+        (*sampleIDs_per_class)[i], random_number_generator);
+  }
+
+  if (keep_inbag) {
+    // All observation are 0 or 1 times inbag
+    inbag_counts.resize(num_samples, 1);
+    for (size_t i = 0; i < oob_sampleIDs.size(); i++) {
+      inbag_counts[oob_sampleIDs[i]] = 0;
+    }
+  }
+}
+
+} // namespace ranger
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/TreeProbability.h
@@ -0,0 +1,120 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#ifndef TREEPROBABILITY_H_
+#define TREEPROBABILITY_H_
+
+#include <map>
+#include <vector>
+
+#include "globals.h"
+#include "Tree.h"
+
+namespace ranger {
+
+class TreeProbability: public Tree {
+public:
+  TreeProbability(std::vector<double>* class_values, std::vector<uint>* response_classIDs,
+      std::vector<std::vector<size_t>>* sampleIDs_per_class, std::vector<double>* class_weights);
+
+  // Create from loaded forest
+  TreeProbability(std::vector<std::vector<size_t>>& child_nodeIDs, std::vector<size_t>& split_varIDs,
+      std::vector<double>& split_values, std::vector<double>* class_values, std::vector<uint>* response_classIDs,
+      std::vector<std::vector<double>>& terminal_class_counts);
+
+  TreeProbability(const TreeProbability&) = delete;
+  TreeProbability& operator=(const TreeProbability&) = delete;
+
+  virtual ~TreeProbability() override = default;
+
+  void allocateMemory() override;
+
+  void addToTerminalNodes(size_t nodeID);
+  void computePermutationImportanceInternal(std::vector<std::vector<size_t>>* permutations);
+  void appendToFileInternal(std::ofstream& file) override;
+
+  const std::vector<double>& getPrediction(size_t sampleID) const {
+    size_t terminal_nodeID = prediction_terminal_nodeIDs[sampleID];
+    return terminal_class_counts[terminal_nodeID];
+  }
+
+  size_t getPredictionTerminalNodeID(size_t sampleID) const {
+    return prediction_terminal_nodeIDs[sampleID];
+  }
+
+  const std::vector<std::vector<double>>& getTerminalClassCounts() const {
+    return terminal_class_counts;
+  }
+
+private:
+  bool splitNodeInternal(size_t nodeID, std::vector<size_t>& possible_split_varIDs) override;
+  void createEmptyNodeInternal() override;
+
+  double computePredictionAccuracyInternal(std::vector<double>* prediction_error_casewise) override;
+  
+  // Called by splitNodeInternal(). Sets split_varIDs and split_values.
+  bool findBestSplit(size_t nodeID, std::vector<size_t>& possible_split_varIDs);
+  void findBestSplitValueSmallQ(size_t nodeID, size_t varID, size_t num_classes,
+      const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+      double& best_decrease);
+  void findBestSplitValueSmallQ(size_t nodeID, size_t varID, size_t num_classes,
+      const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+      double& best_decrease, const std::vector<double>& possible_split_values, std::vector<size_t>& counter_per_class,
+      std::vector<size_t>& counter);
+  void findBestSplitValueLargeQ(size_t nodeID, size_t varID, size_t num_classes,
+      const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+      double& best_decrease);
+  void findBestSplitValueUnordered(size_t nodeID, size_t varID, size_t num_classes,
+      const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+      double& best_decrease);
+
+  bool findBestSplitExtraTrees(size_t nodeID, std::vector<size_t>& possible_split_varIDs);
+  void findBestSplitValueExtraTrees(size_t nodeID, size_t varID, size_t num_classes,
+      const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+      double& best_decrease);
+  void findBestSplitValueExtraTrees(size_t nodeID, size_t varID, size_t num_classes,
+      const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+      double& best_decrease, const std::vector<double>& possible_split_values, std::vector<size_t>& class_counts_right,
+      std::vector<size_t>& n_right);
+  void findBestSplitValueExtraTreesUnordered(size_t nodeID, size_t varID, size_t num_classes,
+      const std::vector<size_t>& class_counts, size_t num_samples_node, double& best_value, size_t& best_varID,
+      double& best_decrease);
+
+  void addImpurityImportance(size_t nodeID, size_t varID, double decrease);
+
+  void bootstrapClassWise() override;
+  void bootstrapWithoutReplacementClassWise() override;
+
+  void cleanUpInternal() override {
+    counter.clear();
+    counter.shrink_to_fit();
+    counter_per_class.clear();
+    counter_per_class.shrink_to_fit();
+  }
+
+  // Classes of the dependent variable and classIDs for responses
+  const std::vector<double>* class_values;
+  const std::vector<uint>* response_classIDs;
+  const std::vector<std::vector<size_t>>* sampleIDs_per_class;
+
+  // Class counts in terminal nodes. Empty for non-terminal nodes.
+  std::vector<std::vector<double>> terminal_class_counts;
+
+  // Splitting weights
+  const std::vector<double>* class_weights;
+
+  std::vector<size_t> counter;
+  std::vector<size_t> counter_per_class;
+};
+
+} // namespace ranger
+
+#endif /* TREEPROBABILITY_H_ */
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/TreeRegression.cpp
@@ -0,0 +1,926 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#include <algorithm>
+#include <iostream>
+#include <iterator>
+
+#include <ctime>
+
+#include "utility.h"
+#include "TreeRegression.h"
+#include "Data.h"
+
+namespace ranger {
+
+TreeRegression::TreeRegression(std::vector<std::vector<size_t>>& child_nodeIDs, std::vector<size_t>& split_varIDs,
+    std::vector<double>& split_values) :
+    Tree(child_nodeIDs, split_varIDs, split_values), counter(0), sums(0) {
+}
+
+void TreeRegression::allocateMemory() {
+  // Init counters if not in memory efficient mode
+  if (!memory_saving_splitting) {
+    size_t max_num_splits = data->getMaxNumUniqueValues();
+
+    // Use number of random splits for extratrees
+    if (splitrule == EXTRATREES && num_random_splits > max_num_splits) {
+      max_num_splits = num_random_splits;
+    }
+
+    counter.resize(max_num_splits);
+    sums.resize(max_num_splits);
+  }
+}
+
+double TreeRegression::estimate(size_t nodeID) {
+
+// Mean of responses of samples in node
+  double sum_responses_in_node = 0;
+  size_t num_samples_in_node = end_pos[nodeID] - start_pos[nodeID];
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    sum_responses_in_node += data->get_y(sampleID, 0);
+  }
+  return (sum_responses_in_node / (double) num_samples_in_node);
+}
+
+void TreeRegression::appendToFileInternal(std::ofstream& file) { // #nocov start
+// Empty on purpose
+} // #nocov end
+
+bool TreeRegression::splitNodeInternal(size_t nodeID, std::vector<size_t>& possible_split_varIDs) {
+
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+
+  // Stop if maximum node size or depth reached
+  if (num_samples_node <= min_node_size || (nodeID >= last_left_nodeID && max_depth > 0 && depth >= max_depth)) {
+    split_values[nodeID] = estimate(nodeID);
+    return true;
+  }
+
+  // Check if node is pure and set split_value to estimate and stop if pure
+  bool pure = true;
+  double pure_value = 0;
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    double value = data->get_y(sampleID, 0);
+    if (pos != start_pos[nodeID] && value != pure_value) {
+      pure = false;
+      break;
+    }
+    pure_value = value;
+  }
+  if (pure) {
+    split_values[nodeID] = pure_value;
+    return true;
+  }
+
+  // Find best split, stop if no decrease of impurity
+  bool stop;
+  if (splitrule == MAXSTAT) {
+    stop = findBestSplitMaxstat(nodeID, possible_split_varIDs);
+  } else if (splitrule == EXTRATREES) {
+    stop = findBestSplitExtraTrees(nodeID, possible_split_varIDs);
+  } else if (splitrule == BETA) {
+    stop = findBestSplitBeta(nodeID, possible_split_varIDs);
+  } else {
+    stop = findBestSplit(nodeID, possible_split_varIDs);
+  }
+
+  if (stop) {
+    split_values[nodeID] = estimate(nodeID);
+    return true;
+  }
+
+  return false;
+}
+
+void TreeRegression::createEmptyNodeInternal() {
+// Empty on purpose
+}
+
+double TreeRegression::computePredictionAccuracyInternal(std::vector<double>* prediction_error_casewise) {
+
+  size_t num_predictions = prediction_terminal_nodeIDs.size();
+  double sum_of_squares = 0;
+  for (size_t i = 0; i < num_predictions; ++i) {
+    size_t terminal_nodeID = prediction_terminal_nodeIDs[i];
+    double predicted_value = split_values[terminal_nodeID];
+    double real_value = data->get_y(oob_sampleIDs[i], 0);
+    if (predicted_value != real_value) {
+      double diff = (predicted_value - real_value) * (predicted_value - real_value);
+      if (prediction_error_casewise) {
+        (*prediction_error_casewise)[i] = diff;
+      }
+      sum_of_squares += diff;
+    }
+  }
+  return (1.0 - sum_of_squares / (double) num_predictions);
+}
+
+bool TreeRegression::findBestSplit(size_t nodeID, std::vector<size_t>& possible_split_varIDs) {
+
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+  double best_decrease = -1;
+  size_t best_varID = 0;
+  double best_value = 0;
+
+  // Compute sum of responses in node
+  double sum_node = 0;
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    sum_node += data->get_y(sampleID, 0);
+  }
+
+  // For all possible split variables
+  for (auto& varID : possible_split_varIDs) {
+
+    // Find best split value, if ordered consider all values as split values, else all 2-partitions
+    if (data->isOrderedVariable(varID)) {
+
+      // Use memory saving method if option set
+      if (memory_saving_splitting) {
+        findBestSplitValueSmallQ(nodeID, varID, sum_node, num_samples_node, best_value, best_varID, best_decrease);
+      } else {
+        // Use faster method for both cases
+        double q = (double) num_samples_node / (double) data->getNumUniqueDataValues(varID);
+        if (q < Q_THRESHOLD) {
+          findBestSplitValueSmallQ(nodeID, varID, sum_node, num_samples_node, best_value, best_varID, best_decrease);
+        } else {
+          findBestSplitValueLargeQ(nodeID, varID, sum_node, num_samples_node, best_value, best_varID, best_decrease);
+        }
+      }
+    } else {
+      findBestSplitValueUnordered(nodeID, varID, sum_node, num_samples_node, best_value, best_varID, best_decrease);
+    }
+  }
+
+// Stop if no good split found
+  if (best_decrease < 0) {
+    return true;
+  }
+
+// Save best values
+  split_varIDs[nodeID] = best_varID;
+  split_values[nodeID] = best_value;
+
+// Compute decrease of impurity for this node and add to variable importance if needed
+  if (importance_mode == IMP_GINI || importance_mode == IMP_GINI_CORRECTED) {
+    addImpurityImportance(nodeID, best_varID, best_decrease);
+  }
+  
+  // Regularization
+  saveSplitVarID(best_varID);
+
+  return false;
+}
+
+void TreeRegression::findBestSplitValueSmallQ(size_t nodeID, size_t varID, double sum_node, size_t num_samples_node,
+    double& best_value, size_t& best_varID, double& best_decrease) {
+
+  // Create possible split values
+  std::vector<double> possible_split_values;
+  data->getAllValues(possible_split_values, sampleIDs, varID, start_pos[nodeID], end_pos[nodeID]);
+
+  // Try next variable if all equal for this
+  if (possible_split_values.size() < 2) {
+    return;
+  }
+
+  const size_t num_splits = possible_split_values.size();
+  if (memory_saving_splitting) {
+    std::vector<double> sums_right(num_splits);
+    std::vector<size_t> n_right(num_splits);
+    findBestSplitValueSmallQ(nodeID, varID, sum_node, num_samples_node, best_value, best_varID, best_decrease,
+            possible_split_values, sums_right, n_right);
+  } else {
+    std::fill_n(sums.begin(), num_splits, 0);
+    std::fill_n(counter.begin(), num_splits, 0);
+    findBestSplitValueSmallQ(nodeID, varID, sum_node, num_samples_node, best_value, best_varID, best_decrease,
+           possible_split_values, sums, counter);
+  }
+}
+
+void TreeRegression::findBestSplitValueSmallQ(size_t nodeID, size_t varID, double sum_node, size_t num_samples_node,
+    double& best_value, size_t& best_varID, double& best_decrease, std::vector<double> possible_split_values,
+    std::vector<double>& sums, std::vector<size_t>& counter) {
+
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    size_t idx = std::lower_bound(possible_split_values.begin(), possible_split_values.end(),
+        data->get_x(sampleID, varID)) - possible_split_values.begin();
+
+    sums[idx] += data->get_y(sampleID, 0);
+    ++counter[idx];
+  }
+
+  size_t n_left = 0;
+  double sum_left = 0;
+
+  // Compute decrease of impurity for each split
+  for (size_t i = 0; i < possible_split_values.size() - 1; ++i) {
+
+    // Stop if nothing here
+    if (counter[i] == 0) {
+      continue;
+    }
+
+    n_left += counter[i];
+    sum_left += sums[i];
+
+    // Stop if right child empty
+    size_t n_right = num_samples_node - n_left;
+    if (n_right == 0) {
+      break;
+    }
+
+    double sum_right = sum_node - sum_left;
+    double decrease = sum_left * sum_left / (double) n_left + sum_right * sum_right / (double) n_right;
+
+    // Regularization
+    regularize(decrease, varID);
+
+    // If better than before, use this
+    if (decrease > best_decrease) {
+      // Use mid-point split
+      best_value = (possible_split_values[i] + possible_split_values[i + 1]) / 2;
+      best_varID = varID;
+      best_decrease = decrease;
+
+      // Use smaller value if average is numerically the same as the larger value
+      if (best_value == possible_split_values[i + 1]) {
+        best_value = possible_split_values[i];
+      }
+    }
+  }
+}
+
+void TreeRegression::findBestSplitValueLargeQ(size_t nodeID, size_t varID, double sum_node, size_t num_samples_node,
+    double& best_value, size_t& best_varID, double& best_decrease) {
+
+  // Set counters to 0
+  size_t num_unique = data->getNumUniqueDataValues(varID);
+  std::fill_n(counter.begin(), num_unique, 0);
+  std::fill_n(sums.begin(), num_unique, 0);
+
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    size_t index = data->getIndex(sampleID, varID);
+
+    sums[index] += data->get_y(sampleID, 0);
+    ++counter[index];
+  }
+
+  size_t n_left = 0;
+  double sum_left = 0;
+
+  // Compute decrease of impurity for each split
+  for (size_t i = 0; i < num_unique - 1; ++i) {
+
+    // Stop if nothing here
+    if (counter[i] == 0) {
+      continue;
+    }
+
+    n_left += counter[i];
+    sum_left += sums[i];
+
+    // Stop if right child empty
+    size_t n_right = num_samples_node - n_left;
+    if (n_right == 0) {
+      break;
+    }
+
+    double sum_right = sum_node - sum_left;
+    double decrease = sum_left * sum_left / (double) n_left + sum_right * sum_right / (double) n_right;
+
+    // Regularization
+    regularize(decrease, varID);
+
+    // If better than before, use this
+    if (decrease > best_decrease) {
+      // Find next value in this node
+      size_t j = i + 1;
+      while (j < num_unique && counter[j] == 0) {
+        ++j;
+      }
+
+      // Use mid-point split
+      best_value = (data->getUniqueDataValue(varID, i) + data->getUniqueDataValue(varID, j)) / 2;
+      best_varID = varID;
+      best_decrease = decrease;
+
+      // Use smaller value if average is numerically the same as the larger value
+      if (best_value == data->getUniqueDataValue(varID, j)) {
+        best_value = data->getUniqueDataValue(varID, i);
+      }
+    }
+  }
+}
+
+void TreeRegression::findBestSplitValueUnordered(size_t nodeID, size_t varID, double sum_node, size_t num_samples_node,
+    double& best_value, size_t& best_varID, double& best_decrease) {
+
+// Create possible split values
+  std::vector<double> factor_levels;
+  data->getAllValues(factor_levels, sampleIDs, varID, start_pos[nodeID], end_pos[nodeID]);
+
+// Try next variable if all equal for this
+  if (factor_levels.size() < 2) {
+    return;
+  }
+
+// Number of possible splits is 2^num_levels
+  size_t num_splits = (1ULL << factor_levels.size());
+
+// Compute decrease of impurity for each possible split
+// Split where all left (0) or all right (1) are excluded
+// The second half of numbers is just left/right switched the first half -> Exclude second half
+  for (size_t local_splitID = 1; local_splitID < num_splits / 2; ++local_splitID) {
+
+    // Compute overall splitID by shifting local factorIDs to global positions
+    size_t splitID = 0;
+    for (size_t j = 0; j < factor_levels.size(); ++j) {
+      if ((local_splitID & (1ULL << j))) {
+        double level = factor_levels[j];
+        size_t factorID = floor(level) - 1;
+        splitID = splitID | (1ULL << factorID);
+      }
+    }
+
+    // Initialize
+    double sum_right = 0;
+    size_t n_right = 0;
+
+    // Sum in right child
+    for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+      size_t sampleID = sampleIDs[pos];
+      double response = data->get_y(sampleID, 0);
+      double value = data->get_x(sampleID, varID);
+      size_t factorID = floor(value) - 1;
+
+      // If in right child, count
+      // In right child, if bitwise splitID at position factorID is 1
+      if ((splitID & (1ULL << factorID))) {
+        ++n_right;
+        sum_right += response;
+      }
+    }
+    size_t n_left = num_samples_node - n_right;
+
+    // Sum of squares
+    double sum_left = sum_node - sum_right;
+    double decrease = sum_left * sum_left / (double) n_left + sum_right * sum_right / (double) n_right;
+
+    // Regularization
+    regularize(decrease, varID);
+
+    // If better than before, use this
+    if (decrease > best_decrease) {
+      best_value = splitID;
+      best_varID = varID;
+      best_decrease = decrease;
+    }
+  }
+}
+
+bool TreeRegression::findBestSplitMaxstat(size_t nodeID, std::vector<size_t>& possible_split_varIDs) {
+
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+
+  // Compute ranks
+  std::vector<double> response;
+  response.reserve(num_samples_node);
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    response.push_back(data->get_y(sampleID, 0));
+  }
+  std::vector<double> ranks = rank(response);
+
+  // Save split stats
+  std::vector<double> pvalues;
+  pvalues.reserve(possible_split_varIDs.size());
+  std::vector<double> values;
+  values.reserve(possible_split_varIDs.size());
+  std::vector<double> candidate_varIDs;
+  candidate_varIDs.reserve(possible_split_varIDs.size());
+  std::vector<double> test_statistics;
+  test_statistics.reserve(possible_split_varIDs.size());
+
+  // Compute p-values
+  for (auto& varID : possible_split_varIDs) {
+
+    // Get all observations
+    std::vector<double> x;
+    x.reserve(num_samples_node);
+    for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+      size_t sampleID = sampleIDs[pos];
+      x.push_back(data->get_x(sampleID, varID));
+    }
+
+    // Order by x
+    std::vector<size_t> indices = order(x, false);
+    //std::vector<size_t> indices = orderInData(data, sampleIDs[nodeID], varID, false);
+
+    // Compute maximally selected rank statistics
+    double best_maxstat;
+    double best_split_value;
+    maxstat(ranks, x, indices, best_maxstat, best_split_value, minprop, 1 - minprop);
+    //maxstatInData(scores, data, sampleIDs[nodeID], varID, indices, best_maxstat, best_split_value, minprop, 1 - minprop);
+
+    if (best_maxstat > -1) {
+      // Compute number of samples left of cutpoints
+      std::vector<size_t> num_samples_left = numSamplesLeftOfCutpoint(x, indices);
+      //std::vector<size_t> num_samples_left = numSamplesLeftOfCutpointInData(data, sampleIDs[nodeID], varID, indices);
+
+      // Compute p-values
+      double pvalue_lau92 = maxstatPValueLau92(best_maxstat, minprop, 1 - minprop);
+      double pvalue_lau94 = maxstatPValueLau94(best_maxstat, minprop, 1 - minprop, num_samples_node, num_samples_left);
+
+      // Use minimum of Lau92 and Lau94
+      double pvalue = std::min(pvalue_lau92, pvalue_lau94);
+
+      // Save split stats
+      pvalues.push_back(pvalue);
+      values.push_back(best_split_value);
+      candidate_varIDs.push_back(varID);
+      test_statistics.push_back(best_maxstat);
+    }
+  }
+
+  double adjusted_best_pvalue = std::numeric_limits<double>::max();
+  size_t best_varID = 0;
+  double best_value = 0;
+  double best_maxstat = 0;
+
+  if (pvalues.size() > 0) {
+    // Adjust p-values with Benjamini/Hochberg
+    std::vector<double> adjusted_pvalues = adjustPvalues(pvalues);
+
+    // Use smallest p-value
+    double min_pvalue = std::numeric_limits<double>::max();
+    for (size_t i = 0; i < pvalues.size(); ++i) {
+      if (pvalues[i] < min_pvalue) {
+        min_pvalue = pvalues[i];
+        best_varID = candidate_varIDs[i];
+        best_value = values[i];
+        adjusted_best_pvalue = adjusted_pvalues[i];
+        best_maxstat = test_statistics[i];
+      }
+    }
+  }
+
+  // Stop if no good split found (this is terminal node).
+  if (adjusted_best_pvalue > alpha) {
+    return true;
+  } else {
+    // If not terminal node save best values
+    split_varIDs[nodeID] = best_varID;
+    split_values[nodeID] = best_value;
+
+    // Compute decrease of impurity for this node and add to variable importance if needed
+    if (importance_mode == IMP_GINI || importance_mode == IMP_GINI_CORRECTED) {
+      addImpurityImportance(nodeID, best_varID, best_maxstat);
+    }
+
+    return false;
+  }
+}
+
+bool TreeRegression::findBestSplitExtraTrees(size_t nodeID, std::vector<size_t>& possible_split_varIDs) {
+
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+  double best_decrease = -1;
+  size_t best_varID = 0;
+  double best_value = 0;
+
+  // Compute sum of responses in node
+  double sum_node = 0;
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    sum_node += data->get_y(sampleID, 0);
+  }
+
+  // For all possible split variables
+  for (auto& varID : possible_split_varIDs) {
+
+    // Find best split value, if ordered consider all values as split values, else all 2-partitions
+    if (data->isOrderedVariable(varID)) {
+      findBestSplitValueExtraTrees(nodeID, varID, sum_node, num_samples_node, best_value, best_varID, best_decrease);
+    } else {
+      findBestSplitValueExtraTreesUnordered(nodeID, varID, sum_node, num_samples_node, best_value, best_varID,
+          best_decrease);
+    }
+  }
+
+  // Stop if no good split found
+  if (best_decrease < 0) {
+    return true;
+  }
+
+  // Save best values
+  split_varIDs[nodeID] = best_varID;
+  split_values[nodeID] = best_value;
+
+  // Compute decrease of impurity for this node and add to variable importance if needed
+  if (importance_mode == IMP_GINI || importance_mode == IMP_GINI_CORRECTED) {
+    addImpurityImportance(nodeID, best_varID, best_decrease);
+  }
+  
+  // Regularization
+  saveSplitVarID(best_varID);
+
+  return false;
+}
+
+void TreeRegression::findBestSplitValueExtraTrees(size_t nodeID, size_t varID, double sum_node, size_t num_samples_node,
+    double& best_value, size_t& best_varID, double& best_decrease) {
+
+  // Get min/max values of covariate in node
+  double min;
+  double max;
+  data->getMinMaxValues(min, max, sampleIDs, varID, start_pos[nodeID], end_pos[nodeID]);
+
+  // Try next variable if all equal for this
+  if (min == max) {
+    return;
+  }
+
+  // Create possible split values: Draw randomly between min and max
+  std::vector<double> possible_split_values;
+  std::uniform_real_distribution<double> udist(min, max);
+  possible_split_values.reserve(num_random_splits);
+  for (size_t i = 0; i < num_random_splits; ++i) {
+    possible_split_values.push_back(udist(random_number_generator));
+  }
+  if (num_random_splits > 1) {
+    std::sort(possible_split_values.begin(), possible_split_values.end());
+  }
+
+  const size_t num_splits = possible_split_values.size();
+  if (memory_saving_splitting) {
+    std::vector<double> sums_right(num_splits);
+    std::vector<size_t> n_right(num_splits);
+    findBestSplitValueExtraTrees(nodeID, varID, sum_node, num_samples_node, best_value, best_varID, best_decrease,
+        possible_split_values, sums_right, n_right);
+  } else {
+    std::fill_n(sums.begin(), num_splits, 0);
+    std::fill_n(counter.begin(), num_splits, 0);
+    findBestSplitValueExtraTrees(nodeID, varID, sum_node, num_samples_node, best_value, best_varID, best_decrease,
+        possible_split_values, sums, counter);
+  }
+}
+
+void TreeRegression::findBestSplitValueExtraTrees(size_t nodeID, size_t varID, double sum_node, size_t num_samples_node,
+    double& best_value, size_t& best_varID, double& best_decrease, std::vector<double> possible_split_values,
+    std::vector<double>& sums_right, std::vector<size_t>& n_right) {
+  const size_t num_splits = possible_split_values.size();
+
+  // Sum in right child and possbile split
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    double value = data->get_x(sampleID, varID);
+    double response = data->get_y(sampleID, 0);
+
+    // Count samples until split_value reached
+    for (size_t i = 0; i < num_splits; ++i) {
+      if (value > possible_split_values[i]) {
+        ++n_right[i];
+        sums_right[i] += response;
+      } else {
+        break;
+      }
+    }
+  }
+
+  // Compute decrease of impurity for each possible split
+  for (size_t i = 0; i < num_splits; ++i) {
+
+    // Stop if one child empty
+    size_t n_left = num_samples_node - n_right[i];
+    if (n_left == 0 || n_right[i] == 0) {
+      continue;
+    }
+
+    double sum_right = sums_right[i];
+    double sum_left = sum_node - sum_right;
+    double decrease = sum_left * sum_left / (double) n_left + sum_right * sum_right / (double) n_right[i];
+
+    // Regularization
+    regularize(decrease, varID);
+
+    // If better than before, use this
+    if (decrease > best_decrease) {
+      best_value = possible_split_values[i];
+      best_varID = varID;
+      best_decrease = decrease;
+    }
+  }
+}
+
+void TreeRegression::findBestSplitValueExtraTreesUnordered(size_t nodeID, size_t varID, double sum_node,
+    size_t num_samples_node, double& best_value, size_t& best_varID, double& best_decrease) {
+
+  size_t num_unique_values = data->getNumUniqueDataValues(varID);
+
+  // Get all factor indices in node
+  std::vector<bool> factor_in_node(num_unique_values, false);
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    size_t index = data->getIndex(sampleID, varID);
+    factor_in_node[index] = true;
+  }
+
+  // Vector of indices in and out of node
+  std::vector<size_t> indices_in_node;
+  std::vector<size_t> indices_out_node;
+  indices_in_node.reserve(num_unique_values);
+  indices_out_node.reserve(num_unique_values);
+  for (size_t i = 0; i < num_unique_values; ++i) {
+    if (factor_in_node[i]) {
+      indices_in_node.push_back(i);
+    } else {
+      indices_out_node.push_back(i);
+    }
+  }
+
+  // Generate num_random_splits splits
+  for (size_t i = 0; i < num_random_splits; ++i) {
+    std::vector<size_t> split_subset;
+    split_subset.reserve(num_unique_values);
+
+    // Draw random subsets, sample all partitions with equal probability
+    if (indices_in_node.size() > 1) {
+      size_t num_partitions = (2ULL << (indices_in_node.size() - 1ULL)) - 2ULL; // 2^n-2 (don't allow full or empty)
+      std::uniform_int_distribution<size_t> udist(1, num_partitions);
+      size_t splitID_in_node = udist(random_number_generator);
+      for (size_t j = 0; j < indices_in_node.size(); ++j) {
+        if ((splitID_in_node & (1ULL << j)) > 0) {
+          split_subset.push_back(indices_in_node[j]);
+        }
+      }
+    }
+    if (indices_out_node.size() > 1) {
+      size_t num_partitions = (2ULL << (indices_out_node.size() - 1ULL)) - 1ULL; // 2^n-1 (allow full or empty)
+      std::uniform_int_distribution<size_t> udist(0, num_partitions);
+      size_t splitID_out_node = udist(random_number_generator);
+      for (size_t j = 0; j < indices_out_node.size(); ++j) {
+        if ((splitID_out_node & (1ULL << j)) > 0) {
+          split_subset.push_back(indices_out_node[j]);
+        }
+      }
+    }
+
+    // Assign union of the two subsets to right child
+    size_t splitID = 0;
+    for (auto& idx : split_subset) {
+      splitID |= 1ULL << idx;
+    }
+
+    // Initialize
+    double sum_right = 0;
+    size_t n_right = 0;
+
+    // Sum in right child
+    for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+      size_t sampleID = sampleIDs[pos];
+      double response = data->get_y(sampleID, 0);
+      double value = data->get_x(sampleID, varID);
+      size_t factorID = floor(value) - 1;
+
+      // If in right child, count
+      // In right child, if bitwise splitID at position factorID is 1
+      if ((splitID & (1ULL << factorID))) {
+        ++n_right;
+        sum_right += response;
+      }
+    }
+    size_t n_left = num_samples_node - n_right;
+
+    // Sum of squares
+    double sum_left = sum_node - sum_right;
+    double decrease = sum_left * sum_left / (double) n_left + sum_right * sum_right / (double) n_right;
+
+    // Regularization
+    regularize(decrease, varID);
+
+    // If better than before, use this
+    if (decrease > best_decrease) {
+      best_value = splitID;
+      best_varID = varID;
+      best_decrease = decrease;
+    }
+  }
+}
+
+bool TreeRegression::findBestSplitBeta(size_t nodeID, std::vector<size_t>& possible_split_varIDs) {
+
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+  double best_decrease = -std::numeric_limits<double>::infinity();
+  size_t best_varID = 0;
+  double best_value = 0;
+
+  // Compute sum of responses in node
+  double sum_node = 0;
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    sum_node += data->get_y(sampleID, 0);
+  }
+
+  // For all possible split variables find best split value
+  for (auto& varID : possible_split_varIDs) {
+    findBestSplitValueBeta(nodeID, varID, sum_node, num_samples_node, best_value, best_varID, best_decrease);
+  }
+
+  // Stop if no good split found
+  if (std::isinf(-best_decrease)) {
+    return true;
+  }
+
+  // Save best values
+  split_varIDs[nodeID] = best_varID;
+  split_values[nodeID] = best_value;
+
+  // Compute decrease of impurity for this node and add to variable importance if needed
+  if (importance_mode == IMP_GINI || importance_mode == IMP_GINI_CORRECTED) {
+    addImpurityImportance(nodeID, best_varID, best_decrease);
+  }
+
+  // Regularization
+  saveSplitVarID(best_varID);
+
+  return false;
+}
+
+void TreeRegression::findBestSplitValueBeta(size_t nodeID, size_t varID, double sum_node, size_t num_samples_node,
+    double& best_value, size_t& best_varID, double& best_decrease) {
+
+  // Create possible split values
+  std::vector<double> possible_split_values;
+  data->getAllValues(possible_split_values, sampleIDs, varID, start_pos[nodeID], end_pos[nodeID]);
+
+  // Try next variable if all equal for this
+  if (possible_split_values.size() < 2) {
+    return;
+  }
+
+  // -1 because no split possible at largest value
+  size_t num_splits = possible_split_values.size() - 1;
+  if (memory_saving_splitting) {
+    std::vector<double> sums_right(num_splits);
+    std::vector<size_t> n_right(num_splits);
+    findBestSplitValueBeta(nodeID, varID, sum_node, num_samples_node, best_value, best_varID, best_decrease,
+        possible_split_values, sums_right, n_right);
+  } else {
+    std::fill_n(sums.begin(), num_splits, 0);
+    std::fill_n(counter.begin(), num_splits, 0);
+    findBestSplitValueBeta(nodeID, varID, sum_node, num_samples_node, best_value, best_varID, best_decrease,
+        possible_split_values, sums, counter);
+  }
+}
+
+void TreeRegression::findBestSplitValueBeta(size_t nodeID, size_t varID, double sum_node, size_t num_samples_node,
+    double& best_value, size_t& best_varID, double& best_decrease, std::vector<double> possible_split_values,
+    std::vector<double>& sums_right, std::vector<size_t>& n_right) {
+  // -1 because no split possible at largest value
+  const size_t num_splits = possible_split_values.size() - 1;
+
+  // Sum in right child and possbile split
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    double value = data->get_x(sampleID, varID);
+    double response = data->get_y(sampleID, 0);
+
+    // Count samples until split_value reached
+    for (size_t i = 0; i < num_splits; ++i) {
+      if (value > possible_split_values[i]) {
+        ++n_right[i];
+        sums_right[i] += response;
+      } else {
+        break;
+      }
+    }
+  }
+
+  // Compute LogLik of beta distribution for each possible split
+  for (size_t i = 0; i < num_splits; ++i) {
+
+    // Stop if one child too small
+    size_t n_left = num_samples_node - n_right[i];
+    if (n_left < 2 || n_right[i] < 2) {
+      continue;
+    }
+
+    // Compute mean
+    double sum_right = sums_right[i];
+    double mean_right = sum_right / (double) n_right[i];
+    double sum_left = sum_node - sum_right;
+    double mean_left = sum_left / (double) n_left;
+
+    // Compute variance
+    double var_right = 0;
+    double var_left = 0;
+    for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+      size_t sampleID = sampleIDs[pos];
+      double value = data->get_x(sampleID, varID);
+      double response = data->get_y(sampleID, 0);
+
+      if (value > possible_split_values[i]) {
+        var_right += (response - mean_right) * (response - mean_right);
+      } else {
+        var_left += (response - mean_left) * (response - mean_left);
+      }
+    }
+    var_right /= (double) n_right[i] - 1;
+    var_left /= (double) n_left - 1;
+
+    // Stop if zero variance
+    if (var_right < std::numeric_limits<double>::epsilon() || var_left < std::numeric_limits<double>::epsilon()) {
+      continue;
+    }
+
+    // Compute phi for beta distribution
+    double phi_right = mean_right * (1 - mean_right) / var_right - 1;
+    double phi_left = mean_left * (1 - mean_left) / var_left - 1;
+
+    // Compute LogLik of beta distribution
+    double beta_loglik_right = 0;
+    double beta_loglik_left = 0;
+    for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+      size_t sampleID = sampleIDs[pos];
+      double value = data->get_x(sampleID, varID);
+      double response = data->get_y(sampleID, 0);
+
+      if (value > possible_split_values[i]) {
+        beta_loglik_right += betaLogLik(response, mean_right, phi_right);
+      } else {
+        beta_loglik_left += betaLogLik(response, mean_left, phi_left);
+      }
+    }
+
+    // Split statistic is sum of both log-likelihoods
+    double decrease = beta_loglik_right + beta_loglik_left;
+
+    // Stop if no result
+    if (std::isnan(decrease)) {
+      continue;
+    }
+
+    // Regularization (negative values)
+    regularizeNegative(decrease, varID);
+
+    // If better than before, use this
+    if (decrease > best_decrease) {
+      best_value = (possible_split_values[i] + possible_split_values[i + 1]) / 2;
+      best_varID = varID;
+      best_decrease = decrease;
+
+      // Use smaller value if average is numerically the same as the larger value
+      if (best_value == possible_split_values[i + 1]) {
+        best_value = possible_split_values[i];
+      }
+    }
+  }
+}
+
+void TreeRegression::addImpurityImportance(size_t nodeID, size_t varID, double decrease) {
+
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+
+  double best_decrease = decrease;
+  if (splitrule != MAXSTAT) {
+    double sum_node = 0;
+    for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+      size_t sampleID = sampleIDs[pos];
+      sum_node += data->get_y(sampleID, 0);
+    }
+    
+    double impurity_node = (sum_node * sum_node / (double) num_samples_node);
+
+    // Account for the regularization
+    regularize(impurity_node, varID);
+    
+    best_decrease = decrease - impurity_node;
+  }
+
+  // No variable importance for no split variables
+  size_t tempvarID = data->getUnpermutedVarID(varID);
+
+  // Subtract if corrected importance and permuted variable, else add
+  if (importance_mode == IMP_GINI_CORRECTED && varID >= data->getNumCols()) {
+    (*variable_importance)[tempvarID] -= best_decrease;
+  } else {
+    (*variable_importance)[tempvarID] += best_decrease;
+  }
+}
+
+} // namespace ranger
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/TreeRegression.h
@@ -0,0 +1,103 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#ifndef TREEREGRESSION_H_
+#define TREEREGRESSION_H_
+
+#include <vector>
+
+#include "globals.h"
+#include "Tree.h"
+
+namespace ranger {
+
+class TreeRegression: public Tree {
+public:
+  TreeRegression() = default;
+
+  // Create from loaded forest
+  TreeRegression(std::vector<std::vector<size_t>>& child_nodeIDs, std::vector<size_t>& split_varIDs,
+      std::vector<double>& split_values);
+
+  TreeRegression(const TreeRegression&) = delete;
+  TreeRegression& operator=(const TreeRegression&) = delete;
+
+  virtual ~TreeRegression() override = default;
+
+  void allocateMemory() override;
+
+  double estimate(size_t nodeID);
+  void computePermutationImportanceInternal(std::vector<std::vector<size_t>>* permutations);
+  void appendToFileInternal(std::ofstream& file) override;
+
+  double getPrediction(size_t sampleID) const {
+    size_t terminal_nodeID = prediction_terminal_nodeIDs[sampleID];
+    return (split_values[terminal_nodeID]);
+  }
+
+  size_t getPredictionTerminalNodeID(size_t sampleID) const {
+    return prediction_terminal_nodeIDs[sampleID];
+  }
+
+private:
+  bool splitNodeInternal(size_t nodeID, std::vector<size_t>& possible_split_varIDs) override;
+  void createEmptyNodeInternal() override;
+
+  double computePredictionAccuracyInternal(std::vector<double>* prediction_error_casewise) override;
+  
+  // Called by splitNodeInternal(). Sets split_varIDs and split_values.
+  bool findBestSplit(size_t nodeID, std::vector<size_t>& possible_split_varIDs);
+  void findBestSplitValueSmallQ(size_t nodeID, size_t varID, double sum_node, size_t num_samples_node,
+      double& best_value, size_t& best_varID, double& best_decrease);
+  void findBestSplitValueSmallQ(size_t nodeID, size_t varID, double sum_node, size_t num_samples_node,
+      double& best_value, size_t& best_varID, double& best_decrease, std::vector<double> possible_split_values,
+      std::vector<double>& sums, std::vector<size_t>& counter);
+  void findBestSplitValueLargeQ(size_t nodeID, size_t varID, double sum_node, size_t num_samples_node,
+      double& best_value, size_t& best_varID, double& best_decrease);
+  void findBestSplitValueUnordered(size_t nodeID, size_t varID, double sum_node, size_t num_samples_node,
+      double& best_value, size_t& best_varID, double& best_decrease);
+
+  bool findBestSplitMaxstat(size_t nodeID, std::vector<size_t>& possible_split_varIDs);
+
+  bool findBestSplitExtraTrees(size_t nodeID, std::vector<size_t>& possible_split_varIDs);
+  void findBestSplitValueExtraTrees(size_t nodeID, size_t varID, double sum_node, size_t num_samples_node,
+      double& best_value, size_t& best_varID, double& best_decrease);
+  void findBestSplitValueExtraTrees(size_t nodeID, size_t varID, double sum_node, size_t num_samples_node,
+      double& best_value, size_t& best_varID, double& best_decrease, std::vector<double> possible_split_values,
+      std::vector<double>& sums_right, std::vector<size_t>& n_right);
+  void findBestSplitValueExtraTreesUnordered(size_t nodeID, size_t varID, double sum_node, size_t num_samples_node,
+      double& best_value, size_t& best_varID, double& best_decrease);
+
+  bool findBestSplitBeta(size_t nodeID, std::vector<size_t>& possible_split_varIDs);
+  void findBestSplitValueBeta(size_t nodeID, size_t varID, double sum_node, size_t num_samples_node, double& best_value,
+      size_t& best_varID, double& best_decrease);
+  void findBestSplitValueBeta(size_t nodeID, size_t varID, double sum_node, size_t num_samples_node, double& best_value,
+      size_t& best_varID, double& best_decrease, std::vector<double> possible_split_values,
+      std::vector<double>& sums_right, std::vector<size_t>& n_right);
+
+  void addImpurityImportance(size_t nodeID, size_t varID, double decrease);
+
+  double computePredictionMSE();
+
+  void cleanUpInternal() override {
+    counter.clear();
+    counter.shrink_to_fit();
+    sums.clear();
+    sums.shrink_to_fit();
+  }
+
+  std::vector<size_t> counter;
+  std::vector<double> sums;
+};
+
+} // namespace ranger
+
+#endif /* TREEREGRESSION_H_ */
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/TreeSurvival.cpp
@@ -0,0 +1,928 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#include <algorithm>
+#include <cmath>
+#include <iostream>
+#include <iterator>
+#include <numeric>
+
+#include "utility.h"
+#include "TreeSurvival.h"
+#include "Data.h"
+
+namespace ranger {
+
+TreeSurvival::TreeSurvival(std::vector<double>* unique_timepoints, std::vector<size_t>* response_timepointIDs) :
+    unique_timepoints(unique_timepoints), response_timepointIDs(response_timepointIDs), num_deaths(0), num_samples_at_risk(
+        0) {
+  this->num_timepoints = unique_timepoints->size();
+}
+
+TreeSurvival::TreeSurvival(std::vector<std::vector<size_t>>& child_nodeIDs, std::vector<size_t>& split_varIDs,
+    std::vector<double>& split_values, std::vector<std::vector<double>> chf, std::vector<double>* unique_timepoints,
+    std::vector<size_t>* response_timepointIDs) :
+    Tree(child_nodeIDs, split_varIDs, split_values), unique_timepoints(unique_timepoints), response_timepointIDs(
+        response_timepointIDs), chf(chf), num_deaths(0), num_samples_at_risk(0) {
+  this->num_timepoints = unique_timepoints->size();
+}
+
+void TreeSurvival::allocateMemory() {
+  // Number of deaths and samples at risk for each timepoint
+  num_deaths.resize(num_timepoints);
+  num_samples_at_risk.resize(num_timepoints);
+}
+
+void TreeSurvival::appendToFileInternal(std::ofstream& file) {  // #nocov start
+
+  // Convert to vector without empty elements and save
+  std::vector<size_t> terminal_nodes;
+  std::vector<std::vector<double>> chf_vector;
+  for (size_t i = 0; i < chf.size(); ++i) {
+    if (!chf[i].empty()) {
+      terminal_nodes.push_back(i);
+      chf_vector.push_back(chf[i]);
+    }
+  }
+  saveVector1D(terminal_nodes, file);
+  saveVector2D(chf_vector, file);
+} // #nocov end
+
+void TreeSurvival::createEmptyNodeInternal() {
+  chf.push_back(std::vector<double>());
+}
+
+void TreeSurvival::computeSurvival(size_t nodeID) {
+  std::vector<double> chf_temp;
+  chf_temp.reserve(num_timepoints);
+  double chf_value = 0;
+  for (size_t i = 0; i < num_timepoints; ++i) {
+    if (num_samples_at_risk[i] != 0) {
+      chf_value += (double) num_deaths[i] / (double) num_samples_at_risk[i];
+    }
+    chf_temp.push_back(chf_value);
+  }
+  chf[nodeID] = chf_temp;
+}
+
+double TreeSurvival::computePredictionAccuracyInternal(std::vector<double>* prediction_error_casewise) {
+
+  // Compute summed chf for samples
+  std::vector<double> sum_chf;
+  for (size_t i = 0; i < prediction_terminal_nodeIDs.size(); ++i) {
+    size_t terminal_nodeID = prediction_terminal_nodeIDs[i];
+    sum_chf.push_back(std::accumulate(chf[terminal_nodeID].begin(), chf[terminal_nodeID].end(), 0.0));
+  }
+
+  // Return concordance index
+  return computeConcordanceIndex(*data, sum_chf, oob_sampleIDs, prediction_error_casewise);
+}
+
+bool TreeSurvival::splitNodeInternal(size_t nodeID, std::vector<size_t>& possible_split_varIDs) {
+
+  if (splitrule == MAXSTAT) {
+    return findBestSplitMaxstat(nodeID, possible_split_varIDs);
+  } else if (splitrule == EXTRATREES) {
+    return findBestSplitExtraTrees(nodeID, possible_split_varIDs);
+  } else {
+    return findBestSplit(nodeID, possible_split_varIDs);
+  }
+}
+
+bool TreeSurvival::findBestSplit(size_t nodeID, std::vector<size_t>& possible_split_varIDs) {
+
+  double best_decrease = -1;
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+  size_t best_varID = 0;
+  double best_value = 0;
+
+  computeDeathCounts(nodeID);
+
+  // Stop if maximum node size or depth reached (will check again for each child node)
+  if (num_samples_node <= min_node_size || (nodeID >= last_left_nodeID && max_depth > 0 && depth >= max_depth)) {
+    computeSurvival(nodeID);
+    return true;
+  }
+
+  // Stop early if no split posssible
+  if (num_samples_node >= 2 * min_node_size) {
+
+    // For all possible split variables
+    for (auto& varID : possible_split_varIDs) {
+
+      // Find best split value, if ordered consider all values as split values, else all 2-partitions
+      if (data->isOrderedVariable(varID)) {
+        if (splitrule == LOGRANK) {
+          findBestSplitValueLogRank(nodeID, varID, best_value, best_varID, best_decrease);
+        } else if (splitrule == AUC || splitrule == AUC_IGNORE_TIES) {
+          findBestSplitValueAUC(nodeID, varID, best_value, best_varID, best_decrease);
+        }
+      } else {
+        findBestSplitValueLogRankUnordered(nodeID, varID, best_value, best_varID, best_decrease);
+      }
+
+    }
+  }
+
+  // Stop and save CHF if no good split found (this is terminal node).
+  if (best_decrease < 0) {
+    computeSurvival(nodeID);
+    return true;
+  } else {
+    // If not terminal node save best values
+    split_varIDs[nodeID] = best_varID;
+    split_values[nodeID] = best_value;
+
+    // Compute decrease of impurity for this node and add to variable importance if needed
+    if (importance_mode == IMP_GINI || importance_mode == IMP_GINI_CORRECTED) {
+      addImpurityImportance(nodeID, best_varID, best_decrease);
+    }
+
+    // Regularization
+    saveSplitVarID(best_varID);
+
+    return false;
+  }
+}
+
+bool TreeSurvival::findBestSplitMaxstat(size_t nodeID, std::vector<size_t>& possible_split_varIDs) {
+
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+
+  // Stop if maximum node size or depth reached
+  if (num_samples_node <= min_node_size || (nodeID >= last_left_nodeID && max_depth > 0 && depth >= max_depth)) {
+    computeDeathCounts(nodeID);
+    computeSurvival(nodeID);
+    return true;
+  }
+
+  // Compute scores
+  std::vector<double> time;
+  time.reserve(num_samples_node);
+  std::vector<double> status;
+  status.reserve(num_samples_node);
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    time.push_back(data->get_y(sampleID, 0));
+    status.push_back(data->get_y(sampleID, 1));
+  }
+  std::vector<double> scores = logrankScores(time, status);
+
+  // Save split stats
+  std::vector<double> pvalues;
+  pvalues.reserve(possible_split_varIDs.size());
+  std::vector<double> values;
+  values.reserve(possible_split_varIDs.size());
+  std::vector<double> candidate_varIDs;
+  candidate_varIDs.reserve(possible_split_varIDs.size());
+  std::vector<double> test_statistics;
+  test_statistics.reserve(possible_split_varIDs.size());
+
+  // Compute p-values
+  for (auto& varID : possible_split_varIDs) {
+
+    // Get all observations
+    std::vector<double> x;
+    x.reserve(num_samples_node);
+    for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+      size_t sampleID = sampleIDs[pos];
+      x.push_back(data->get_x(sampleID, varID));
+    }
+
+    // Order by x
+    std::vector<size_t> indices = order(x, false);
+    //std::vector<size_t> indices = orderInData(data, sampleIDs[nodeID], varID, false);
+
+    // Compute maximally selected rank statistics
+    double best_maxstat;
+    double best_split_value;
+    maxstat(scores, x, indices, best_maxstat, best_split_value, minprop, 1 - minprop);
+    //maxstatInData(scores, data, sampleIDs[nodeID], varID, indices, best_maxstat, best_split_value, minprop, 1 - minprop);
+
+    if (best_maxstat > -1) {
+      // Compute number of samples left of cutpoints
+      std::vector<size_t> num_samples_left = numSamplesLeftOfCutpoint(x, indices);
+      //std::vector<size_t> num_samples_left = numSamplesLeftOfCutpointInData(data, sampleIDs[nodeID], varID, indices);
+
+      // Remove largest cutpoint (all observations left)
+      num_samples_left.pop_back();
+
+      // Use unadjusted p-value if only 1 split point
+      double pvalue;
+      if (num_samples_left.size() == 1) {
+        pvalue = maxstatPValueUnadjusted(best_maxstat);
+      } else {
+        // Compute p-values
+        double pvalue_lau92 = maxstatPValueLau92(best_maxstat, minprop, 1 - minprop);
+        double pvalue_lau94 = maxstatPValueLau94(best_maxstat, minprop, 1 - minprop, num_samples_node,
+            num_samples_left);
+
+        // Use minimum of Lau92 and Lau94
+        pvalue = std::min(pvalue_lau92, pvalue_lau94);
+      }
+
+      // Save split stats
+      pvalues.push_back(pvalue);
+      values.push_back(best_split_value);
+      candidate_varIDs.push_back(varID);
+      test_statistics.push_back(best_maxstat);
+    }
+  }
+
+  double adjusted_best_pvalue = std::numeric_limits<double>::max();
+  size_t best_varID = 0;
+  double best_value = 0;
+  double best_maxstat = 0;
+
+  if (pvalues.size() > 0) {
+    // Adjust p-values with Benjamini/Hochberg
+    std::vector<double> adjusted_pvalues = adjustPvalues(pvalues);
+
+    double min_pvalue = std::numeric_limits<double>::max();
+    for (size_t i = 0; i < pvalues.size(); ++i) {
+      if (pvalues[i] < min_pvalue) {
+        min_pvalue = pvalues[i];
+        best_varID = candidate_varIDs[i];
+        best_value = values[i];
+        adjusted_best_pvalue = adjusted_pvalues[i];
+        best_maxstat = test_statistics[i];
+      }
+    }
+  }
+
+  // Stop and save CHF if no good split found (this is terminal node).
+  if (adjusted_best_pvalue > alpha) {
+    computeDeathCounts(nodeID);
+    computeSurvival(nodeID);
+    return true;
+  } else {
+    // If not terminal node save best values
+    split_varIDs[nodeID] = best_varID;
+    split_values[nodeID] = best_value;
+
+    // Compute decrease of impurity for this node and add to variable importance if needed
+    if (importance_mode == IMP_GINI || importance_mode == IMP_GINI_CORRECTED) {
+      addImpurityImportance(nodeID, best_varID, best_maxstat);
+    }
+
+    return false;
+  }
+}
+
+void TreeSurvival::computeDeathCounts(size_t nodeID) {
+
+  // Initialize
+  for (size_t i = 0; i < num_timepoints; ++i) {
+    num_deaths[i] = 0;
+    num_samples_at_risk[i] = 0;
+  }
+
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    double survival_time = data->get_y(sampleID, 0);
+
+    size_t t = 0;
+    while (t < num_timepoints && (*unique_timepoints)[t] < survival_time) {
+      ++num_samples_at_risk[t];
+      ++t;
+    }
+
+    // Now t is the survival time, add to at risk and to death if death
+    if (t < num_timepoints) {
+      ++num_samples_at_risk[t];
+      if (data->get_y(sampleID, 1) == 1) {
+        ++num_deaths[t];
+      }
+    }
+  }
+}
+
+void TreeSurvival::computeChildDeathCounts(size_t nodeID, size_t varID, std::vector<double>& possible_split_values,
+    std::vector<size_t>& num_samples_right_child, std::vector<size_t>& delta_samples_at_risk_right_child,
+    std::vector<size_t>& num_deaths_right_child, size_t num_splits) {
+
+  // Count deaths in right child per timepoint and possbile split
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    double value = data->get_x(sampleID, varID);
+    size_t survival_timeID = (*response_timepointIDs)[sampleID];
+
+    // Count deaths until split_value reached
+    for (size_t i = 0; i < num_splits; ++i) {
+
+      if (value > possible_split_values[i]) {
+        ++num_samples_right_child[i];
+        ++delta_samples_at_risk_right_child[i * num_timepoints + survival_timeID];
+        if (data->get_y(sampleID, 1) == 1) {
+          ++num_deaths_right_child[i * num_timepoints + survival_timeID];
+        }
+      } else {
+        break;
+      }
+    }
+  }
+}
+
+void TreeSurvival::findBestSplitValueLogRank(size_t nodeID, size_t varID, double& best_value, size_t& best_varID,
+    double& best_logrank) {
+
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+
+  // Create possible split values
+  std::vector<double> possible_split_values;
+  data->getAllValues(possible_split_values, sampleIDs, varID, start_pos[nodeID], end_pos[nodeID]);
+
+  // Try next variable if all equal for this
+  if (possible_split_values.size() < 2) {
+    return;
+  }
+
+  // -1 because no split possible at largest value
+  size_t num_splits = possible_split_values.size() - 1;
+
+  // Initialize
+  std::vector<size_t> num_deaths_right_child(num_splits * num_timepoints);
+  std::vector<size_t> delta_samples_at_risk_right_child(num_splits * num_timepoints);
+  std::vector<size_t> num_samples_right_child(num_splits);
+
+  computeChildDeathCounts(nodeID, varID, possible_split_values, num_samples_right_child,
+      delta_samples_at_risk_right_child, num_deaths_right_child, num_splits);
+
+  // Compute logrank test for all splits and use best
+  for (size_t i = 0; i < num_splits; ++i) {
+    double numerator = 0;
+    double denominator_squared = 0;
+
+    // Stop if minimal node size reached
+    size_t num_samples_left_child = num_samples_node - num_samples_right_child[i];
+    if (num_samples_right_child[i] < min_node_size || num_samples_left_child < min_node_size) {
+      continue;
+    }
+
+    // Compute logrank test statistic for this split
+    size_t num_samples_at_risk_right_child = num_samples_right_child[i];
+    for (size_t t = 0; t < num_timepoints; ++t) {
+      if (num_samples_at_risk[t] < 2 || num_samples_at_risk_right_child < 1) {
+        break;
+      }
+
+      if (num_deaths[t] > 0) {
+        // Numerator and demoninator for log-rank test, notation from Ishwaran et al.
+        double di = (double) num_deaths[t];
+        double di1 = (double) num_deaths_right_child[i * num_timepoints + t];
+        double Yi = (double) num_samples_at_risk[t];
+        double Yi1 = (double) num_samples_at_risk_right_child;
+        numerator += di1 - Yi1 * (di / Yi);
+        denominator_squared += (Yi1 / Yi) * (1.0 - Yi1 / Yi) * ((Yi - di) / (Yi - 1)) * di;
+      }
+
+      // Reduce number of samples at risk for next timepoint
+      num_samples_at_risk_right_child -= delta_samples_at_risk_right_child[i * num_timepoints + t];
+
+    }
+    double logrank = -1;
+    if (denominator_squared != 0) {
+      logrank = fabs(numerator / sqrt(denominator_squared));
+    }
+
+    // Regularization
+    regularize(logrank, varID);
+
+    if (logrank > best_logrank) {
+      best_value = (possible_split_values[i] + possible_split_values[i + 1]) / 2;
+      best_varID = varID;
+      best_logrank = logrank;
+
+      // Use smaller value if average is numerically the same as the larger value
+      if (best_value == possible_split_values[i + 1]) {
+        best_value = possible_split_values[i];
+      }
+    }
+  }
+}
+
+void TreeSurvival::findBestSplitValueLogRankUnordered(size_t nodeID, size_t varID, double& best_value,
+    size_t& best_varID, double& best_logrank) {
+
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+
+  // Create possible split values
+  std::vector<double> factor_levels;
+  data->getAllValues(factor_levels, sampleIDs, varID, start_pos[nodeID], end_pos[nodeID]);
+
+  // Try next variable if all equal for this
+  if (factor_levels.size() < 2) {
+    return;
+  }
+
+  // Number of possible splits is 2^num_levels
+  size_t num_splits = (1ULL << factor_levels.size());
+
+  // Compute logrank test statistic for each possible split
+  // Split where all left (0) or all right (1) are excluded
+  // The second half of numbers is just left/right switched the first half -> Exclude second half
+  for (size_t local_splitID = 1; local_splitID < num_splits / 2; ++local_splitID) {
+
+    // Compute overall splitID by shifting local factorIDs to global positions
+    size_t splitID = 0;
+    for (size_t j = 0; j < factor_levels.size(); ++j) {
+      if ((local_splitID & (1ULL << j))) {
+        double level = factor_levels[j];
+        size_t factorID = floor(level) - 1;
+        splitID = splitID | (1ULL << factorID);
+      }
+    }
+
+    // Initialize
+    std::vector<size_t> num_deaths_right_child(num_timepoints);
+    std::vector<size_t> delta_samples_at_risk_right_child(num_timepoints);
+    size_t num_samples_right_child = 0;
+    double numerator = 0;
+    double denominator_squared = 0;
+
+    // Count deaths in right child per timepoint
+    for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+      size_t sampleID = sampleIDs[pos];
+      size_t survival_timeID = (*response_timepointIDs)[sampleID];
+      double value = data->get_x(sampleID, varID);
+      size_t factorID = floor(value) - 1;
+
+      // If in right child, count
+      // In right child, if bitwise splitID at position factorID is 1
+      if ((splitID & (1ULL << factorID))) {
+        ++num_samples_right_child;
+        ++delta_samples_at_risk_right_child[survival_timeID];
+        if (data->get_y(sampleID, 1) == 1) {
+          ++num_deaths_right_child[survival_timeID];
+        }
+      }
+
+    }
+
+    // Stop if minimal node size reached
+    size_t num_samples_left_child = num_samples_node - num_samples_right_child;
+    if (num_samples_right_child < min_node_size || num_samples_left_child < min_node_size) {
+      continue;
+    }
+
+    // Compute logrank test statistic for this split
+    size_t num_samples_at_risk_right_child = num_samples_right_child;
+    for (size_t t = 0; t < num_timepoints; ++t) {
+      if (num_samples_at_risk[t] < 2 || num_samples_at_risk_right_child < 1) {
+        break;
+      }
+
+      if (num_deaths[t] > 0) {
+        // Numerator and demoninator for log-rank test, notation from Ishwaran et al.
+        double di = (double) num_deaths[t];
+        double di1 = (double) num_deaths_right_child[t];
+        double Yi = (double) num_samples_at_risk[t];
+        double Yi1 = (double) num_samples_at_risk_right_child;
+        numerator += di1 - Yi1 * (di / Yi);
+        denominator_squared += (Yi1 / Yi) * (1.0 - Yi1 / Yi) * ((Yi - di) / (Yi - 1)) * di;
+      }
+
+      // Reduce number of samples at risk for next timepoint
+      num_samples_at_risk_right_child -= delta_samples_at_risk_right_child[t];
+    }
+    double logrank = -1;
+    if (denominator_squared != 0) {
+      logrank = fabs(numerator / sqrt(denominator_squared));
+    }
+
+    // Regularization
+    regularize(logrank, varID);
+
+    if (logrank > best_logrank) {
+      best_value = splitID;
+      best_varID = varID;
+      best_logrank = logrank;
+    }
+  }
+}
+
+void TreeSurvival::findBestSplitValueAUC(size_t nodeID, size_t varID, double& best_value, size_t& best_varID,
+    double& best_auc) {
+
+  // Create possible split values
+  std::vector<double> possible_split_values;
+  data->getAllValues(possible_split_values, sampleIDs, varID, start_pos[nodeID], end_pos[nodeID]);
+
+  // Try next variable if all equal for this
+  if (possible_split_values.size() < 2) {
+    return;
+  }
+
+  size_t num_node_samples = end_pos[nodeID] - start_pos[nodeID];
+  size_t num_splits = possible_split_values.size() - 1;
+  size_t num_possible_pairs = num_node_samples * (num_node_samples - 1) / 2;
+
+  // Initialize
+  std::vector<double> num_count(num_splits, num_possible_pairs);
+  std::vector<double> num_total(num_splits, num_possible_pairs);
+  std::vector<size_t> num_samples_left_child(num_splits);
+
+  // For all pairs
+  for (size_t k = start_pos[nodeID]; k < end_pos[nodeID]; ++k) {
+    size_t sample_k = sampleIDs[k];
+    double time_k = data->get_y(sample_k, 0);
+    double status_k = data->get_y(sample_k, 1);
+    double value_k = data->get_x(sample_k, varID);
+
+    // Count samples in left node
+    for (size_t i = 0; i < num_splits; ++i) {
+      double split_value = possible_split_values[i];
+      if (value_k <= split_value) {
+        ++num_samples_left_child[i];
+      }
+    }
+
+    for (size_t l = k + 1; l < end_pos[nodeID]; ++l) {
+      size_t sample_l = sampleIDs[l];
+      double time_l = data->get_y(sample_l, 0);
+      double status_l = data->get_y(sample_l, 1);
+      double value_l = data->get_x(sample_l, varID);
+
+      // Compute split
+      computeAucSplit(time_k, time_l, status_k, status_l, value_k, value_l, num_splits, possible_split_values,
+          num_count, num_total);
+    }
+  }
+
+  for (size_t i = 0; i < num_splits; ++i) {
+    // Do not consider this split point if fewer than min_node_size samples in one node
+    size_t num_samples_right_child = num_node_samples - num_samples_left_child[i];
+    if (num_samples_left_child[i] < min_node_size || num_samples_right_child < min_node_size) {
+      continue;
+    } else {
+      double auc = fabs((num_count[i] / 2) / num_total[i] - 0.5);
+
+      // Regularization
+      regularize(auc, varID);
+
+      if (auc > best_auc) {
+        best_value = (possible_split_values[i] + possible_split_values[i + 1]) / 2;
+        best_varID = varID;
+        best_auc = auc;
+
+        // Use smaller value if average is numerically the same as the larger value
+        if (best_value == possible_split_values[i + 1]) {
+          best_value = possible_split_values[i];
+        }
+      }
+    }
+  }
+}
+
+void TreeSurvival::computeAucSplit(double time_k, double time_l, double status_k, double status_l, double value_k,
+    double value_l, size_t num_splits, std::vector<double>& possible_split_values, std::vector<double>& num_count,
+    std::vector<double>& num_total) {
+
+  bool ignore_pair = false;
+  bool do_nothing = false;
+
+  double value_smaller = 0;
+  double value_larger = 0;
+  double status_smaller = 0;
+
+  if (time_k < time_l) {
+    value_smaller = value_k;
+    value_larger = value_l;
+    status_smaller = status_k;
+  } else if (time_l < time_k) {
+    value_smaller = value_l;
+    value_larger = value_k;
+    status_smaller = status_l;
+  } else {
+    // Tie in survival time
+    if (status_k == 0 || status_l == 0) {
+      ignore_pair = true;
+    } else {
+      if (splitrule == AUC_IGNORE_TIES) {
+        ignore_pair = true;
+      } else {
+        if (value_k == value_l) {
+          // Tie in survival time and in covariate
+          ignore_pair = true;
+        } else {
+          // Tie in survival time in covariate
+          do_nothing = true;
+        }
+      }
+    }
+  }
+
+  // Do not count if smaller time censored
+  if (status_smaller == 0) {
+    ignore_pair = true;
+  }
+
+  if (ignore_pair) {
+    for (size_t i = 0; i < num_splits; ++i) {
+      --num_count[i];
+      --num_total[i];
+    }
+  } else if (do_nothing) {
+    // Do nothing
+  } else {
+    for (size_t i = 0; i < num_splits; ++i) {
+      double split_value = possible_split_values[i];
+
+      if (value_smaller <= split_value && value_larger > split_value) {
+        ++num_count[i];
+      } else if (value_smaller > split_value && value_larger <= split_value) {
+        --num_count[i];
+      } else if (value_smaller <= split_value && value_larger <= split_value) {
+        break;
+      }
+    }
+  }
+
+}
+
+bool TreeSurvival::findBestSplitExtraTrees(size_t nodeID, std::vector<size_t>& possible_split_varIDs) {
+
+  double best_decrease = -1;
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+  size_t best_varID = 0;
+  double best_value = 0;
+
+  computeDeathCounts(nodeID);
+
+  // Stop if maximum node size or depth reached (will check again for each child node)
+  if (num_samples_node <= min_node_size || (nodeID >= last_left_nodeID && max_depth > 0 && depth >= max_depth)) {
+    computeSurvival(nodeID);
+    return true;
+  }
+
+  // Stop early if no split posssible
+  if (num_samples_node >= 2 * min_node_size) {
+
+    // For all possible split variables
+    for (auto& varID : possible_split_varIDs) {
+
+      // Find best split value, if ordered consider all values as split values, else all 2-partitions
+      if (data->isOrderedVariable(varID)) {
+        findBestSplitValueExtraTrees(nodeID, varID, best_value, best_varID, best_decrease);
+      } else {
+        findBestSplitValueExtraTreesUnordered(nodeID, varID, best_value, best_varID, best_decrease);
+      }
+
+    }
+  }
+
+  // Stop and save CHF if no good split found (this is terminal node).
+  if (best_decrease < 0) {
+    computeSurvival(nodeID);
+    return true;
+  } else {
+    // If not terminal node save best values
+    split_varIDs[nodeID] = best_varID;
+    split_values[nodeID] = best_value;
+
+    // Compute decrease of impurity for this node and add to variable importance if needed
+    if (importance_mode == IMP_GINI || importance_mode == IMP_GINI_CORRECTED) {
+      addImpurityImportance(nodeID, best_varID, best_decrease);
+    }
+
+    // Regularization
+    saveSplitVarID(best_varID);
+
+    return false;
+  }
+}
+
+void TreeSurvival::findBestSplitValueExtraTrees(size_t nodeID, size_t varID, double& best_value, size_t& best_varID,
+    double& best_logrank) {
+
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+
+  // Get min/max values of covariate in node
+  double min;
+  double max;
+  data->getMinMaxValues(min, max, sampleIDs, varID, start_pos[nodeID], end_pos[nodeID]);
+
+  // Try next variable if all equal for this
+  if (min == max) {
+    return;
+  }
+
+  // Create possible split values: Draw randomly between min and max
+  std::vector<double> possible_split_values;
+  std::uniform_real_distribution<double> udist(min, max);
+  possible_split_values.reserve(num_random_splits);
+  for (size_t i = 0; i < num_random_splits; ++i) {
+    possible_split_values.push_back(udist(random_number_generator));
+  }
+  if (num_random_splits > 1) {
+    std::sort(possible_split_values.begin(), possible_split_values.end());
+  }
+
+  size_t num_splits = possible_split_values.size();
+
+  // Initialize
+  std::vector<size_t> num_deaths_right_child(num_splits * num_timepoints);
+  std::vector<size_t> delta_samples_at_risk_right_child(num_splits * num_timepoints);
+  std::vector<size_t> num_samples_right_child(num_splits);
+
+  computeChildDeathCounts(nodeID, varID, possible_split_values, num_samples_right_child,
+      delta_samples_at_risk_right_child, num_deaths_right_child, num_splits);
+
+  // Compute logrank test for all splits and use best
+  for (size_t i = 0; i < num_splits; ++i) {
+    double numerator = 0;
+    double denominator_squared = 0;
+
+    // Stop if minimal node size reached
+    size_t num_samples_left_child = num_samples_node - num_samples_right_child[i];
+    if (num_samples_right_child[i] < min_node_size || num_samples_left_child < min_node_size) {
+      continue;
+    }
+
+    // Compute logrank test statistic for this split
+    size_t num_samples_at_risk_right_child = num_samples_right_child[i];
+    for (size_t t = 0; t < num_timepoints; ++t) {
+      if (num_samples_at_risk[t] < 2 || num_samples_at_risk_right_child < 1) {
+        break;
+      }
+
+      if (num_deaths[t] > 0) {
+        // Numerator and demoninator for log-rank test, notation from Ishwaran et al.
+        double di = (double) num_deaths[t];
+        double di1 = (double) num_deaths_right_child[i * num_timepoints + t];
+        double Yi = (double) num_samples_at_risk[t];
+        double Yi1 = (double) num_samples_at_risk_right_child;
+        numerator += di1 - Yi1 * (di / Yi);
+        denominator_squared += (Yi1 / Yi) * (1.0 - Yi1 / Yi) * ((Yi - di) / (Yi - 1)) * di;
+      }
+
+      // Reduce number of samples at risk for next timepoint
+      num_samples_at_risk_right_child -= delta_samples_at_risk_right_child[i * num_timepoints + t];
+
+    }
+    double logrank = -1;
+    if (denominator_squared != 0) {
+      logrank = fabs(numerator / sqrt(denominator_squared));
+    }
+
+    // Regularization
+    regularize(logrank, varID);
+
+    if (logrank > best_logrank) {
+      best_value = possible_split_values[i];
+      best_varID = varID;
+      best_logrank = logrank;
+    }
+  }
+}
+
+void TreeSurvival::findBestSplitValueExtraTreesUnordered(size_t nodeID, size_t varID, double& best_value,
+    size_t& best_varID, double& best_logrank) {
+
+  size_t num_samples_node = end_pos[nodeID] - start_pos[nodeID];
+  size_t num_unique_values = data->getNumUniqueDataValues(varID);
+
+  // Get all factor indices in node
+  std::vector<bool> factor_in_node(num_unique_values, false);
+  for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+    size_t sampleID = sampleIDs[pos];
+    size_t index = data->getIndex(sampleID, varID);
+    factor_in_node[index] = true;
+  }
+
+  // Vector of indices in and out of node
+  std::vector<size_t> indices_in_node;
+  std::vector<size_t> indices_out_node;
+  indices_in_node.reserve(num_unique_values);
+  indices_out_node.reserve(num_unique_values);
+  for (size_t i = 0; i < num_unique_values; ++i) {
+    if (factor_in_node[i]) {
+      indices_in_node.push_back(i);
+    } else {
+      indices_out_node.push_back(i);
+    }
+  }
+
+  // Generate num_random_splits splits
+  for (size_t i = 0; i < num_random_splits; ++i) {
+    std::vector<size_t> split_subset;
+    split_subset.reserve(num_unique_values);
+
+    // Draw random subsets, sample all partitions with equal probability
+    if (indices_in_node.size() > 1) {
+      size_t num_partitions = (2ULL << (indices_in_node.size() - 1ULL)) - 2ULL; // 2^n-2 (don't allow full or empty)
+      std::uniform_int_distribution<size_t> udist(1, num_partitions);
+      size_t splitID_in_node = udist(random_number_generator);
+      for (size_t j = 0; j < indices_in_node.size(); ++j) {
+        if ((splitID_in_node & (1ULL << j)) > 0) {
+          split_subset.push_back(indices_in_node[j]);
+        }
+      }
+    }
+    if (indices_out_node.size() > 1) {
+      size_t num_partitions = (2ULL << (indices_out_node.size() - 1ULL)) - 1ULL; // 2^n-1 (allow full or empty)
+      std::uniform_int_distribution<size_t> udist(0, num_partitions);
+      size_t splitID_out_node = udist(random_number_generator);
+      for (size_t j = 0; j < indices_out_node.size(); ++j) {
+        if ((splitID_out_node & (1ULL << j)) > 0) {
+          split_subset.push_back(indices_out_node[j]);
+        }
+      }
+    }
+
+    // Assign union of the two subsets to right child
+    size_t splitID = 0;
+    for (auto& idx : split_subset) {
+      splitID |= 1ULL << idx;
+    }
+
+    // Initialize
+    std::vector<size_t> num_deaths_right_child(num_timepoints);
+    std::vector<size_t> delta_samples_at_risk_right_child(num_timepoints);
+    size_t num_samples_right_child = 0;
+    double numerator = 0;
+    double denominator_squared = 0;
+
+    // Count deaths in right child per timepoint
+    for (size_t pos = start_pos[nodeID]; pos < end_pos[nodeID]; ++pos) {
+      size_t sampleID = sampleIDs[pos];
+      size_t survival_timeID = (*response_timepointIDs)[sampleID];
+      double value = data->get_x(sampleID, varID);
+      size_t factorID = floor(value) - 1;
+
+      // If in right child, count
+      // In right child, if bitwise splitID at position factorID is 1
+      if ((splitID & (1ULL << factorID))) {
+        ++num_samples_right_child;
+        ++delta_samples_at_risk_right_child[survival_timeID];
+        if (data->get_y(sampleID, 1) == 1) {
+          ++num_deaths_right_child[survival_timeID];
+        }
+      }
+
+    }
+
+    // Stop if minimal node size reached
+    size_t num_samples_left_child = num_samples_node - num_samples_right_child;
+    if (num_samples_right_child < min_node_size || num_samples_left_child < min_node_size) {
+      continue;
+    }
+
+    // Compute logrank test statistic for this split
+    size_t num_samples_at_risk_right_child = num_samples_right_child;
+    for (size_t t = 0; t < num_timepoints; ++t) {
+      if (num_samples_at_risk[t] < 2 || num_samples_at_risk_right_child < 1) {
+        break;
+      }
+
+      if (num_deaths[t] > 0) {
+        // Numerator and demoninator for log-rank test, notation from Ishwaran et al.
+        double di = (double) num_deaths[t];
+        double di1 = (double) num_deaths_right_child[t];
+        double Yi = (double) num_samples_at_risk[t];
+        double Yi1 = (double) num_samples_at_risk_right_child;
+        numerator += di1 - Yi1 * (di / Yi);
+        denominator_squared += (Yi1 / Yi) * (1.0 - Yi1 / Yi) * ((Yi - di) / (Yi - 1)) * di;
+      }
+
+      // Reduce number of samples at risk for next timepoint
+      num_samples_at_risk_right_child -= delta_samples_at_risk_right_child[t];
+    }
+    double logrank = -1;
+    if (denominator_squared != 0) {
+      logrank = fabs(numerator / sqrt(denominator_squared));
+    }
+
+    // Regularization
+    regularize(logrank, varID);
+
+    if (logrank > best_logrank) {
+      best_value = splitID;
+      best_varID = varID;
+      best_logrank = logrank;
+    }
+  }
+}
+
+void TreeSurvival::addImpurityImportance(size_t nodeID, size_t varID, double decrease) {
+
+  // No variable importance for no split variables
+  size_t tempvarID = data->getUnpermutedVarID(varID);
+
+  // Subtract if corrected importance and permuted variable, else add
+  if (importance_mode == IMP_GINI_CORRECTED && varID >= data->getNumCols()) {
+    (*variable_importance)[tempvarID] -= decrease;
+  } else {
+    (*variable_importance)[tempvarID] += decrease;
+  }
+}
+
+} // namespace ranger
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/TreeSurvival.h
@@ -0,0 +1,115 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#ifndef TREESURVIVAL_H_
+#define TREESURVIVAL_H_
+
+#include <vector>
+
+#include "globals.h"
+#include "Tree.h"
+
+namespace ranger {
+
+class TreeSurvival: public Tree {
+public:
+  TreeSurvival(std::vector<double>* unique_timepoints, std::vector<size_t>* response_timepointIDs);
+
+  // Create from loaded forest
+  TreeSurvival(std::vector<std::vector<size_t>>& child_nodeIDs, std::vector<size_t>& split_varIDs,
+      std::vector<double>& split_values, std::vector<std::vector<double>> chf, std::vector<double>* unique_timepoints,
+      std::vector<size_t>* response_timepointIDs);
+
+  TreeSurvival(const TreeSurvival&) = delete;
+  TreeSurvival& operator=(const TreeSurvival&) = delete;
+
+  virtual ~TreeSurvival() override = default;
+
+  void allocateMemory() override;
+
+  void appendToFileInternal(std::ofstream& file) override;
+  void computePermutationImportanceInternal(std::vector<std::vector<size_t>>* permutations);
+
+  const std::vector<std::vector<double>>& getChf() const {
+    return chf;
+  }
+
+  const std::vector<double>& getPrediction(size_t sampleID) const {
+    size_t terminal_nodeID = prediction_terminal_nodeIDs[sampleID];
+    return chf[terminal_nodeID];
+  }
+
+  size_t getPredictionTerminalNodeID(size_t sampleID) const {
+    return prediction_terminal_nodeIDs[sampleID];
+  }
+
+private:
+
+  void createEmptyNodeInternal() override;
+  void computeSurvival(size_t nodeID);
+  double computePredictionAccuracyInternal(std::vector<double>* prediction_error_casewise) override;
+  
+  bool splitNodeInternal(size_t nodeID, std::vector<size_t>& possible_split_varIDs) override;
+
+  bool findBestSplit(size_t nodeID, std::vector<size_t>& possible_split_varIDs);
+  bool findBestSplitMaxstat(size_t nodeID, std::vector<size_t>& possible_split_varIDs);
+
+  void findBestSplitValueLogRank(size_t nodeID, size_t varID, std::vector<double>& possible_split_values,
+      double& best_value, size_t& best_varID, double& best_logrank);
+  void findBestSplitValueLogRankUnordered(size_t nodeID, size_t varID, std::vector<double>& factor_levels,
+      double& best_value, size_t& best_varID, double& best_logrank);
+  void findBestSplitValueAUC(size_t nodeID, size_t varID, double& best_value, size_t& best_varID, double& best_auc);
+
+  void computeDeathCounts(size_t nodeID);
+  void computeChildDeathCounts(size_t nodeID, size_t varID, std::vector<double>& possible_split_values,
+      std::vector<size_t>& num_samples_right_child, std::vector<size_t>& num_samples_at_risk_right_child,
+      std::vector<size_t>& num_deaths_right_child, size_t num_splits);
+
+  void computeAucSplit(double time_k, double time_l, double status_k, double status_l, double value_k, double value_l,
+      size_t num_splits, std::vector<double>& possible_split_values, std::vector<double>& num_count,
+      std::vector<double>& num_total);
+
+  void findBestSplitValueLogRank(size_t nodeID, size_t varID, double& best_value, size_t& best_varID,
+      double& best_logrank);
+  void findBestSplitValueLogRankUnordered(size_t nodeID, size_t varID, double& best_value, size_t& best_varID,
+      double& best_logrank);
+
+  bool findBestSplitExtraTrees(size_t nodeID, std::vector<size_t>& possible_split_varIDs);
+  void findBestSplitValueExtraTrees(size_t nodeID, size_t varID, double& best_value, size_t& best_varID,
+      double& best_logrank);
+  void findBestSplitValueExtraTreesUnordered(size_t nodeID, size_t varID, double& best_value, size_t& best_varID,
+      double& best_logrank);
+
+  void addImpurityImportance(size_t nodeID, size_t varID, double decrease);
+
+  void cleanUpInternal() override {
+    num_deaths.clear();
+    num_deaths.shrink_to_fit();
+    num_samples_at_risk.clear();
+    num_samples_at_risk.shrink_to_fit();
+  }
+
+  // Unique time points for all individuals (not only this bootstrap), sorted
+  const std::vector<double>* unique_timepoints;
+  size_t num_timepoints;
+  const std::vector<size_t>* response_timepointIDs;
+
+  // For all terminal nodes CHF for all unique timepoints. For other nodes empty vector.
+  std::vector<std::vector<double>> chf;
+
+  // Fields to save to while tree growing
+  std::vector<size_t> num_deaths;
+  std::vector<size_t> num_samples_at_risk;
+};
+
+} // namespace ranger
+
+#endif /* TREESURVIVAL_H_ */
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/globals.h
@@ -0,0 +1,110 @@
+/*-------------------------------------------------------------------------------
+This file is part of ranger.
+
+Copyright (c) [2014-2018] [Marvin N. Wright]
+
+This software may be modified and distributed under the terms of the MIT license.
+
+Please note that the C++ core of ranger is distributed under MIT license and the
+R package "ranger" under GPL3 license.
+#-------------------------------------------------------------------------------*/
+
+#ifndef GLOBALS_H_
+#define GLOBALS_H_
+
+namespace ranger {
+
+#ifndef M_PI
+#define M_PI 3.14159265358979323846
+#endif
+
+// Old/new Win build
+#ifdef WIN_R_BUILD
+  #if __cplusplus < 201103L
+    #define OLD_WIN_R_BUILD
+  #else
+    #define NEW_WIN_R_BUILD
+  #endif
+#endif
+
+typedef unsigned int uint;
+
+// Tree types, probability is not selected by ID
+enum TreeType {
+  TREE_CLASSIFICATION = 1,
+  TREE_REGRESSION = 3,
+  TREE_SURVIVAL = 5,
+  TREE_PROBABILITY = 9
+};
+
+// Memory modes
+enum MemoryMode {
+  MEM_DOUBLE = 0,
+  MEM_FLOAT = 1,
+  MEM_CHAR = 2
+};
+const uint MAX_MEM_MODE = 2;
+
+// Mask and Offset to store 2 bit values in bytes
+static const int mask[4] = {192,48,12,3};
+static const int offset[4] = {6,4,2,0};
+
+// Variable importance
+enum ImportanceMode {
+  IMP_NONE = 0,
+  IMP_GINI = 1,
+  IMP_PERM_BREIMAN = 2,
+  IMP_PERM_LIAW = 4,
+  IMP_PERM_RAW = 3,
+  IMP_GINI_CORRECTED = 5,
+  IMP_PERM_CASEWISE = 6
+};
+const uint MAX_IMP_MODE = 6;
+
+// Split mode
+enum SplitRule {
+  LOGRANK = 1,
+  AUC = 2,
+  AUC_IGNORE_TIES = 3,
+  MAXSTAT = 4,
+  EXTRATREES = 5,
+  BETA = 6,
+  HELLINGER = 7
+};
+
+// Prediction type
+enum PredictionType {
+  RESPONSE = 1,
+  TERMINALNODES = 2
+};
+
+// Default values
+const uint DEFAULT_NUM_TREE = 500;
+const uint DEFAULT_NUM_THREADS = 0;
+const ImportanceMode DEFAULT_IMPORTANCE_MODE = IMP_NONE;
+
+const uint DEFAULT_MIN_NODE_SIZE_CLASSIFICATION = 1;
+const uint DEFAULT_MIN_NODE_SIZE_REGRESSION = 5;
+const uint DEFAULT_MIN_NODE_SIZE_SURVIVAL = 3;
+const uint DEFAULT_MIN_NODE_SIZE_PROBABILITY = 10;
+
+const SplitRule DEFAULT_SPLITRULE = LOGRANK;
+const double DEFAULT_ALPHA = 0.5;
+const double DEFAULT_MINPROP = 0.1;
+
+const uint DEFAULT_MAXDEPTH = 0;
+const PredictionType DEFAULT_PREDICTIONTYPE = RESPONSE;
+const uint DEFAULT_NUM_RANDOM_SPLITS = 1;
+
+const double DEFAULT_SAMPLE_FRACTION_REPLACE = 1;
+const double DEFAULT_SAMPLE_FRACTION_NOREPLACE = 0.632;
+
+// Interval to print progress in seconds
+const double STATUS_INTERVAL = 30.0;
+
+// Threshold for q value split method switch
+const double Q_THRESHOLD = 0.02;
+
+} // namespace ranger
+
+#endif /* GLOBALS_H_ */
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/rangerCpp.cpp
@@ -0,0 +1,284 @@
+/*-------------------------------------------------------------------------------
+ This file is part of Ranger.
+
+ Ranger is free software: you can redistribute it and/or modify
+ it under the terms of the GNU General Public License as published by
+ the Free Software Foundation, either version 3 of the License, or
+ (at your option) any later version.
+
+ Ranger is distributed in the hope that it will be useful,
+ but WITHOUT ANY WARRANTY; without even the implied warranty of
+ MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ GNU General Public License for more details.
+
+ You should have received a copy of the GNU General Public License
+ along with Ranger. If not, see <http://www.gnu.org/licenses/>.
+
+ Written by:
+
+ Marvin N. Wright
+ Institut für Medizinische Biometrie und Statistik
+ Universität zu Lübeck
+ Ratzeburger Allee 160
+ 23562 Lübeck
+
+ http://www.imbs-luebeck.de
+ #-------------------------------------------------------------------------------*/
+
+#include <RcppEigen.h>
+#include <vector>
+#include <sstream>
+#include <memory>
+#include <utility>
+
+#include "globals.h"
+#include "Forest.h"
+#include "ForestClassification.h"
+#include "ForestRegression.h"
+#include "ForestSurvival.h"
+#include "ForestProbability.h"
+#include "Data.h"
+#include "DataChar.h"
+#include "DataRcpp.h"
+#include "DataFloat.h"
+#include "DataSparse.h"
+#include "utility.h"
+
+using namespace ranger;
+
+// [[Rcpp::depends(RcppEigen)]]
+// [[Rcpp::export]]
+Rcpp::List rangerCpp(uint treetype, Rcpp::NumericMatrix& input_x, Rcpp::NumericMatrix& input_y,
+    std::vector<std::string> variable_names, uint mtry, uint num_trees, bool verbose, uint seed, uint num_threads,
+    bool write_forest, uint importance_mode_r, uint min_node_size,
+    std::vector<std::vector<double>>& split_select_weights, bool use_split_select_weights,
+    std::vector<std::string>& always_split_variable_names, bool use_always_split_variable_names,
+    bool prediction_mode, Rcpp::List loaded_forest, Rcpp::RawMatrix snp_data,
+    bool sample_with_replacement, bool probability, std::vector<std::string>& unordered_variable_names,
+    bool use_unordered_variable_names, bool save_memory, uint splitrule_r, std::vector<double>& case_weights,
+    bool use_case_weights, std::vector<double>& class_weights, bool predict_all, bool keep_inbag,
+    std::vector<double>& sample_fraction, double alpha, double minprop, bool holdout, uint prediction_type_r,
+    uint num_random_splits, Eigen::SparseMatrix<double>& sparse_x, 
+    bool use_sparse_data, bool order_snps, bool oob_error, uint max_depth, 
+    std::vector<std::vector<size_t>>& inbag, bool use_inbag,
+    std::vector<double>& regularization_factor, bool use_regularization_factor, bool regularization_usedepth) {
+  
+  Rcpp::List result;
+
+  try {
+    std::unique_ptr<Forest> forest { };
+    std::unique_ptr<Data> data { };
+
+    // Empty split select weights and always split variables if not used
+    if (!use_split_select_weights) {
+      split_select_weights.clear();
+    }
+    if (!use_always_split_variable_names) {
+      always_split_variable_names.clear();
+    }
+    if (!use_unordered_variable_names) {
+      unordered_variable_names.clear();
+    }
+    if (!use_case_weights) {
+      case_weights.clear();
+    }
+    if (!use_inbag) {
+      inbag.clear();
+    }
+    if (!use_regularization_factor) {
+      regularization_factor.clear();
+    }
+
+    std::ostream* verbose_out;
+    if (verbose) {
+      verbose_out = &Rcpp::Rcout;
+    } else {
+      verbose_out = new std::stringstream;
+    }
+
+    size_t num_rows;
+    size_t num_cols;
+    if (use_sparse_data) {
+      num_rows = sparse_x.rows();
+      num_cols = sparse_x.cols();
+    } else {
+      num_rows = input_x.nrow();
+      num_cols = input_x.ncol();
+    }
+
+    // Initialize data 
+    if (use_sparse_data) {
+      data = make_unique<DataSparse>(sparse_x, input_y, variable_names, num_rows, num_cols);
+    } else {
+      data = make_unique<DataRcpp>(input_x, input_y, variable_names, num_rows, num_cols);
+    }
+
+    // If there is snp data, add it
+    if (snp_data.nrow() > 1) {
+      data->addSnpData(snp_data.begin(), snp_data.ncol());
+
+      // Load SNP order if available
+      if (prediction_mode && loaded_forest.containsElementNamed("snp.order")) {
+        std::vector<std::vector<size_t>> snp_order = loaded_forest["snp.order"];
+        data->setSnpOrder(snp_order);
+      }
+    }
+
+    switch (treetype) {
+    case TREE_CLASSIFICATION:
+      if (probability) {
+        forest = make_unique<ForestProbability>();
+      } else {
+        forest = make_unique<ForestClassification>();
+      }
+      break;
+    case TREE_REGRESSION:
+      forest = make_unique<ForestRegression>();
+      break;
+    case TREE_SURVIVAL:
+      forest = make_unique<ForestSurvival>();
+      break;
+    case TREE_PROBABILITY:
+      forest = make_unique<ForestProbability>();
+      break;
+    }
+
+    ImportanceMode importance_mode = (ImportanceMode) importance_mode_r;
+    SplitRule splitrule = (SplitRule) splitrule_r;
+    PredictionType prediction_type = (PredictionType) prediction_type_r;
+
+    // Init Ranger
+    forest->initR(std::move(data), mtry, num_trees, verbose_out, seed, num_threads,
+        importance_mode, min_node_size, split_select_weights, always_split_variable_names,
+        prediction_mode, sample_with_replacement, unordered_variable_names, save_memory, splitrule, case_weights,
+        inbag, predict_all, keep_inbag, sample_fraction, alpha, minprop, holdout, prediction_type, num_random_splits, 
+        order_snps, max_depth, regularization_factor, regularization_usedepth);
+
+    // Load forest object if in prediction mode
+    if (prediction_mode) {
+      std::vector<std::vector<std::vector<size_t>> > child_nodeIDs = loaded_forest["child.nodeIDs"];
+      std::vector<std::vector<size_t>> split_varIDs = loaded_forest["split.varIDs"];
+      std::vector<std::vector<double>> split_values = loaded_forest["split.values"];
+      std::vector<bool> is_ordered = loaded_forest["is.ordered"];
+
+      if (treetype == TREE_CLASSIFICATION) {
+        std::vector<double> class_values = loaded_forest["class.values"];
+        auto& temp = dynamic_cast<ForestClassification&>(*forest);
+        temp.loadForest(num_trees, child_nodeIDs, split_varIDs, split_values, class_values,
+            is_ordered);
+      } else if (treetype == TREE_REGRESSION) {
+        auto& temp = dynamic_cast<ForestRegression&>(*forest);
+        temp.loadForest(num_trees, child_nodeIDs, split_varIDs, split_values, is_ordered);
+      } else if (treetype == TREE_SURVIVAL) {
+        std::vector<std::vector<std::vector<double>> > chf = loaded_forest["chf"];
+        std::vector<double> unique_timepoints = loaded_forest["unique.death.times"];
+        auto& temp = dynamic_cast<ForestSurvival&>(*forest);
+        temp.loadForest(num_trees, child_nodeIDs, split_varIDs, split_values, chf,
+            unique_timepoints, is_ordered);
+      } else if (treetype == TREE_PROBABILITY) {
+        std::vector<double> class_values = loaded_forest["class.values"];
+        std::vector<std::vector<std::vector<double>>> terminal_class_counts = loaded_forest["terminal.class.counts"];
+        auto& temp = dynamic_cast<ForestProbability&>(*forest);
+        temp.loadForest(num_trees, child_nodeIDs, split_varIDs, split_values, class_values,
+            terminal_class_counts, is_ordered);
+      }
+    } else {
+      // Set class weights
+      if (treetype == TREE_CLASSIFICATION && !class_weights.empty()) {
+        auto& temp = dynamic_cast<ForestClassification&>(*forest);
+        temp.setClassWeights(class_weights);
+      } else if (treetype == TREE_PROBABILITY && !class_weights.empty()) {
+        auto& temp = dynamic_cast<ForestProbability&>(*forest);
+        temp.setClassWeights(class_weights);
+      }
+    }
+
+    // Run Ranger
+    forest->run(false, oob_error);
+
+    if (use_split_select_weights && importance_mode != IMP_NONE) {
+      if (verbose_out) {
+        *verbose_out
+            << "Warning: Split select weights used. Variable importance measures are only comparable for variables with equal weights."
+            << std::endl;
+      }
+    }
+
+    // Use first non-empty dimension of predictions
+    const std::vector<std::vector<std::vector<double>>>& predictions = forest->getPredictions();
+    if (predictions.size() == 1) {
+      if (predictions[0].size() == 1) {
+        result.push_back(forest->getPredictions()[0][0], "predictions");
+      } else {
+        result.push_back(forest->getPredictions()[0], "predictions");
+      }
+    } else {
+      result.push_back(forest->getPredictions(), "predictions");
+    }
+
+    // Return output
+    result.push_back(forest->getNumTrees(), "num.trees");
+    result.push_back(forest->getNumIndependentVariables(), "num.independent.variables");
+    if (treetype == TREE_SURVIVAL) {
+      auto& temp = dynamic_cast<ForestSurvival&>(*forest);
+      result.push_back(temp.getUniqueTimepoints(), "unique.death.times");
+    }
+    if (!prediction_mode) {
+      result.push_back(forest->getMtry(), "mtry");
+      result.push_back(forest->getMinNodeSize(), "min.node.size");
+      if (importance_mode != IMP_NONE) {
+        result.push_back(forest->getVariableImportance(), "variable.importance");
+        if (importance_mode == IMP_PERM_CASEWISE) {
+          result.push_back(forest->getVariableImportanceCasewise(), "variable.importance.local");
+        }
+      }
+      result.push_back(forest->getOverallPredictionError(), "prediction.error");
+    }
+
+    if (keep_inbag) {
+      result.push_back(forest->getInbagCounts(), "inbag.counts");
+    }
+
+    // Save forest if needed
+    if (write_forest) {
+      Rcpp::List forest_object;
+      forest_object.push_back(forest->getNumTrees(), "num.trees");
+      forest_object.push_back(forest->getChildNodeIDs(), "child.nodeIDs");
+      forest_object.push_back(forest->getSplitVarIDs(), "split.varIDs");
+      forest_object.push_back(forest->getSplitValues(), "split.values");
+      forest_object.push_back(forest->getIsOrderedVariable(), "is.ordered");
+
+      if (snp_data.nrow() > 1 && order_snps) {
+        // Exclude permuted SNPs (if any)
+        std::vector<std::vector<size_t>> snp_order = forest->getSnpOrder();
+        forest_object.push_back(std::vector<std::vector<size_t>>(snp_order.begin(), snp_order.begin() + snp_data.ncol()), "snp.order");
+      }
+      
+      if (treetype == TREE_CLASSIFICATION) {
+        auto& temp = dynamic_cast<ForestClassification&>(*forest);
+        forest_object.push_back(temp.getClassValues(), "class.values");
+      } else if (treetype == TREE_PROBABILITY) {
+        auto& temp = dynamic_cast<ForestProbability&>(*forest);
+        forest_object.push_back(temp.getClassValues(), "class.values");
+        forest_object.push_back(temp.getTerminalClassCounts(), "terminal.class.counts");
+      } else if (treetype == TREE_SURVIVAL) {
+        auto& temp = dynamic_cast<ForestSurvival&>(*forest);
+        forest_object.push_back(temp.getChf(), "chf");
+        forest_object.push_back(temp.getUniqueTimepoints(), "unique.death.times");
+      }
+      result.push_back(forest_object, "forest");
+    }
+    
+    if (!verbose) {
+      delete verbose_out;
+    }
+  } catch (std::exception& e) {
+    if (strcmp(e.what(), "User interrupt.") != 0) {
+      Rcpp::Rcerr << "Error: " << e.what() << " Ranger will EXIT now." << std::endl;
+    }
+    return result;
+  }
+
+  return result;
+}
+
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/utility.cpp
@@ -0,0 +1,690 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#include <math.h>
+#include <iostream>
+#include <sstream>
+#include <unordered_set>
+#include <unordered_map>
+#include <algorithm>
+#include <random>
+#include <stdexcept>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "utility.h"
+#include "globals.h"
+#include "Data.h"
+
+namespace ranger {
+
+void equalSplit(std::vector<uint>& result, uint start, uint end, uint num_parts) {
+
+  result.reserve(num_parts + 1);
+
+  // Return range if only 1 part
+  if (num_parts == 1) {
+    result.push_back(start);
+    result.push_back(end + 1);
+    return;
+  }
+
+  // Return vector from start to end+1 if more parts than elements
+  if (num_parts > end - start + 1) {
+    for (uint i = start; i <= end + 1; ++i) {
+      result.push_back(i);
+    }
+    return;
+  }
+
+  uint length = (end - start + 1);
+  uint part_length_short = length / num_parts;
+  uint part_length_long = (uint) ceil(length / ((double) num_parts));
+  uint cut_pos = length % num_parts;
+
+  // Add long ranges
+  for (uint i = start; i < start + cut_pos * part_length_long; i = i + part_length_long) {
+    result.push_back(i);
+  }
+
+  // Add short ranges
+  for (uint i = start + cut_pos * part_length_long; i <= end + 1; i = i + part_length_short) {
+    result.push_back(i);
+  }
+}
+
+void loadDoubleVectorFromFile(std::vector<double>& result, std::string filename) { // #nocov start
+
+  // Open input file
+  std::ifstream input_file;
+  input_file.open(filename);
+  if (!input_file.good()) {
+    throw std::runtime_error("Could not open file: " + filename);
+  }
+
+  // Read the first line, ignore the rest
+  std::string line;
+  getline(input_file, line);
+  std::stringstream line_stream(line);
+  double token;
+  while (line_stream >> token) {
+    result.push_back(token);
+  }
+} // #nocov end
+
+void drawWithoutReplacement(std::vector<size_t>& result, std::mt19937_64& random_number_generator, size_t max,
+    size_t num_samples) {
+  if (num_samples < max / 10) {
+    drawWithoutReplacementSimple(result, random_number_generator, max, num_samples);
+  } else {
+    //drawWithoutReplacementKnuth(result, random_number_generator, max, skip, num_samples);
+    drawWithoutReplacementFisherYates(result, random_number_generator, max, num_samples);
+  }
+}
+
+void drawWithoutReplacementSkip(std::vector<size_t>& result, std::mt19937_64& random_number_generator, size_t max,
+    const std::vector<size_t>& skip, size_t num_samples) {
+  if (num_samples < max / 10) {
+    drawWithoutReplacementSimple(result, random_number_generator, max, skip, num_samples);
+  } else {
+    //drawWithoutReplacementKnuth(result, random_number_generator, max, skip, num_samples);
+    drawWithoutReplacementFisherYates(result, random_number_generator, max, skip, num_samples);
+  }
+}
+
+void drawWithoutReplacementSimple(std::vector<size_t>& result, std::mt19937_64& random_number_generator, size_t max,
+    size_t num_samples) {
+
+  result.reserve(num_samples);
+
+  // Set all to not selected
+  std::vector<bool> temp;
+  temp.resize(max, false);
+
+  std::uniform_int_distribution<size_t> unif_dist(0, max - 1);
+  for (size_t i = 0; i < num_samples; ++i) {
+    size_t draw;
+    do {
+      draw = unif_dist(random_number_generator);
+    } while (temp[draw]);
+    temp[draw] = true;
+    result.push_back(draw);
+  }
+}
+
+void drawWithoutReplacementSimple(std::vector<size_t>& result, std::mt19937_64& random_number_generator, size_t max,
+    const std::vector<size_t>& skip, size_t num_samples) {
+
+  result.reserve(num_samples);
+
+  // Set all to not selected
+  std::vector<bool> temp;
+  temp.resize(max, false);
+
+  std::uniform_int_distribution<size_t> unif_dist(0, max - 1 - skip.size());
+  for (size_t i = 0; i < num_samples; ++i) {
+    size_t draw;
+    do {
+      draw = unif_dist(random_number_generator);
+      for (auto& skip_value : skip) {
+        if (draw >= skip_value) {
+          ++draw;
+        }
+      }
+    } while (temp[draw]);
+    temp[draw] = true;
+    result.push_back(draw);
+  }
+}
+
+void drawWithoutReplacementFisherYates(std::vector<size_t>& result, std::mt19937_64& random_number_generator,
+    size_t max, size_t num_samples) {
+
+  // Create indices
+  result.resize(max);
+  std::iota(result.begin(), result.end(), 0);
+
+  // Draw without replacement using Fisher Yates algorithm
+  std::uniform_real_distribution<double> distribution(0.0, 1.0);
+  for (size_t i = 0; i < num_samples; ++i) {
+    size_t j = i + distribution(random_number_generator) * (max - i);
+    std::swap(result[i], result[j]);
+  }
+
+  result.resize(num_samples);
+}
+
+void drawWithoutReplacementFisherYates(std::vector<size_t>& result, std::mt19937_64& random_number_generator,
+    size_t max, const std::vector<size_t>& skip, size_t num_samples) {
+
+  // Create indices
+  result.resize(max);
+  std::iota(result.begin(), result.end(), 0);
+
+  // Skip indices
+  for (size_t i = 0; i < skip.size(); ++i) {
+    result.erase(result.begin() + skip[skip.size() - 1 - i]);
+  }
+
+  // Draw without replacement using Fisher Yates algorithm
+  std::uniform_real_distribution<double> distribution(0.0, 1.0);
+  for (size_t i = 0; i < num_samples; ++i) {
+    size_t j = i + distribution(random_number_generator) * (max - skip.size() - i);
+    std::swap(result[i], result[j]);
+  }
+
+  result.resize(num_samples);
+}
+
+void drawWithoutReplacementWeighted(std::vector<size_t>& result, std::mt19937_64& random_number_generator,
+    size_t max_index, size_t num_samples, const std::vector<double>& weights) {
+
+  result.reserve(num_samples);
+
+  // Set all to not selected
+  std::vector<bool> temp;
+  temp.resize(max_index + 1, false);
+
+  std::discrete_distribution<> weighted_dist(weights.begin(), weights.end());
+  for (size_t i = 0; i < num_samples; ++i) {
+    size_t draw;
+    do {
+      draw = weighted_dist(random_number_generator);
+    } while (temp[draw]);
+    temp[draw] = true;
+    result.push_back(draw);
+  }
+}
+
+double mostFrequentValue(const std::unordered_map<double, size_t>& class_count,
+    std::mt19937_64 random_number_generator) {
+  std::vector<double> major_classes;
+
+  // Find maximum count
+  size_t max_count = 0;
+  for (auto& class_value : class_count) {
+    if (class_value.second > max_count) {
+      max_count = class_value.second;
+      major_classes.clear();
+      major_classes.push_back(class_value.first);
+    } else if (class_value.second == max_count) {
+      major_classes.push_back(class_value.first);
+    }
+  }
+
+  if (major_classes.size() == 1) {
+    return major_classes[0];
+  } else {
+    // Choose randomly
+    std::uniform_int_distribution<size_t> unif_dist(0, major_classes.size() - 1);
+    return major_classes[unif_dist(random_number_generator)];
+  }
+}
+
+double computeConcordanceIndex(const Data& data, const std::vector<double>& sum_chf,
+    const std::vector<size_t>& sample_IDs, std::vector<double>* prediction_error_casewise) {
+
+  // Compute concordance index
+  double concordance = 0;
+  double permissible = 0;
+  
+  std::vector<double> concordance_casewise;
+  std::vector<double> permissible_casewise;
+  if (prediction_error_casewise) {
+    concordance_casewise.resize(prediction_error_casewise->size(), 0);
+    permissible_casewise.resize(prediction_error_casewise->size(), 0);
+  }
+  
+  for (size_t i = 0; i < sum_chf.size(); ++i) {
+    size_t sample_i = i;
+    if (!sample_IDs.empty()) {
+      sample_i = sample_IDs[i];
+    }
+    double time_i = data.get_y(sample_i, 0);
+    double status_i = data.get_y(sample_i, 1);
+
+    double conc, perm;
+    if (prediction_error_casewise) {
+      conc = concordance_casewise[i];
+      perm = permissible_casewise[i];
+    } else {
+      conc = 0;
+      perm = 0;
+    }
+    
+    for (size_t j = i + 1; j < sum_chf.size(); ++j) {
+      size_t sample_j = j;
+      if (!sample_IDs.empty()) {
+        sample_j = sample_IDs[j];
+      }
+      double time_j = data.get_y(sample_j, 0);
+      double status_j = data.get_y(sample_j, 1);
+
+      if (time_i < time_j && status_i == 0) {
+        continue;
+      }
+      if (time_j < time_i && status_j == 0) {
+        continue;
+      }
+      if (time_i == time_j && status_i == status_j) {
+        continue;
+      }
+
+      double co;
+      if (time_i < time_j && sum_chf[i] > sum_chf[j]) {
+        co = 1;
+      } else if (time_j < time_i && sum_chf[j] > sum_chf[i]) {
+        co = 1;
+      } else if (sum_chf[i] == sum_chf[j]) {
+        co = 0.5;
+      } else {
+        co = 0;
+      }
+      
+      conc += co;
+      perm += 1;
+      
+      if (prediction_error_casewise) {
+        concordance_casewise[j] += co;
+        permissible_casewise[j] += 1;
+      }
+    }
+    
+    concordance += conc;
+    permissible += perm;
+    if (prediction_error_casewise) {
+      concordance_casewise[i] = conc;
+      permissible_casewise[i] = perm;
+    }
+  }
+  
+  if (prediction_error_casewise) {
+    for (size_t i = 0; i < prediction_error_casewise->size(); ++i) {
+      (*prediction_error_casewise)[i] = 1 - concordance_casewise[i] / permissible_casewise[i];
+    }
+  }
+  
+  return (concordance / permissible);
+}
+
+std::string uintToString(uint number) {
+#if WIN_R_BUILD == 1
+  std::stringstream temp;
+  temp << number;
+  return temp.str();
+#else
+  return std::to_string(number);
+#endif
+}
+
+std::string beautifyTime(uint seconds) { // #nocov start
+  std::string result;
+
+  // Add seconds, minutes, hours, days if larger than zero
+  uint out_seconds = (uint) seconds % 60;
+  result = uintToString(out_seconds) + " seconds";
+  uint out_minutes = (seconds / 60) % 60;
+  if (seconds / 60 == 0) {
+    return result;
+  } else if (out_minutes == 1) {
+    result = "1 minute, " + result;
+  } else {
+    result = uintToString(out_minutes) + " minutes, " + result;
+  }
+  uint out_hours = (seconds / 3600) % 24;
+  if (seconds / 3600 == 0) {
+    return result;
+  } else if (out_hours == 1) {
+    result = "1 hour, " + result;
+  } else {
+    result = uintToString(out_hours) + " hours, " + result;
+  }
+  uint out_days = (seconds / 86400);
+  if (out_days == 0) {
+    return result;
+  } else if (out_days == 1) {
+    result = "1 day, " + result;
+  } else {
+    result = uintToString(out_days) + " days, " + result;
+  }
+  return result;
+} // #nocov end
+
+// #nocov start
+size_t roundToNextMultiple(size_t value, uint multiple) {
+
+  if (multiple == 0) {
+    return value;
+  }
+
+  size_t remainder = value % multiple;
+  if (remainder == 0) {
+    return value;
+  }
+
+  return value + multiple - remainder;
+}
+// #nocov end
+
+void splitString(std::vector<std::string>& result, const std::string& input, char split_char) { // #nocov start
+
+  std::istringstream ss(input);
+  std::string token;
+
+  while (std::getline(ss, token, split_char)) {
+    result.push_back(token);
+  }
+} // #nocov end
+
+void splitString(std::vector<double>& result, const std::string& input, char split_char) { // #nocov start
+
+  std::istringstream ss(input);
+  std::string token;
+
+  while (std::getline(ss, token, split_char)) {
+    result.push_back(std::stod(token));
+  }
+} // #nocov end
+
+void shuffleAndSplit(std::vector<size_t>& first_part, std::vector<size_t>& second_part, size_t n_all, size_t n_first,
+    std::mt19937_64 random_number_generator) {
+
+  // Reserve space
+  first_part.resize(n_all);
+
+  // Fill with 0..n_all-1 and shuffle
+  std::iota(first_part.begin(), first_part.end(), 0);
+  std::shuffle(first_part.begin(), first_part.end(), random_number_generator);
+
+  // Copy to second part
+  second_part.resize(n_all - n_first);
+  std::copy(first_part.begin() + n_first, first_part.end(), second_part.begin());
+
+  // Resize first part
+  first_part.resize(n_first);
+}
+
+void shuffleAndSplitAppend(std::vector<size_t>& first_part, std::vector<size_t>& second_part, size_t n_all,
+    size_t n_first, const std::vector<size_t>& mapping, std::mt19937_64 random_number_generator) {
+  // Old end is start position for new data
+  size_t first_old_size = first_part.size();
+  size_t second_old_size = second_part.size();
+
+  // Reserve space
+  first_part.resize(first_old_size + n_all);
+  std::vector<size_t>::iterator first_start_pos = first_part.begin() + first_old_size;
+
+  // Fill with 0..n_all-1 and shuffle
+  std::iota(first_start_pos, first_part.end(), 0);
+  std::shuffle(first_start_pos, first_part.end(), random_number_generator);
+
+  // Mapping
+  for (std::vector<size_t>::iterator j = first_start_pos; j != first_part.end(); ++j) {
+    *j = mapping[*j];
+  }
+
+  // Copy to second part
+  second_part.resize(second_part.size() + n_all - n_first);
+  std::vector<size_t>::iterator second_start_pos = second_part.begin() + second_old_size;
+  std::copy(first_start_pos + n_first, first_part.end(), second_start_pos);
+
+  // Resize first part
+  first_part.resize(first_old_size + n_first);
+}
+
+std::string checkUnorderedVariables(const Data& data, const std::vector<std::string>& unordered_variable_names) { // #nocov start
+  size_t num_rows = data.getNumRows();
+  std::vector<size_t> sampleIDs(num_rows);
+  std::iota(sampleIDs.begin(), sampleIDs.end(), 0);
+
+  // Check for all unordered variables
+  for (auto& variable_name : unordered_variable_names) {
+    size_t varID = data.getVariableID(variable_name);
+    std::vector<double> all_values;
+    data.getAllValues(all_values, sampleIDs, varID, 0, sampleIDs.size());
+
+    // Check level count
+    size_t max_level_count = 8 * sizeof(size_t) - 1;
+    if (all_values.size() > max_level_count) {
+      return "Too many levels in unordered categorical variable " + variable_name + ". Only "
+          + uintToString(max_level_count) + " levels allowed on this system.";
+    }
+
+    // Check positive integers
+    if (!checkPositiveIntegers(all_values)) {
+      return "Not all values in unordered categorical variable " + variable_name + " are positive integers.";
+    }
+  }
+  return "";
+} // #nocov end
+
+bool checkPositiveIntegers(const std::vector<double>& all_values) { // #nocov start
+  for (auto& value : all_values) {
+    if (value < 1 || !(floor(value) == value)) {
+      return false;
+    }
+  }
+  return true;
+} // #nocov end
+
+double maxstatPValueLau92(double b, double minprop, double maxprop) {
+
+  if (b < 1) {
+    return 1.0;
+  }
+
+  // Compute only once (minprop/maxprop don't change during runtime)
+  static double logprop = log((maxprop * (1 - minprop)) / ((1 - maxprop) * minprop));
+
+  double db = dstdnorm(b);
+  double p = 4 * db / b + db * (b - 1 / b) * logprop;
+
+  if (p > 0) {
+    return p;
+  } else {
+    return 0;
+  }
+}
+
+double maxstatPValueLau94(double b, double minprop, double maxprop, size_t N, const std::vector<size_t>& m) {
+
+  double D = 0;
+  for (size_t i = 0; i < m.size() - 1; ++i) {
+    double m1 = m[i];
+    double m2 = m[i + 1];
+
+    double t = sqrt(1.0 - m1 * (N - m2) / ((N - m1) * m2));
+    D += 1 / M_PI * exp(-b * b / 2) * (t - (b * b / 4 - 1) * (t * t * t) / 6);
+  }
+
+  return 2 * (1 - pstdnorm(b)) + D;
+}
+
+double maxstatPValueUnadjusted(double b) {
+  return 2 * pstdnorm(-b);
+}
+
+double dstdnorm(double x) {
+  return exp(-0.5 * x * x) / sqrt(2 * M_PI);
+}
+
+double pstdnorm(double x) {
+  return 0.5 * (1 + erf(x / sqrt(2.0)));
+}
+
+std::vector<double> adjustPvalues(std::vector<double>& unadjusted_pvalues) {
+  size_t num_pvalues = unadjusted_pvalues.size();
+  std::vector<double> adjusted_pvalues(num_pvalues, 0);
+
+  // Get order of p-values
+  std::vector<size_t> indices = order(unadjusted_pvalues, true);
+
+  // Compute adjusted p-values
+  adjusted_pvalues[indices[0]] = unadjusted_pvalues[indices[0]];
+  for (size_t i = 1; i < indices.size(); ++i) {
+    size_t idx = indices[i];
+    size_t idx_last = indices[i - 1];
+
+    adjusted_pvalues[idx] = std::min(adjusted_pvalues[idx_last],
+        (double) num_pvalues / (double) (num_pvalues - i) * unadjusted_pvalues[idx]);
+  }
+  return adjusted_pvalues;
+}
+
+std::vector<double> logrankScores(const std::vector<double>& time, const std::vector<double>& status) {
+  size_t n = time.size();
+  std::vector<double> scores(n);
+
+  // Get order of timepoints
+  std::vector<size_t> indices = order(time, false);
+
+  // Compute scores
+  double cumsum = 0;
+  size_t last_unique = -1;
+  for (size_t i = 0; i < n; ++i) {
+
+    // Continue if next value is the same
+    if (i < n - 1 && time[indices[i]] == time[indices[i + 1]]) {
+      continue;
+    }
+
+    // Compute sum and scores for all non-unique values in a row
+    for (size_t j = last_unique + 1; j <= i; ++j) {
+      cumsum += status[indices[j]] / (n - i);
+    }
+    for (size_t j = last_unique + 1; j <= i; ++j) {
+      scores[indices[j]] = status[indices[j]] - cumsum;
+    }
+
+    // Save last computed value
+    last_unique = i;
+  }
+
+  return scores;
+}
+
+void maxstat(const std::vector<double>& scores, const std::vector<double>& x, const std::vector<size_t>& indices,
+    double& best_maxstat, double& best_split_value, double minprop, double maxprop) {
+  size_t n = x.size();
+
+  double sum_all_scores = 0;
+  for (size_t i = 0; i < n; ++i) {
+    sum_all_scores += scores[indices[i]];
+  }
+
+  // Compute sum of differences from mean for variance
+  double mean_scores = sum_all_scores / n;
+  double sum_mean_diff = 0;
+  for (size_t i = 0; i < n; ++i) {
+    sum_mean_diff += (scores[i] - mean_scores) * (scores[i] - mean_scores);
+  }
+
+  // Get smallest and largest split to consider, -1 for compatibility with R maxstat
+  size_t minsplit = 0;
+  if (n * minprop > 1) {
+    minsplit = n * minprop - 1;
+  }
+  size_t maxsplit = n * maxprop - 1;
+
+  // For all unique x-values
+  best_maxstat = -1;
+  best_split_value = -1;
+  double sum_scores = 0;
+  size_t n_left = 0;
+  for (size_t i = 0; i <= maxsplit; ++i) {
+
+    sum_scores += scores[indices[i]];
+    n_left++;
+
+    // Dont consider splits smaller than minsplit for splitting (but count)
+    if (i < minsplit) {
+      continue;
+    }
+
+    // Consider only unique values
+    if (i < n - 1 && x[indices[i]] == x[indices[i + 1]]) {
+      continue;
+    }
+
+    // If value is largest possible value, stop
+    if (x[indices[i]] == x[indices[n - 1]]) {
+      break;
+    }
+
+    double S = sum_scores;
+    double E = (double) n_left / (double) n * sum_all_scores;
+    double V = (double) n_left * (double) (n - n_left) / (double) (n * (n - 1)) * sum_mean_diff;
+    double T = fabs((S - E) / sqrt(V));
+
+    if (T > best_maxstat) {
+      best_maxstat = T;
+
+      // Use mid-point split if possible
+      if (i < n - 1) {
+        best_split_value = (x[indices[i]] + x[indices[i + 1]]) / 2;
+      } else {
+        best_split_value = x[indices[i]];
+      }
+    }
+  }
+}
+
+std::vector<size_t> numSamplesLeftOfCutpoint(std::vector<double>& x, const std::vector<size_t>& indices) {
+  std::vector<size_t> num_samples_left;
+  num_samples_left.reserve(x.size());
+
+  for (size_t i = 0; i < x.size(); ++i) {
+    if (i == 0) {
+      num_samples_left.push_back(1);
+    } else if (x[indices[i]] == x[indices[i - 1]]) {
+      ++num_samples_left[num_samples_left.size() - 1];
+    } else {
+      num_samples_left.push_back(num_samples_left[num_samples_left.size() - 1] + 1);
+    }
+  }
+
+  return num_samples_left;
+}
+
+// #nocov start
+std::stringstream& readFromStream(std::stringstream& in, double& token) {
+  if (!(in >> token) && (std::fpclassify(token) == FP_SUBNORMAL)) {
+    in.clear();
+  }
+  return in;
+}
+// #nocov end
+
+double betaLogLik(double y, double mean, double phi) {
+
+  // Avoid 0 and 1
+  if (y < std::numeric_limits<double>::epsilon()) {
+    y = std::numeric_limits<double>::epsilon();
+  } else if (y >= 1) {
+    y = 1 - std::numeric_limits<double>::epsilon();
+  }
+  if (mean < std::numeric_limits<double>::epsilon()) {
+    mean = std::numeric_limits<double>::epsilon();
+  } else if (mean >= 1) {
+    mean = 1 - std::numeric_limits<double>::epsilon();
+  }
+  if (phi < std::numeric_limits<double>::epsilon()) {
+    phi = std::numeric_limits<double>::epsilon();
+  } else if (mean >= 1) {
+    phi = 1 - std::numeric_limits<double>::epsilon();
+  }
+
+  return (lgamma(phi) - lgamma(mean * phi) - lgamma((1 - mean) * phi) + (mean * phi - 1) * log(y)
+      + ((1 - mean) * phi - 1) * log(1 - y));
+}
+
+} // namespace ranger
--- /dev/null
+++ mocca-1.1/src/lib/ranger/src/utility.h
@@ -0,0 +1,573 @@
+/*-------------------------------------------------------------------------------
+ This file is part of ranger.
+
+ Copyright (c) [2014-2018] [Marvin N. Wright]
+
+ This software may be modified and distributed under the terms of the MIT license.
+
+ Please note that the C++ core of ranger is distributed under MIT license and the
+ R package "ranger" under GPL3 license.
+ #-------------------------------------------------------------------------------*/
+
+#ifndef UTILITY_H_
+#define UTILITY_H_
+
+#include <vector>
+#include <iostream>
+#include <fstream>
+#include <algorithm>
+#include <random>
+#include <unordered_set>
+#include <unordered_map>
+#include <cstddef> 
+#include <memory> 
+#include <type_traits> 
+#include <utility> 
+
+#ifdef R_BUILD
+#include <Rinternals.h>
+#endif
+
+#include "globals.h"
+#include "Data.h"
+
+namespace ranger {
+
+/**
+ * Split sequence start..end in num_parts parts with sizes as equal as possible.
+ * @param result Result vector of size num_parts+1. Ranges for the parts are then result[0]..result[1]-1, result[1]..result[2]-1, ..
+ * @param start minimum value
+ * @param end maximum value
+ * @param num_parts number of parts
+ */
+void equalSplit(std::vector<uint>& result, uint start, uint end, uint num_parts);
+
+// #nocov start
+/**
+ * Write a 1d vector to filestream. First the size is written as size_t, then all vector elements.
+ * @param vector Vector with elements of type T to write to file.
+ * @param file ofstream object to write to.
+ */
+
+/**
+ * Write a 1d vector to filestream. First the size is written, then all vector elements.
+ * @param vector Vector of type T to save
+ * @param file ofstream object to write to.
+ */
+template<typename T>
+inline void saveVector1D(const std::vector<T>& vector, std::ofstream& file) {
+  // Save length
+  size_t length = vector.size();
+  file.write((char*) &length, sizeof(length));
+  file.write((char*) vector.data(), length * sizeof(T));
+}
+
+template<>
+inline void saveVector1D(const std::vector<bool>& vector, std::ofstream& file) {
+  // Save length
+  size_t length = vector.size();
+  file.write((char*) &length, sizeof(length));
+
+  // Save vector
+  for (size_t i = 0; i < vector.size(); ++i) {
+    bool v = vector[i];
+    file.write((char*) &v, sizeof(v));
+  }
+}
+
+/**
+ * Read a 1d vector written by saveVector1D() from filestream.
+ * @param result Result vector with elements of type T.
+ * @param file ifstream object to read from.
+ */
+template<typename T>
+inline void readVector1D(std::vector<T>& result, std::ifstream& file) {
+  // Read length
+  size_t length;
+  file.read((char*) &length, sizeof(length));
+  result.resize(length);
+  file.read((char*) result.data(), length * sizeof(T));
+}
+
+template<>
+inline void readVector1D(std::vector<bool>& result, std::ifstream& file) {
+  // Read length
+  size_t length;
+  file.read((char*) &length, sizeof(length));
+
+  // Read vector.
+  for (size_t i = 0; i < length; ++i) {
+    bool temp;
+    file.read((char*) &temp, sizeof(temp));
+    result.push_back(temp);
+  }
+}
+
+/**
+ * Write a 2d vector to filestream. First the size of the first dim is written as size_t, then for all inner vectors the size and elements.
+ * @param vector Vector of vectors of type T to write to file.
+ * @param file ofstream object to write to.
+ */
+template<typename T>
+inline void saveVector2D(const std::vector<std::vector<T>>& vector, std::ofstream& file) {
+  // Save length of first dim
+  size_t length = vector.size();
+  file.write((char*) &length, sizeof(length));
+
+  // Save outer vector
+  for (auto& inner_vector : vector) {
+    // Save inner vector
+    saveVector1D(inner_vector, file);
+  }
+}
+
+/**
+ * Read a 2d vector written by saveVector2D() from filestream.
+ * @param result Result vector of vectors with elements of type T.
+ * @param file ifstream object to read from.
+ */
+template<typename T>
+inline void readVector2D(std::vector<std::vector<T>>& result, std::ifstream& file) {
+  // Read length of first dim
+  size_t length;
+  file.read((char*) &length, sizeof(length));
+  result.resize(length);
+
+  // Read outer vector
+  for (size_t i = 0; i < length; ++i) {
+    // Read inner vector
+    readVector1D(result[i], file);
+  }
+}
+
+/**
+ * Read a double vector from text file. Reads only the first line.
+ * @param result Result vector of doubles with contents
+ * @param filename filename of input file
+ */
+void loadDoubleVectorFromFile(std::vector<double>& result, std::string filename);
+// #nocov end
+
+/**
+ * Draw random numbers in a range without replacements.
+ * @param result Vector to add results to. Will not be cleaned before filling.
+ * @param random_number_generator Random number generator
+ * @param range_length Length of range. Interval to draw from: 0..max-1
+ * @param num_samples Number of samples to draw
+ */
+void drawWithoutReplacement(std::vector<size_t>& result, std::mt19937_64& random_number_generator, size_t range_length,
+    size_t num_samples);
+
+/**
+ * Draw random numbers in a range without replacement and skip values.
+ * @param result Vector to add results to. Will not be cleaned before filling.
+ * @param random_number_generator Random number generator
+ * @param range_length Length of range. Interval to draw from: 0..max-1
+ * @param skip Values to skip
+ * @param num_samples Number of samples to draw
+ */
+void drawWithoutReplacementSkip(std::vector<size_t>& result, std::mt19937_64& random_number_generator,
+    size_t range_length, const std::vector<size_t>& skip, size_t num_samples);
+
+/**
+ * Simple algorithm for sampling without replacement, faster for smaller num_samples
+ * @param result Vector to add results to. Will not be cleaned before filling.
+ * @param random_number_generator Random number generator
+ * @param range_length Length of range. Interval to draw from: 0..max-1
+ * @param num_samples Number of samples to draw
+ */
+void drawWithoutReplacementSimple(std::vector<size_t>& result, std::mt19937_64& random_number_generator, size_t max,
+    size_t num_samples);
+
+/**
+ * Simple algorithm for sampling without replacement (skip values), faster for smaller num_samples
+ * @param result Vector to add results to. Will not be cleaned before filling.
+ * @param random_number_generator Random number generator
+ * @param range_length Length of range. Interval to draw from: 0..max-1
+ * @param skip Values to skip
+ * @param num_samples Number of samples to draw
+ */
+void drawWithoutReplacementSimple(std::vector<size_t>& result, std::mt19937_64& random_number_generator, size_t max,
+    const std::vector<size_t>& skip, size_t num_samples);
+
+/**
+ * Fisher Yates algorithm for sampling without replacement.
+ * @param result Vector to add results to. Will not be cleaned before filling.
+ * @param random_number_generator Random number generator
+ * @param max Length of range. Interval to draw from: 0..max-1
+ * @param num_samples Number of samples to draw
+ */
+void drawWithoutReplacementFisherYates(std::vector<size_t>& result, std::mt19937_64& random_number_generator,
+    size_t max, size_t num_samples);
+
+/**
+ * Fisher Yates algorithm for sampling without replacement (skip values).
+ * @param result Vector to add results to. Will not be cleaned before filling.
+ * @param random_number_generator Random number generator
+ * @param max Length of range. Interval to draw from: 0..max-1
+ * @param skip Values to skip
+ * @param num_samples Number of samples to draw
+ */
+void drawWithoutReplacementFisherYates(std::vector<size_t>& result, std::mt19937_64& random_number_generator,
+    size_t max, const std::vector<size_t>& skip, size_t num_samples);
+
+/**
+ * Draw random numers without replacement and with weighted probabilites from 0..n-1.
+ * @param result Vector to add results to. Will not be cleaned before filling.
+ * @param random_number_generator Random number generator
+ * @param max_index Maximum index to draw
+ * @param num_samples Number of samples to draw
+ * @param weights A weight for each element of indices
+ */
+void drawWithoutReplacementWeighted(std::vector<size_t>& result, std::mt19937_64& random_number_generator,
+    size_t max_index, size_t num_samples, const std::vector<double>& weights);
+
+/**
+ * Draw random numbers of a vector without replacement.
+ * @param result Vector to add results to. Will not be cleaned before filling.
+ * @param input Vector to draw values from.
+ * @param random_number_generator Random number generator
+ * @param num_samples Number of samples to draw
+ */
+template<typename T>
+void drawWithoutReplacementFromVector(std::vector<T>& result, const std::vector<T>& input,
+    std::mt19937_64& random_number_generator, size_t num_samples) {
+
+  // Draw random indices
+  std::vector<size_t> result_idx;
+  result_idx.reserve(num_samples);
+  std::vector<size_t> skip; // Empty vector (no skip)
+  drawWithoutReplacementSkip(result_idx, random_number_generator, input.size(), skip, num_samples);
+
+  // Add vector values to result
+  for (auto& idx : result_idx) {
+    result.push_back(input[idx]);
+  }
+}
+
+/**
+ * Returns the most frequent class index of a vector with counts for the classes. Returns a random class if counts are equal.
+ * @param class_count Vector with class counts
+ * @param random_number_generator Random number generator
+ * @return Most frequent class index. Out of range index if all 0.
+ */
+template<typename T>
+size_t mostFrequentClass(const std::vector<T>& class_count, std::mt19937_64 random_number_generator) {
+  std::vector<size_t> major_classes;
+
+// Find maximum count
+  T max_count = 0;
+  for (size_t i = 0; i < class_count.size(); ++i) {
+    T count = class_count[i];
+    if (count > max_count) {
+      max_count = count;
+      major_classes.clear();
+      major_classes.push_back(i);
+    } else if (count == max_count) {
+      major_classes.push_back(i);
+    }
+  }
+
+  if (max_count == 0) {
+    return class_count.size();
+  } else if (major_classes.size() == 1) {
+    return major_classes[0];
+  } else {
+    // Choose randomly
+    std::uniform_int_distribution<size_t> unif_dist(0, major_classes.size() - 1);
+    return major_classes[unif_dist(random_number_generator)];
+  }
+}
+
+/**
+ * Returns the most frequent value of a map with counts for the values. Returns a random class if counts are equal.
+ * @param class_count Map with classes and counts
+ * @param random_number_generator Random number generator
+ * @return Most frequent value
+ */
+double mostFrequentValue(const std::unordered_map<double, size_t>& class_count,
+    std::mt19937_64 random_number_generator);
+
+/**
+ * Compute concordance index for given data and summed cumulative hazard function/estimate
+ * @param data Reference to Data object
+ * @param sum_chf Summed chf over timepoints for each sample
+ * @param sample_IDs IDs of samples, for example OOB samples
+ * @param prediction_error_casewise An optional output vector with casewise prediction errors.
+ *   If pointer is NULL, casewise prediction errors should not be computed.
+ * @return concordance index
+ */
+double computeConcordanceIndex(const Data& data, const std::vector<double>& sum_chf,
+    const std::vector<size_t>& sample_IDs, std::vector<double>* prediction_error_casewise);
+
+/**
+ * Convert a unsigned integer to string
+ * @param number Number to convert
+ * @return Converted number as string
+ */
+std::string uintToString(uint number);
+
+/**
+ * Beautify output of time.
+ * @param seconds Time in seconds
+ * @return Time in days, hours, minutes and seconds as string
+ */
+std::string beautifyTime(uint seconds);
+
+/**
+ * Round up to next multiple of a number.
+ * @param value Value to be rounded up.
+ * @param multiple Number to multiply.
+ * @return Rounded number
+ */
+size_t roundToNextMultiple(size_t value, uint multiple);
+
+/**
+ * Split string in string parts separated by character.
+ * @param result Splitted string
+ * @param input String to be splitted
+ * @param split_char Char to separate parts
+ */
+void splitString(std::vector<std::string>& result, const std::string& input, char split_char);
+
+/**
+ * Split string in double parts separated by character.
+ * @param result Splitted string
+ * @param input String to be splitted
+ * @param split_char Char to separate parts
+ */
+void splitString(std::vector<double>& result, const std::string& input, char split_char);
+
+/**
+ * Create numbers from 0 to n_all-1, shuffle and split in two parts.
+ * @param first_part First part
+ * @param second_part Second part
+ * @param n_all Number elements
+ * @param n_first Number of elements of first part
+ * @param random_number_generator Random number generator
+ */
+void shuffleAndSplit(std::vector<size_t>& first_part, std::vector<size_t>& second_part, size_t n_all, size_t n_first,
+    std::mt19937_64 random_number_generator);
+
+/**
+ * Create numbers from 0 to n_all-1, shuffle and split in two parts. Append to existing data.
+ * @param first_part First part
+ * @param second_part Second part
+ * @param n_all Number elements
+ * @param n_first Number of elements of first part
+ * @param mapping Values to use instead of 0...n-1
+ * @param random_number_generator Random number generator
+ */
+void shuffleAndSplitAppend(std::vector<size_t>& first_part, std::vector<size_t>& second_part, size_t n_all,
+    size_t n_first, const std::vector<size_t>& mapping, std::mt19937_64 random_number_generator);
+
+/**
+ * Check if not too many factor levels and all values in unordered categorical variables are positive integers.
+ * @param data Reference to data object
+ * @param unordered_variable_names Names of unordered variables
+ * @return Error message, empty if no problem occured
+ */
+std::string checkUnorderedVariables(const Data& data, const std::vector<std::string>& unordered_variable_names);
+
+/**
+ * Check if all values in double vector are positive integers.
+ * @param all_values Double vector to check
+ * @return True if all values are positive integers
+ */
+bool checkPositiveIntegers(const std::vector<double>& all_values);
+
+/**
+ * Compute p-value for maximally selected rank statistics using Lau92 approximation
+ * See Lausen, B. & Schumacher, M. (1992). Biometrics 48, 73-85.
+ * @param b Quantile
+ * @param minprop Minimal proportion of observations left of cutpoint
+ * @param maxprop Maximal proportion of observations left of cutpoint
+ * @return p-value for quantile b
+ */
+double maxstatPValueLau92(double b, double minprop, double maxprop);
+
+/**
+ * Compute p-value for maximally selected rank statistics using Lau94 approximation
+ * See Lausen, B., Sauerbrei, W. & Schumacher, M. (1994). Computational Statistics. 483-496.
+ * @param b Quantile
+ * @param minprop Minimal proportion of observations left of cutpoint
+ * @param maxprop Maximal proportion of observations left of cutpoint
+ * @param N Number of observations
+ * @param m Vector with number of observations smaller or equal than cutpoint, sorted, only for unique cutpoints
+ * @return p-value for quantile b
+ */
+double maxstatPValueLau94(double b, double minprop, double maxprop, size_t N, const std::vector<size_t>& m);
+
+/**
+ * Compute unadjusted p-value for rank statistics
+ * @param b Quantile
+ * @return p-value for quantile b
+ */
+double maxstatPValueUnadjusted(double b);
+
+/**
+ * Standard normal density
+ * @param x Quantile
+ * @return Standard normal density at quantile x
+ */
+double dstdnorm(double x);
+
+/**
+ * Standard normal distribution
+ * @param x Quantile
+ * @return Standard normal distribution at quantile x
+ */
+double pstdnorm(double x);
+
+/**
+ * Adjust p-values with Benjamini/Hochberg
+ * @param unadjusted_pvalues Unadjusted p-values (input)
+ * @param adjusted_pvalues Adjusted p-values (result)
+ */
+std::vector<double> adjustPvalues(std::vector<double>& unadjusted_pvalues);
+
+/**
+ * Get indices of sorted values
+ * @param values Values to sort
+ * @param decreasing Order decreasing
+ * @return Indices of sorted values
+ */
+template<typename T>
+std::vector<size_t> order(const std::vector<T>& values, bool decreasing) {
+// Create index vector
+  std::vector<size_t> indices(values.size());
+  std::iota(indices.begin(), indices.end(), 0);
+
+// Sort index vector based on value vector
+  if (decreasing) {
+    std::sort(std::begin(indices), std::end(indices), [&](size_t i1, size_t i2) {return values[i1] > values[i2];});
+  } else {
+    std::sort(std::begin(indices), std::end(indices), [&](size_t i1, size_t i2) {return values[i1] < values[i2];});
+  }
+  return indices;
+}
+
+/**
+ * Sample ranks starting from 1. Ties are given the average rank.
+ * @param values Values to rank
+ * @return Ranks of input values
+ */
+template<typename T>
+std::vector<double> rank(const std::vector<T>& values) {
+  size_t num_values = values.size();
+
+// Order
+  std::vector<size_t> indices = order(values, false);
+
+// Compute ranks, start at 1
+  std::vector<double> ranks(num_values);
+  size_t reps = 1;
+  for (size_t i = 0; i < num_values; i += reps) {
+
+    // Find number of replications
+    reps = 1;
+    while (i + reps < num_values && values[indices[i]] == values[indices[i + reps]]) {
+      ++reps;
+    }
+
+    // Assign rank to all replications
+    for (size_t j = 0; j < reps; ++j)
+      ranks[indices[i + j]] = (2 * (double) i + (double) reps - 1) / 2 + 1;
+  }
+
+  return ranks;
+}
+
+/**
+ * Compute Logrank scores for survival times
+ * @param time Survival time
+ * @param status Censoring indicator
+ * @return Logrank scores
+ */
+std::vector<double> logrankScores(const std::vector<double>& time, const std::vector<double>& status);
+
+/**
+ * Compute maximally selected rank statistics
+ * @param scores Scores for dependent variable (y)
+ * @param x Independent variable
+ * @param indices Ordering of x values
+ * @param best_maxstat Maximally selected statistic (output)
+ * @param best_split_value Split value for maximally selected statistic (output)
+ * @param minprop Minimal proportion of observations left of cutpoint
+ * @param maxprop Maximal proportion of observations left of cutpoint
+ */
+void maxstat(const std::vector<double>& scores, const std::vector<double>& x, const std::vector<size_t>& indices,
+    double& best_maxstat, double& best_split_value, double minprop, double maxprop);
+
+/**
+ * Compute number of samples smaller or equal than each unique value in x
+ * @param x Value vector
+ * @param indices Ordering of x
+ * @return Vector of number of samples smaller or equal than each unique value in x
+ */
+std::vector<size_t> numSamplesLeftOfCutpoint(std::vector<double>& x, const std::vector<size_t>& indices);
+
+/**
+ * Read from stringstream and ignore failbit for subnormal numbers
+ * See: https://bugs.llvm.org/show_bug.cgi?id=39012
+ * @param in Input string stream
+ * @param token Output token
+ * @return Input string stream with removed failbit if subnormal number
+ */
+std::stringstream& readFromStream(std::stringstream& in, double& token);
+
+/**
+ * Compute log-likelihood of beta distribution
+ * @param y Response
+ * @param mean Mean
+ * @param phi Phi
+ * @return Log-likelihood
+ */
+double betaLogLik(double y, double mean, double phi);
+
+// User interrupt from R
+#ifdef R_BUILD
+static void chkIntFn(void *dummy) {
+  R_CheckUserInterrupt();
+}
+
+inline bool checkInterrupt() {
+  return (R_ToplevelExec(chkIntFn, NULL) == FALSE);
+}
+#endif
+
+// Provide make_unique (not available in C++11)
+namespace detail {
+
+template<class T> struct _Unique_if {
+  typedef std::unique_ptr<T> _Single_object;
+};
+
+template<class T> struct _Unique_if<T[]> {
+  typedef std::unique_ptr<T[]> _Unknown_bound;
+};
+
+template<class T, size_t N> struct _Unique_if<T[N]> {
+  typedef void _Known_bound;
+};
+
+} // namespace detail
+
+template<class T, class ... Args>
+typename detail::_Unique_if<T>::_Single_object make_unique(Args&&... args) {
+  return std::unique_ptr<T>(new T(std::forward<Args>(args)...));
+}
+
+template<class T>
+typename detail::_Unique_if<T>::_Unknown_bound make_unique(size_t n) {
+  typedef typename std::remove_extent<T>::type U;
+  return std::unique_ptr<T>(new U[n]());
+}
+
+template<class T, class ... Args>
+typename detail::_Unique_if<T>::_Known_bound make_unique(Args&&...) = delete;
+
+}
+// namespace ranger
+
+#endif /* UTILITY_H_ */
--- mocca-1.1.orig/src/main.cpp
+++ mocca-1.1/src/main.cpp
@@ -23,11 +23,17 @@ using namespace rapidxml;
 #include "models/baseclassifier.hpp"
 #include "models/sequenceclassifier.hpp"
 #include "models/svmmocca.hpp"
+#include "models/rfmocca.hpp"
 #include "models/cpredictor.hpp"
 #include "models/dummypredictor.hpp"
 #include "models/seqsvm.hpp"
+#include "models/seqrf.hpp"
 #include "models/seqlo.hpp"
 #include "models/seqdummy.hpp"
+#ifdef USE_SHOGUN
+#include "models/seqlda.hpp"
+#include "models/seqperceptron.hpp"
+#endif
 
 ////////////////////////////////////////////////////////////////////////////////////
 // File list
@@ -826,6 +832,23 @@ cmdArg argumentTypes[] = {
 	},
 	{
 		// Argument
+		"-C:RF",
+		// Pass
+		1,
+		// Parameters
+		0,
+		// Documentation
+		"-C:RF",
+		{ "Sets the classifier to Random Forest, using separately specified",
+		  "feature spaces." },
+		// Code
+		[](std::vector<std::string> params, config*cfg, motifList*ml, featureSet*features, seqList*trainseq, seqList*calseq, seqList*valseq) -> bool {
+			cfg->classifier=cSEQRF;
+			return true;
+		}
+	},
+	{
+		// Argument
 		"-C:SVM-MOCCA",
 		// Pass
 		1,
@@ -875,6 +898,58 @@ cmdArg argumentTypes[] = {
 			return true;
 		}
 	},
+	{
+		// Argument
+		"-C:RF-MOCCA",
+		// Pass
+		1,
+		// Parameters
+		0,
+		// Documentation
+		"-C:RF-MOCCA",
+		{ "Sets the classifier to RF-MOCCA." },
+		// Code
+		[](std::vector<std::string> params, config*cfg, motifList*ml, featureSet*features, seqList*trainseq, seqList*calseq, seqList*valseq) -> bool {
+			cfg->classifier=cRFMOCCA;
+			return true;
+		}
+	},
+	#ifdef USE_SHOGUN
+	{
+		// Argument
+		"-C:LDA",
+		// Pass
+		1,
+		// Parameters
+		0,
+		// Documentation
+		"-C:LDA",
+		{ "Sets the classifier to Linear Discriminant Analysis, using separately",
+		  "specified feature spaces." },
+		// Code
+		[](std::vector<std::string> params, config*cfg, motifList*ml, featureSet*features, seqList*trainseq, seqList*calseq, seqList*valseq) -> bool {
+			cfg->classifier=cSEQLDA;
+			return true;
+		}
+	},
+	{
+		// Argument
+		"-C:Perceptron",
+		// Pass
+		1,
+		// Parameters
+		0,
+		// Documentation
+		"-C:Perceptron",
+		{ "Sets the classifier to a perceptron, using separately specified feature",
+		  "spaces." },
+		// Code
+		[](std::vector<std::string> params, config*cfg, motifList*ml, featureSet*features, seqList*trainseq, seqList*calseq, seqList*valseq) -> bool {
+			cfg->classifier=cSEQPerceptron;
+			return true;
+		}
+	},
+	#endif
 	//---------------------------
 	// Classifier configuration
 	{
@@ -1134,6 +1209,22 @@ cmdArg argumentTypes[] = {
 	},
 	{
 		// Argument
+		"-RF:trees",
+		// Pass
+		1,
+		// Parameters
+		1,
+		// Documentation
+		"-RF:trees VALUE",
+		{ "Sets the number of trees for random forests." },
+		// Code
+		[](std::vector<std::string> params, config*cfg, motifList*ml, featureSet*features, seqList*trainseq, seqList*calseq, seqList*valseq) -> bool {
+			cfg->RF_nTrees=strtod(params[0].c_str(), 0);
+			return true;
+		}
+	},
+	{
+		// Argument
 		"-threshold",
 		// Pass
 		1,
@@ -1210,6 +1301,27 @@ cmdArg argumentTypes[] = {
 			return true;
 		}
 	},
+	{
+		// Argument
+		"-threads",
+		// Pass
+		1,
+		// Parameters
+		1,
+		// Documentation
+		"-threads VALUE",
+		{ "Sets the number of threads to use (currently only supported by",
+		  "RF-based models)." },
+		// Code
+		[](std::vector<std::string> params, config*cfg, motifList*ml, featureSet*features, seqList*trainseq, seqList*calseq, seqList*valseq) -> bool {
+			cfg->nThreads = (int)strtol(params[0].c_str(), 0, 10);
+			if(cfg->nThreads <= 0){
+				argSyntaxError();
+				return false;
+			}
+			return true;
+		}
+	},
 	//---------------------------
 	// Sequences
 	{
@@ -1271,6 +1383,35 @@ cmdArg argumentTypes[] = {
 	},
 	{
 		// Argument
+		"-train:MC",
+		// Pass
+		1,
+		// Parameters
+		6,
+		// Documentation
+		"-train:MC PATH N L CLASS MODE ORDER",
+		{ "Trains an ORDER-th order Markov chain on sequences in the FASTA",
+		  "file PATH, generates N sequences, each of length L, and adds",
+		  "the sequences to training class CLASS.",
+		  "CLASS: A class ID, defined with \"-class\", or one of the",
+		  "pre-specified binary classes: \"+\" for positive or \"-\"",
+		  "for negative." },
+		// Code
+		[](std::vector<std::string> params, config*cfg, motifList*ml, featureSet*features, seqList*trainseq, seqList*calseq, seqList*valseq) -> bool {
+			if(!trainseq->addRandomMC((char*)params[0].c_str(), (int)strtol(params[1].c_str(), 0, 10), (int)strtol(params[2].c_str(), 0, 10), getSeqClassByName(params[3]), getTrainModeByName((char*)params[4].c_str()), (int)strtol(params[5].c_str(), 0, 10))){
+				return false;
+			}
+			/*if(!calseq->loadFastaBatch((char*)params[0].c_str(), getSeqClassByName(params[3]), train_Full)){
+				return false;
+			}*/
+			if(!registerFile("Markov chain training sequences (training)",params[0])){
+				return false;
+			}
+			return true;
+		}
+	},
+	{
+		// Argument
 		"-validate:FASTA",
 		// Pass
 		1,
@@ -1322,6 +1463,32 @@ cmdArg argumentTypes[] = {
 	},
 	{
 		// Argument
+		"-validate:MC",
+		// Pass
+		1,
+		// Parameters
+		5,
+		// Documentation
+		"-validate:MC PATH N L CLASS ORDER",
+		{ "Trains an ORDER-th order Markov chain on sequences in the FASTA",
+		  "file PATH, generates N sequences, each of length L, and adds",
+		  "the sequences to validation class CLASS.",
+		  "CLASS: A class ID, defined with \"-class\", or one of the",
+		  "pre-specified binary classes: \"+\" for positive or \"-\"",
+		  "for negative." },
+		// Code
+		[](std::vector<std::string> params, config*cfg, motifList*ml, featureSet*features, seqList*trainseq, seqList*calseq, seqList*valseq) -> bool {
+			if(!valseq->addRandomMC((char*)params[0].c_str(), (int)strtol(params[1].c_str(), 0, 10), (int)strtol(params[2].c_str(), 0, 10), getSeqClassByName(params[3]), train_Full, (int)strtol(params[4].c_str(), 0, 10))){
+				return false;
+			}
+			if(!registerFile("Markov chain training sequences (validation)",params[0])){
+				return false;
+			}
+			return true;
+		}
+	},
+	{
+		// Argument
 		"-calibrate:FASTA",
 		// Pass
 		1,
@@ -1373,6 +1540,32 @@ cmdArg argumentTypes[] = {
 	},
 	{
 		// Argument
+		"-calibrate:MC",
+		// Pass
+		1,
+		// Parameters
+		5,
+		// Documentation
+		"-calibrate:MC PATH N L CLASS ORDER",
+		{ "Trains an ORDER-th order Markov chain on sequences in the FASTA",
+		  "file PATH, generates N sequences, each of length L, and adds",
+		  "the sequences to calibration class CLASS.",
+		  "CLASS: A class ID, defined with \"-class\", or one of the",
+		  "pre-specified binary classes: \"+\" for positive or \"-\"",
+		  "for negative." },
+		// Code
+		[](std::vector<std::string> params, config*cfg, motifList*ml, featureSet*features, seqList*trainseq, seqList*calseq, seqList*valseq) -> bool {
+			if(!calseq->addRandomMC((char*)params[0].c_str(), (int)strtol(params[1].c_str(), 0, 10), (int)strtol(params[2].c_str(), 0, 10), getSeqClassByName(params[3]), train_Full, (int)strtol(params[4].c_str(), 0, 10))){
+				return false;
+			}
+			if(!registerFile("Markov chain training sequences (calibration)",params[0])){
+				return false;
+			}
+			return true;
+		}
+	},
+	{
+		// Argument
 		"-calibrate:precision",
 		// Pass
 		1,
@@ -1505,6 +1698,88 @@ cmdArg argumentTypes[] = {
 	},
 	{
 		// Argument
+		"-predict:core",
+		// Pass
+		1,
+		// Parameters
+		0,
+		// Documentation
+		"-predict:core",
+		{ "Sets prediction mode to core prediction.",
+		  "Default mode: maximally scoring of continuous cores." },
+		// Code
+		[](std::vector<std::string> params, config*cfg, motifList*ml, featureSet*features, seqList*trainseq, seqList*calseq, seqList*valseq) -> bool {
+			cfg->corePredictionMode = cpmContinuous;
+			cfg->corePredictionMax = true;
+			return true;
+		}
+	},
+	{
+		// Argument
+		"-predict:core:motifs",
+		// Pass
+		1,
+		// Parameters
+		0,
+		// Documentation
+		"-predict:core:motifs",
+		{ "Sets prediction mode to core prediction with predicted motifs." },
+		// Code
+		[](std::vector<std::string> params, config*cfg, motifList*ml, featureSet*features, seqList*trainseq, seqList*calseq, seqList*valseq) -> bool {
+			cfg->corePredictionMode = cpmMotifs;
+			return true;
+		}
+	},
+	{
+		// Argument
+		"-predict:core:motifs:strong",
+		// Pass
+		1,
+		// Parameters
+		0,
+		// Documentation
+		"-predict:core:motifs:strong",
+		{ "Sets prediction mode to core prediction with strongly predicted motifs." },
+		// Code
+		[](std::vector<std::string> params, config*cfg, motifList*ml, featureSet*features, seqList*trainseq, seqList*calseq, seqList*valseq) -> bool {
+			cfg->corePredictionMode = cpmMotifsStrong;
+			return true;
+		}
+	},
+	{
+		// Argument
+		"-predict:core:continuous",
+		// Pass
+		1,
+		// Parameters
+		0,
+		// Documentation
+		"-predict:core:motifs:continuous",
+		{ "Sets prediction mode to continuous core prediction." },
+		// Code
+		[](std::vector<std::string> params, config*cfg, motifList*ml, featureSet*features, seqList*trainseq, seqList*calseq, seqList*valseq) -> bool {
+			cfg->corePredictionMode = cpmContinuous;
+			return true;
+		}
+	},
+	{
+		// Argument
+		"-predict:core:max",
+		// Pass
+		1,
+		// Parameters
+		0,
+		// Documentation
+		"-predict:core:max",
+		{ "Sets prediction mode to predict maximally scoring of core predictions." },
+		// Code
+		[](std::vector<std::string> params, config*cfg, motifList*ml, featureSet*features, seqList*trainseq, seqList*calseq, seqList*valseq) -> bool {
+			cfg->corePredictionMax = true;
+			return true;
+		}
+	},
+	{
+		// Argument
 		"-predict:Wig",
 		// Pass
 		1,
@@ -1522,6 +1797,27 @@ cmdArg argumentTypes[] = {
 	},
 	{
 		// Argument
+		"-auto:order",
+		// Pass
+		1,
+		// Parameters
+		1,
+		// Documentation
+		"-auto:order ORDER",
+		{ "Order of complexity to use for background model.",
+		  "I.i.d. when ORDER is zero, or Markov chain ortherwise." },
+		// Code
+		[](std::vector<std::string> params, config*cfg, motifList*ml, featureSet*features, seqList*trainseq, seqList*calseq, seqList*valseq) -> bool {
+			cfg->bgOrder = (int)strtol(params[0].c_str(), 0, 10);
+			if(cfg->windowSize < 0){
+				argSyntaxError();
+				return false;
+			}
+			return true;
+		}
+	},
+	{
+		// Argument
 		"-auto:FASTA",
 		// Pass
 		2,
@@ -1529,7 +1825,7 @@ cmdArg argumentTypes[] = {
 		1,
 		// Documentation
 		"-auto:FASTA PATH",
-		{ "Sets up limited automated CRM machine learning",
+		{ "Sets up automated/assisted CRE machine learning",
 		  "based on FASTA file containing target class",
 		  "examples. Negatives are generated by an i.i.d.",
 		  "model. Defaults are also set for models and",
@@ -1542,13 +1838,13 @@ cmdArg argumentTypes[] = {
 		// Code
 		[](std::vector<std::string> params, config*cfg, motifList*ml, featureSet*features, seqList*trainseq, seqList*calseq, seqList*valseq) -> bool {
 			// 
-			cmdSection("Automated CRM machine learning");
+			cmdSection("Automated CRE machine learning");
 			if(trainseq->nseq||valseq->nseq||calseq->nseq){
-				cout << m_error << "No previous sequences can be defined for automated CRM machine learning.\n";
+				cout << m_error << "No previous sequences can be defined for automated CRE machine learning.\n";
 			}
-			getSeqClassByName((char*)"+")->name = "CRM";
-			getSeqClassByName((char*)"-")->name = "Dummy CRM";
-			if(cfg->classifier == cSVMMOCCA){
+			getSeqClassByName((char*)"+")->name = "CRE";
+			getSeqClassByName((char*)"-")->name = "Dummy CRE";
+			if(cfg->classifier == cSVMMOCCA || cfg->classifier == cRFMOCCA){
 				if(!registerSeqClass(-2,(char*)"Dummy genomic",false)){
 					return false;
 				}
@@ -1607,25 +1903,44 @@ cmdArg argumentTypes[] = {
 			}
 			cout << "Input genome: " << cfg->genomeFASTAPath << " (" << genomeSize << " bp)\n";
 			// For SVM-MOCCA classifier, if no features were specified, set to standard
-			if(cfg->classifier == cSVMMOCCA){
+			cout << "Background model order: " << cfg->bgOrder << "\n";
+			if(cfg->classifier == cSVMMOCCA || cfg->classifier == cRFMOCCA){
 				if(!( cfg->MOCCA_nOcc | cfg->MOCCA_DNT | cfg->MOCCA_GC )){
 					cfg->MOCCA_nOcc = cfg->MOCCA_DNT = true;
 					cfg->kernel=kQuadratic;
 				}
 				// We can also add a dummy genomic class, as a second negative class
-				if(!trainseq->addRandomIid((char*)cfg->genomeFASTAPath.c_str(), nTrain, meanLen, getSeqClassByName((char*)"--"), getTrainModeByName((char*)"full"))){
-					return false;
+				if(cfg->bgOrder > 0){
+					if(!trainseq->addRandomMC((char*)cfg->genomeFASTAPath.c_str(), nTrain, meanLen, getSeqClassByName((char*)"--"), getTrainModeByName((char*)"full"), cfg->bgOrder)){
+						return false;
+					}
+				}else{
+					if(!trainseq->addRandomIid((char*)cfg->genomeFASTAPath.c_str(), nTrain, meanLen, getSeqClassByName((char*)"--"), getTrainModeByName((char*)"full"))){
+						return false;
+					}
 				}
 			}
 			//
-			if(!trainseq->addRandomIid((char*)params[0].c_str(), nTrain, meanLen, getSeqClassByName((char*)"-"), getTrainModeByName((char*)"full"))){
-				return false;
-			}
-			if(!valseq->addRandomIid((char*)params[0].c_str(), nTrain*100, meanLen, getSeqClassByName((char*)"-"), getTrainModeByName((char*)"full"))){
-				return false;
-			}
-			if(!calseq->addRandomIid((char*)cfg->genomeFASTAPath.c_str(), genomeSize/meanLen, meanLen, getSeqClassByName((char*)"-"), getTrainModeByName((char*)"full"))){
-				return false;
+			if(cfg->bgOrder > 0){
+				if(!trainseq->addRandomMC((char*)params[0].c_str(), nTrain, meanLen, getSeqClassByName((char*)"-"), getTrainModeByName((char*)"full"), cfg->bgOrder)){
+					return false;
+				}
+				if(!valseq->addRandomMC((char*)params[0].c_str(), nTrain*100, meanLen, getSeqClassByName((char*)"-"), getTrainModeByName((char*)"full"), cfg->bgOrder)){
+					return false;
+				}
+				if(!calseq->addRandomMC((char*)cfg->genomeFASTAPath.c_str(), genomeSize/meanLen, meanLen, getSeqClassByName((char*)"-"), getTrainModeByName((char*)"full"), cfg->bgOrder)){
+					return false;
+				}
+			}else{
+				if(!trainseq->addRandomIid((char*)params[0].c_str(), nTrain, meanLen, getSeqClassByName((char*)"-"), getTrainModeByName((char*)"full"))){
+					return false;
+				}
+				if(!valseq->addRandomIid((char*)params[0].c_str(), nTrain*100, meanLen, getSeqClassByName((char*)"-"), getTrainModeByName((char*)"full"))){
+					return false;
+				}
+				if(!calseq->addRandomIid((char*)cfg->genomeFASTAPath.c_str(), genomeSize/meanLen, meanLen, getSeqClassByName((char*)"-"), getTrainModeByName((char*)"full"))){
+					return false;
+				}
 			}
 			cout << "Training sequences: " << trainseq->nseq << "\n";
 			cout << "Validation sequences: " << valseq->nseq << "\n";
@@ -1725,6 +2040,17 @@ LIABILITY, WHETHER IN CONTRACT, STRICT L
 NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n\
 SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n";
 	cout << sepline;
+	cout << t_bold("Dependency license: Ranger") << "\n\
+MIT License\n\
+\n\
+Copyright (c) [2014-2018] [Marvin N. Wright]\n\
+\n\
+Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\
+\n\
+The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\
+\n\
+THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n";
+	cout << sepline;
 	cout << t_bold("Dependency license: RapidXML") << "\n\
 \n\
 Copyright (c) 2006, 2007 Marcin Kalicinski\n\
@@ -1746,6 +2072,39 @@ THE AUTHORS OR COPYRIGHT HOLDERS BE LIAB
 LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n\
 OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS \n\
 IN THE SOFTWARE.\n";
+	#ifdef USE_SHOGUN
+	cout << sepline;
+	cout << t_bold("Dependency license: Shogun") << "\n\
+\n\
+Copyright (c) Shogun Machine Learning Toolbox developers <shogun-team@shogun-toolbox.org>\n\
+All rights reserved.\n\
+\n\
+Redistribution and use in source and binary forms, with or without\n\
+modification, are permitted provided that the following conditions are met:\n\
+\n\
+  1. Redistributions of source code must retain the above copyright notice,\n\
+     this list of conditions and the following disclaimer.\n\
+\n\
+  2. Redistributions in binary form must reproduce the above copyright\n\
+     notice, this list of conditions and the following disclaimer in the\n\
+     documentation and/or other materials provided with the distribution.\n\
+\n\
+  3. Neither the name of the copyright holder nor the names of its\n\
+     contributors may be used to endorse or promote products derived from\n\
+     this software without specific prior written permission.\n\
+\n\
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n\
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n\
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n\
+ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n\
+LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n\
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n\
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n\
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n\
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n\
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n\
+POSSIBILITY OF SUCH DAMAGE.\n";
+	#endif
 	cout << sepline;
 }
 
@@ -1823,11 +2182,17 @@ sequenceClassifier*constructClassifier(m
 	config*cfg=getConfiguration();
 	switch(cfg->classifier){
 		case cSVMMOCCA:cls=SVMMOCCA::create(motifs,cfg->svmtype);break;
+		case cRFMOCCA:cls=RFMOCCA::create(motifs);break;
 		case cCPREdictor:cls=CPREdictor::create(motifs);break;
 		case cDummyPREdictor:cls=DummyPREdictor::create(motifs);break;
 		case cSEQSVM:cls=SEQSVM::create(motifs,features,cfg->svmtype);break;
+		case cSEQRF:cls=SEQRF::create(motifs,features);break;
 		case cSEQLO:cls=SEQLO::create(motifs,features);break;
 		case cSEQDummy:cls=SEQDummy::create(motifs,features);break;
+		#ifdef USE_SHOGUN
+		case cSEQLDA:cls=SEQLDA::create(motifs,features);break;
+		case cSEQPerceptron:cls=SEQPerceptron::create(motifs,features);break;
+		#endif
 		default:cmdError("Invalid classifier.");return 0;
 	}
 	if(!cls)return 0;
--- mocca-1.1.orig/src/models/baseclassifier.cpp
+++ mocca-1.1/src/models/baseclassifier.cpp
@@ -959,3 +959,188 @@ bool MultiClassSVM::exportAnalysisData(F
 	return true;
 }
 
+////////////////////////////////////////////////////////////////////////////////////
+// Random Forest
+//	Trains RF with Ranger.
+
+RFClassifier::RFClassifier(int nf):baseClassifier(nf){
+}
+
+RFClassifier*RFClassifier::create(int nf, std::string name){
+	if(!nf){
+		cmdError("No features.");
+		return 0;
+	}
+	RFClassifier*r=new RFClassifier(nf);
+	if(!r){
+		outOfMemory();
+		return 0;
+	}
+	r->rf.ptr = new RangerRandomForest();
+	r->name = name;
+	return r;
+}
+
+bool RFClassifier::do_train(){
+	// Fill in data values
+	std::unique_ptr<RangerData> data{};
+	data = ranger::make_unique<RangerData>(); //new ranger::DataDouble();
+	data->setDataT(nFeatures, trainingExamples.v);
+	
+	// Train model
+	std::vector<std::string> catvars;
+	std::vector<double> regfac;
+	std::vector<double> sample_fraction_vector = { ranger::DEFAULT_SAMPLE_FRACTION_REPLACE };
+	rf.ptr->init(
+		ranger::MemoryMode::MEM_DOUBLE,
+		std::move(data),
+		(ranger::uint)0, // uint mtry,
+		std::string(""), //std::string output_prefix,
+		getConfiguration()->RF_nTrees, //uint num_trees,
+		(ranger::uint)0, //uint seed,
+		(ranger::uint)getConfiguration()->nThreads, //uint num_threads,
+		//ranger::ImportanceMode::IMP_NONE, //ImportanceMode importance_mode,
+		ranger::ImportanceMode::IMP_PERM_CASEWISE, //ImportanceMode importance_mode,
+		(ranger::uint)0, //uint min_node_size,
+		false, //bool prediction_mode,
+		true, //bool sample_with_replacement,
+		catvars, //const std::vector<std::string>& unordered_variable_names,
+		false, //bool memory_saving_splitting,
+		//ranger::SplitRule::LOGRANK,//SplitRule splitrule,
+		ranger::SplitRule::HELLINGER,//SplitRule splitrule,
+		false, //bool predict_all,
+		sample_fraction_vector, //std::vector<double>& sample_fraction,
+		ranger::DEFAULT_ALPHA, //double alpha,
+		ranger::DEFAULT_MINPROP, //double minprop,
+		false, //bool holdout,
+		ranger::PredictionType::RESPONSE, //PredictionType prediction_type,
+		ranger::DEFAULT_NUM_RANDOM_SPLITS, //uint num_random_splits,
+		false, //bool order_snps,
+		ranger::DEFAULT_MAXDEPTH, //uint max_depth,
+		regfac,//const std::vector<double>& regularization_factor,
+		false //bool regularization_usedepth
+	);
+	rf.ptr->train();
+	
+	
+	return true;
+}
+
+double RFClassifier::do_apply(double*vec){
+	return rf.ptr->predictVec(vec);
+}
+
+void RFClassifier::printInfo(char*header){
+	cout << t_indent << header << "\n";
+}
+
+////////////////////////////////////////////////////////////////////////////////////
+// Linear Discriminant Analysis
+#ifdef USE_SHOGUN
+
+LDAClassifier::LDAClassifier(int nf):baseClassifier(nf){
+}
+
+LDAClassifier*LDAClassifier::create(int nf, std::string name){
+	if(!nf){
+		cmdError("No features.");
+		return 0;
+	}
+	LDAClassifier*r=new LDAClassifier(nf);
+	if(!r){
+		outOfMemory();
+		return 0;
+	}
+	r->name = name;
+	return r;
+}
+
+bool LDAClassifier::do_train(){
+	shogun::init_shogun_with_defaults();
+	int ntrain = trainingExamples.v.size();
+	shogun::SGVector<double> lvec(ntrain);
+	shogun::SGMatrix<double> fmat(nFeatures, ntrain);
+	int row = 0;
+	for(baseClassifierSmp*t: trainingExamples.v){
+		for(int i=0; i<nFeatures; i++)
+			fmat(i, row) = t->vec[i];
+		lvec[row] = t->cls->flag ? 1 : -1;
+		row++;
+	}
+	shogun::CBinaryLabels*lbl = new shogun::CBinaryLabels(lvec);
+	shogun::CDenseFeatures<double>*fv = new shogun::CDenseFeatures<double>(fmat);
+	LDA.ptr = new shogun::CLDA(0.0001, fv, lbl);
+	LDA.ptr->train();
+	return true;
+}
+
+double LDAClassifier::do_apply(double*vec){
+	shogun::SGMatrix<double> fmat(nFeatures, 1);
+	for(int i=0; i<nFeatures; i++)
+		fmat(i, 0) = vec[i];
+	shogun::CDenseFeatures<double>*fv = new shogun::CDenseFeatures<double>(fmat);
+	autodelete<shogun::CBinaryLabels> pred(LDA.ptr->apply_binary(fv));
+	return pred.ptr->get_value(0);
+}
+
+void LDAClassifier::printInfo(char*header){
+	cout << t_indent << header << "\n";
+}
+#endif
+
+////////////////////////////////////////////////////////////////////////////////////
+// Perceptron
+#ifdef USE_SHOGUN
+
+PerceptronClassifier::PerceptronClassifier(int nf):baseClassifier(nf){
+}
+
+PerceptronClassifier*PerceptronClassifier::create(int nf, std::string name){
+	if(!nf){
+		cmdError("No features.");
+		return 0;
+	}
+	PerceptronClassifier*r=new PerceptronClassifier(nf);
+	if(!r){
+		outOfMemory();
+		return 0;
+	}
+	r->name = name;
+	return r;
+}
+
+bool PerceptronClassifier::do_train(){
+	shogun::init_shogun_with_defaults();
+	int ntrain = trainingExamples.v.size();
+	shogun::SGVector<double> lvec(ntrain);
+	shogun::SGMatrix<double> fmat(nFeatures, ntrain);
+	int row = 0;
+	for(baseClassifierSmp*t: trainingExamples.v){
+		for(int i=0; i<nFeatures; i++)
+			fmat(i, row) = t->vec[i];
+		lvec[row] = t->cls->flag ? 1 : -1;
+		row++;
+	}
+	shogun::CBinaryLabels*lbl = new shogun::CBinaryLabels(lvec);
+	shogun::CDenseFeatures<double>*fv = new shogun::CDenseFeatures<double>(fmat);
+	perceptron.ptr = new shogun::CAveragedPerceptron(fv, lbl);
+	perceptron.ptr->set_max_iter(100000);
+	perceptron.ptr->set_learn_rate(1.);
+	perceptron.ptr->train();
+	return true;
+}
+
+double PerceptronClassifier::do_apply(double*vec){
+	shogun::SGMatrix<double> fmat(nFeatures, 1);
+	for(int i=0; i<nFeatures; i++)
+		fmat(i, 0) = vec[i];
+	shogun::CDenseFeatures<double>*fv = new shogun::CDenseFeatures<double>(fmat);
+	autodelete<shogun::CBinaryLabels> pred(perceptron.ptr->apply_binary(fv));
+	return pred.ptr->get_value(0);
+}
+
+void PerceptronClassifier::printInfo(char*header){
+	cout << t_indent << header << "\n";
+}
+#endif
+
--- mocca-1.1.orig/src/models/baseclassifier.hpp
+++ mocca-1.1/src/models/baseclassifier.hpp
@@ -133,3 +133,138 @@ public:
 	bool exportAnalysisData(FILE*f, char*title, char*indent);
 };
 
+////////////////////////////////////////////////////////////////////////////////////
+// Random Forest
+//	Trains RF with Ranger.
+
+#include "../lib/ranger/src/globals.h"
+#include "../lib/ranger/src/ForestClassification.h"
+#include "../lib/ranger/src/ForestRegression.h"
+#include "../lib/ranger/src/ForestProbability.h"
+#include "../lib/ranger/src/utility.h"
+#include "../lib/ranger/src/Data.h"
+#include "../lib/ranger/src/DataDouble.h"
+
+class RangerData: public ranger::DataDouble {
+public:
+	void setDataT(int nFeatures, std::vector<baseClassifierSmp*> dat) {
+		// Set header
+		for(int i=0;i<nFeatures;i++){
+			variable_names.push_back("f" + std::to_string(i+1));
+		}
+		num_cols = variable_names.size();
+		num_cols_no_snp = num_cols;
+		for(baseClassifierSmp*t: dat)
+			num_rows++;
+		reserveMemory(1);
+		bool error = false;
+		// Set rows
+		int row = 0;
+		for(baseClassifierSmp*t: dat){
+			for(int i=0;i<nFeatures;i++)
+				set_x(i, row, t->vec[i], error);
+			set_y(0, row, t->cls->flag ? 1. : -1., error);
+			row++;
+		}
+	}
+	void setVector(double*vec, int ncol){
+		num_rows = 1;
+		num_cols = ncol;
+		num_cols_no_snp = num_cols;
+		reserveMemory(0);
+		bool error = false;
+		for(int i=0;i<ncol;i++)
+			set_x(i, 0, vec[i], error);
+	}
+};
+
+class RangerRandomForest: public ranger::ForestProbability {
+public:
+	void train(){
+		grow();
+		computePredictionError();
+		dependent_variable_names.push_back(std::string("target"));
+	}
+	double predictVec(double*vec){
+		RangerData*rd = (RangerData*)data.get();
+		int ncol = rd->getNumCols();
+		rd->setVector(vec, ncol);
+		num_samples = 1;
+		ranger::equalSplit(thread_ranges, 0, num_trees - 1, num_threads);
+		predict();
+		double ppos = 0.;
+		double pneg = 0.;
+		for(int icls = 0; icls < class_values.size(); icls++){
+			if (class_values[icls] == 1.0)
+				ppos = predictions[0][0][icls];
+			else if (class_values[icls] == -1.0)
+				pneg = predictions[0][0][icls];
+		}
+		return ppos - pneg;
+	}
+};
+
+class RFClassifier:public baseClassifier{
+private:
+	RFClassifier(int nf);
+	autodelete<RangerRandomForest> rf;
+	std::string name;
+public:
+	std::vector<std::string> featureNames;
+	virtual ~RFClassifier(){ };
+	static RFClassifier*create(int nf, std::string name);
+	bool do_train();
+	double do_apply(double*vec);
+	void printInfo(char*header);
+};
+
+////////////////////////////////////////////////////////////////////////////////////
+// Linear Discriminant Analysis
+
+#ifdef USE_SHOGUN
+#include <shogun/base/init.h>
+
+#include <shogun/lib/config.h>
+#include <shogun/labels/MulticlassLabels.h>
+#include <shogun/classifier/LDA.h>
+#include <shogun/features/DenseFeatures.h>
+#include <shogun/io/SGIO.h>
+#include <shogun/lib/common.h>
+#include <shogun/features/DataGenerator.h>
+
+class LDAClassifier:public baseClassifier{
+private:
+	LDAClassifier(int nf);
+	autodelete<shogun::CLDA> LDA;
+	std::string name;
+public:
+	std::vector<std::string> featureNames;
+	virtual ~LDAClassifier(){ };
+	static LDAClassifier*create(int nf, std::string name);
+	bool do_train();
+	double do_apply(double*vec);
+	void printInfo(char*header);
+};
+#endif
+
+////////////////////////////////////////////////////////////////////////////////////
+// Perceptron
+
+#ifdef USE_SHOGUN
+#include <shogun/classifier/AveragedPerceptron.h>
+
+class PerceptronClassifier:public baseClassifier{
+private:
+	PerceptronClassifier(int nf);
+	autodelete<shogun::CAveragedPerceptron> perceptron;
+	std::string name;
+public:
+	std::vector<std::string> featureNames;
+	virtual ~PerceptronClassifier(){ };
+	static PerceptronClassifier*create(int nf, std::string name);
+	bool do_train();
+	double do_apply(double*vec);
+	void printInfo(char*header);
+};
+#endif
+
--- /dev/null
+++ mocca-1.1/src/models/rfmocca.cpp
@@ -0,0 +1,429 @@
+////////////////////////////////////////////////////////////////////////////////////
+// MOCCA
+// Copyright, Bjørn Bredesen, 2019
+// E-mail: bjorn@bjornbredesen.no
+////////////////////////////////////////////////////////////////////////////////////
+// General
+
+#include "../common.hpp"
+#include "../lib/libsvm-3.17/svm.h"
+#include "../aux.hpp"
+#include "../config.hpp"
+#include "../validation.hpp"
+#include "../motifs.hpp"
+#include "../sequences.hpp"
+#include "../sequencelist.hpp"
+#include "baseclassifier.hpp"
+#include "sequenceclassifier.hpp"
+#include "svmmocca.hpp"
+#include "rfmocca.hpp"
+
+////////////////////////////////////////////////////////////////////////////////////
+// Motif classifier
+
+RFMotifOccClassifier::RFMotifOccClassifier(int mi,motifList*ml){
+	cfg=getConfiguration();
+	motifInd=mi;
+	motifs=ml;
+	nfeatures=0;
+}
+
+RFMotifOccClassifier*RFMotifOccClassifier::create(int mi,motifList*ml){
+	autodelete<RFMotifOccClassifier> r(new RFMotifOccClassifier(mi,ml));
+	if(!r.ptr){
+		outOfMemory();
+		return 0;
+	}
+	r.ptr->featureSet.ptr=MotifClassifier_featureSet::create(ml);
+	if(!r.ptr->featureSet.ptr){
+		return 0;
+	}
+	config*cfg=r.ptr->cfg;
+	// Add features
+	if(cfg->MOCCA_nOcc)
+		for(int l=0;l<ml->nmotifs;l++)
+			r.ptr->featureSet.ptr->addFeature(MF_nOcc,l,250,0,0,0);
+	if(cfg->MOCCA_GC)
+		r.ptr->featureSet.ptr->addFeature(MF_GC,0,250,0,0,0);
+	if(cfg->MOCCA_DNT)
+		r.ptr->featureSet.ptr->addFeatures(MF_DNT,0,250,0,0,0);
+	//
+	r.ptr->nfeatures=r.ptr->featureSet.ptr->nfeatures;
+	if(!r.ptr->nfeatures){
+		cmdError("No features for motif classifier");
+		return 0;
+	}
+	r.ptr->features.resize((size_t)r.ptr->nfeatures);
+	if(!r.ptr->features.ptr){
+		return 0;
+	}
+	r.ptr->features.fill((size_t)r.ptr->nfeatures,0);
+	char cName[256];
+	snprintf(cName, sizeof(cName), "Motif occurrence classifier - Motif: %s", ml->motifs[mi].name);
+	r.ptr->classifier.ptr=RFClassifier::create(r.ptr->nfeatures,std::string(cName));
+	if(!r.ptr->classifier.ptr){
+		return 0;
+	}
+	for(int l = 0; l < r.ptr->featureSet.ptr->nfeatures; l++){
+		r.ptr->classifier.ptr->featureNames.push_back(std::string(r.ptr->featureSet.ptr->featureNames[l]));
+	}
+	return r.disown();
+}
+
+bool RFMotifOccClassifier::trainOcc(motifOcc*o,motifOccContainer*moc,long long wpos,char*buf,int bufs,seqClass*_cls){
+	if(!featureSet.ptr->getFeatures(features.ptr,o,moc,wpos,buf,bufs)){
+		return false;
+	}
+	if(!classifier.ptr->addTrain(features.ptr,nfeatures,_cls)){
+		return false;
+	}
+	return true;
+}
+
+bool RFMotifOccClassifier::trainFinish(){
+	if(!classifier.ptr->train())return false;
+	return true;
+}
+
+double RFMotifOccClassifier::applyOcc(motifOcc*o,motifOccContainer*moc,long long wpos,char*buf,int bufs){
+	if(!featureSet.ptr->getFeatures(features,o,moc,wpos,buf,bufs)){
+		return false;
+	}
+	return classifier.ptr->apply(features.ptr,nfeatures);
+}
+
+void RFMotifOccClassifier::printInfo(){
+	classifier.ptr->printInfo((char*)"Motif occurrence classifier");
+}
+
+bool RFMotifOccClassifier::exportAnalysisData(FILE*f){
+	return classifier.ptr->exportAnalysisData(f,(char*)"Motif occurrence classifier",(char*)" - ");
+}
+
+////////////////////////////////////////////////////////////////////////////////////
+// RF-MOCCA
+
+RFMOCCA::RFMOCCA(int nf):sequenceClassifier(nf){
+}
+
+RFMOCCA*RFMOCCA::create(motifList*motifs){
+	if(!motifs){cmdError("Null-pointer argument.");return 0;}
+	if(!motifs->nmotifs){cmdError("No motifs.");return 0;}
+	autodelete<RFMOCCA> r(new RFMOCCA(motifs->nmotifs));
+	if(!r.ptr){
+		outOfMemory();
+		return 0;
+	}
+	r.ptr->mwin.ptr = motifWindow::create(motifs);
+	if(!r.ptr->mwin.ptr){
+		return 0;
+	}
+	r.ptr->moc = r.ptr->mwin.ptr->occContainer;
+	r.ptr->motifs = r.ptr->mwin.ptr->motifs;
+	r.ptr->classifier.ptr=logoddsClassifier::create(motifs->nmotifs);
+	if(!r.ptr->classifier.ptr){
+		return 0;
+	}
+	if(!r.ptr->fvec.resize((size_t)motifs->nmotifs)){
+		return 0;
+	}
+	for(int l=0;l<motifs->nmotifs;l++){
+		r.ptr->classifier.ptr->featureNames.push_back(std::string(motifs->motifs[l].name));
+		RFMotifOccClassifier*sc = RFMotifOccClassifier::create(l,motifs);
+		if(!sc){
+			return 0;
+		}
+		r.ptr->subcls.push_back(sc);
+	}
+	return r.disown();
+}
+
+bool RFMOCCA::trainWindow(char*buf,long long pos,int bufs,seqClass*cls){
+	trainingSequence*ts = trainingSequence::createSequenceClone(buf,bufs,cls);
+	if(!ts)return false;
+	trainSeq.push_back(ts);
+	return true;
+}
+
+bool RFMOCCA::trainFinish(){
+	for(trainingSequence*ts: trainSeq.v){
+		mwin.ptr->flush();
+		if(!mwin.ptr->readWindow(ts->seq,0,ts->length)){
+			return false;
+		}
+		for(int l=0;l<motifs->nmotifs;l++){
+			RFMotifOccClassifier*c=subcls[l];
+			motifOcc*o=moc->getFirst(l);
+			while(o){
+				if(!c->trainOcc(o,moc,0,ts->seq,ts->length,ts->cls)){
+					return false;
+				}
+				o=moc->getNextSame(o);
+			}
+		}
+	}
+	for(RFMotifOccClassifier*sc: subcls.v){
+		if(!sc->trainFinish())return false;
+	}
+	for(trainingSequence*ts: trainSeq.v){
+		mwin.ptr->flush();
+		if(!mwin.ptr->readWindow(ts->seq,0,ts->length)){
+			return false;
+		}
+		for(int x=0;x<motifs->nmotifs;x++){
+			RFMotifOccClassifier*c=subcls[x];
+			motifOcc*o=moc->getFirst(x);
+			int nc=0;
+			while(o){
+				if( c->applyOcc(o,moc,0,ts->seq,ts->length)>0 ){
+					nc++;
+				}
+				o=moc->getNextSame(o);
+			}
+			fvec[x]=double(nc*1000)/double(ts->length);
+		}
+		if(!classifier.ptr->addTrain(fvec.ptr,nFeatures,ts->cls)){
+			return false;
+		}
+	}
+	if(!classifier.ptr->train()){
+		return false;
+	}
+	return true;
+}
+
+double RFMOCCA::do_applyWindow(char*buf,long long pos,int bufs){
+	if(!mwin.ptr->readWindow(buf,pos,bufs)){
+		return false;
+	}
+	for(int x=0;x<motifs->nmotifs;x++){
+		RFMotifOccClassifier*c=subcls[x];
+		motifOcc*o=moc->getFirst(x);
+		int nc=0;
+		while(o){
+			if( c->applyOcc(o,moc,pos,buf,bufs)>0 ){
+				nc++;
+			}
+			o=moc->getNextSame(o);
+		}
+		fvec[x]=double(nc*1000)/double(bufs);
+	}
+	return classifier.ptr->apply(fvec.ptr,nFeatures);
+}
+
+bool RFMOCCA::flush(){
+	return mwin.ptr->flush();
+}
+
+bool RFMOCCA::printInfo(){
+	cout << t_indent << "RF-MOCCA\n";
+	cout << t_indent << "Trained?: " << (trained?"Yes":"No") << "\n";
+	// Uncomment to always display model analysis
+	//for(int l=0;l<motifs->nmotifs;l++){
+	//	subcls[l]->printInfo();
+	//}
+	//classifier.ptr->printInfo((char*)"Log-odds");
+	return true;
+}
+
+bool RFMOCCA::exportAnalysisData(string path){
+	FILE*f=fopen(path.c_str(),"wb");
+	fprintf(f, "RF-MOCCA model analysis\n");
+	for(int l=0;l<motifs->nmotifs;l++){
+		if(!subcls[l]->exportAnalysisData(f))
+			return false;
+	}
+	if(!classifier.ptr->exportAnalysisData(f, (char*)"Log-odds", (char*)" - "))
+		return false;
+	fclose(f);
+	cout << "Saved classifier analysis data to \"" << path << "\"\n";
+	return true;
+}
+
+vector<prediction> RFMOCCA::predictWindow(char*buf,long long pos,int bufs, corePredictionModeT cpm){
+	vector<prediction> ret = vector<prediction>();
+	if(!mwin.ptr->readWindow(buf,pos,bufs)){
+		return ret;
+	}
+	vector<prediction> motpos = vector<prediction>();
+	for(int x=0;x<motifs->nmotifs;x++){
+		RFMotifOccClassifier*c=subcls[x];
+		motifOcc*o=moc->getFirst(x);
+		int nc=0;
+		while(o){
+			if( c->applyOcc(o,moc,pos,buf,bufs)>0 ){
+				nc++;
+				if(cpm != cpmNone){
+					int center = (o->start + o->start + motifs->motifs[x].len) / 2;
+					double mscore = classifier.ptr->getWeight(x);
+					motpos.push_back(prediction(center - 250, center + 250, mscore,
+						o->start,
+						o->start + motifs->motifs[x].len));
+				}
+			}
+			o=moc->getNextSame(o);
+		}
+		fvec[x]=double(nc*1000)/double(bufs);
+	}
+	double cvalue = classifier.ptr->apply(fvec.ptr,nFeatures);
+	if(cpm != cpmNone){
+		if(cvalue >= threshold){
+			// Motif prediction--based core prediction algorithm
+			if(cpm == cpmMotifs || cpm == cpmMotifsStrong){
+				// Try to limit to central occurrences with cumulative scores above threshold
+				vector<prediction> cmotpos = vector<prediction>();
+				sort(motpos.begin(),motpos.end(),
+				[](const prediction a,const prediction b){
+					return a.mstart < b.mstart;
+				});
+				for(auto&oA: motpos){
+					double cumscore = 0.;
+					for(auto&oB: motpos){
+						if(oB.mstart < oA.start) continue;
+						if(oB.mend > oA.end) break;
+						cumscore += oB.score;
+					}
+					if(cumscore >= threshold)
+						cmotpos.push_back(prediction(
+							max(oA.start, 0),
+							min(oA.end, (int)pos + bufs),
+							cumscore));
+				}
+				if(cpm == cpmMotifsStrong) return cmotpos;
+				if(cmotpos.size() > 0) motpos = cmotpos;
+				// Flatten
+				for(auto&occ: motpos){
+					//if(ret.size() > 0 && occ.start < ret.back().end+500){
+					if(ret.size() > 0 && occ.start < ret.back().end){
+						ret.back().end = max(ret.back().end, occ.end);
+						ret.back().score = max(ret.back().score, occ.score);
+					}else{
+						ret.push_back(prediction(occ.start, occ.end, occ.score));
+					}
+				}
+				// If there are predictions scoring above the threshold, predict those.
+				// Otherwise, there must be many low-scoring predictions, so predict
+				// all of them.
+				// Thus: Get predictions with scores above threshold.
+				vector<prediction> pred = vector<prediction>();
+				for(auto&p: ret)
+					if(p.score >= threshold)
+						pred.push_back(p);
+				// If any, return that list
+				if(pred.size() > 0)
+					return pred;
+				// If none, return the regular list of predictions
+			// Continuous core prediction algorithm
+			}else if(cpm == cpmContinuous){
+				// For continuous predictions, we want to predict a core delimited
+				// by motif occurrences that spans potentially a larger region, and
+				// has a high score per basepair.
+				sort(motpos.begin(),motpos.end(),
+				[](const prediction a,const prediction b){
+					return a.center < b.center;
+				});
+				vector<prediction> cmotpos = vector<prediction>();
+				for(auto&m: motpos) cmotpos.push_back(m);
+				sort(cmotpos.begin(),cmotpos.end(),
+				[](const prediction a,const prediction b){
+					return a.mstart < b.mstart;
+				});
+				int maxWinStart = -1;
+				int maxWinEnd = -1;
+				double maxWinScore = -1.;
+				double maxWinCumScore = -1.;
+				int iCA = 0;
+				for(int iA = 0; iA < motpos.size() - 1; iA++){
+					auto&oA = motpos[iA];
+					int iCB = iCA;
+					double cumscoreA = 0.;
+					for(int iB = iA; iB < motpos.size(); iB++){
+						auto&oB = motpos[iB];
+						if(oB.end > oA.start + cfg->windowSize)
+							break;
+						int wA = max(oA.start, 0);
+						int wB = min(oB.end, (int)pos + bufs);
+						if(wB-wA < oA.end-oA.start) continue;
+						bool block = false;
+						double cumscore = cumscoreA;
+						for(int iC = iCB; iC < cmotpos.size(); iC++){
+							auto&oC = cmotpos[iC];
+							// Occurrences are sorted by motif start, so if they
+							// start before the window, we can safely skip.
+							if(oC.mstart < wA){
+								iCA++;
+								iCB++;
+								continue;
+							}
+							//if(oC.mstart > wB) continue;
+							// If the occurrence is past the window end, we can
+							// safely break.
+							if(oC.mstart > wB) break;
+							// If only the end is past, later shorter occurrences
+							// may still be inside, so just block updating of the
+							// bookmark position.
+							if(oC.mend > wB) {
+								block = true;
+								continue;
+							}
+							// The occurrence is inside the window, so add.
+							cumscore += oC.score;
+							// If an occurrence was past the end of the window,
+							// do not update bookmarks.
+							if(!block) {
+								iCB = iC + 1;
+								cumscoreA = cumscore;
+							}
+						}
+						double winScore = cumscore / max(double(wB - wA), 1.);
+						if(maxWinStart == -1 || winScore > maxWinScore){
+							maxWinStart = wA;
+							maxWinEnd = wB;
+							maxWinScore = winScore;
+							maxWinCumScore = cumscore;
+						}
+					}
+				}
+				if(maxWinStart == -1) return ret;
+				ret.push_back(prediction(maxWinStart, maxWinEnd, maxWinCumScore));
+				/*
+				// Unoptimized base algorithm
+				sort(motpos.begin(),motpos.end(),
+				[](const prediction a,const prediction b){
+					return a.center < b.center;
+				});
+				vector<prediction> mwnd = vector<prediction>();
+				for(int iA = 0; iA < motpos.size() - 1; iA++){
+					auto&oA = motpos[iA];
+					for(int iB = iA; iB < motpos.size(); iB++){
+						auto&oB = motpos[iB];
+						if(oB.end > oA.start + cfg->windowSize)
+							break;
+						int wA = max(oA.start, 0);
+						int wB = min(oB.end, (int)pos + bufs);
+						if(wB-wA < oA.end-oA.start) continue;
+						double cumscore = 0.;
+						for(auto&oC: motpos){
+							if(oC.mstart < wA || oC.mend > wB) continue;
+							cumscore += oC.score;
+						}
+						mwnd.push_back(prediction(wA, wB, cumscore));
+					}
+				}
+				if(mwnd.size() == 0) return mwnd;
+				sort(mwnd.begin(),mwnd.end(),
+				[](const prediction a,const prediction b){
+					double sA = a.score / max(double(a.end - a.start), 1.);
+					double sB = b.score / max(double(b.end - b.start), 1.);
+					return sA > sB;
+				});
+				ret.push_back(prediction(mwnd[0].start, mwnd[0].end, mwnd[0].score));
+				*/
+			}
+		}
+	}else{
+		ret.push_back(prediction(pos, pos+bufs, cvalue));
+	}
+	return ret;
+}
+
--- /dev/null
+++ mocca-1.1/src/models/rfmocca.hpp
@@ -0,0 +1,58 @@
+////////////////////////////////////////////////////////////////////////////////////
+// MOCCA
+// Copyright, Bjørn Bredesen, 2019
+// E-mail: bjorn@bjornbredesen.no
+////////////////////////////////////////////////////////////////////////////////////
+// General
+
+#pragma once
+
+////////////////////////////////////////////////////////////////////////////////////
+// Motif classifier
+
+class RFMotifOccClassifier{
+private:
+	config*cfg;
+	autodelete<RFClassifier> classifier;
+	int motifInd;
+	int nfeatures;
+	autofree<double> features;
+	motifList*motifs;
+	autodelete<MotifClassifier_featureSet> featureSet;
+	RFMotifOccClassifier(int mi,motifList*ml);
+public:
+	~RFMotifOccClassifier(){  }
+	static RFMotifOccClassifier*create(int mi,motifList*ml);
+	bool trainOcc(motifOcc*o,motifOccContainer*moc,long long wpos,char*buf,int bufs,seqClass*_cls);
+	bool trainFinish();
+	double applyOcc(motifOcc*o,motifOccContainer*moc,long long wpos,char*buf,int bufs);
+	void printInfo();
+	bool exportAnalysisData(FILE*f);
+};
+
+////////////////////////////////////////////////////////////////////////////////////
+// RF-MOCCA
+
+class RFMOCCA:public sequenceClassifier{
+private:
+	deletevector<RFMotifOccClassifier> subcls;
+	autodelete<motifWindow> mwin;
+	motifOccContainer*moc;
+	motifList*motifs;
+	RFMOCCA(int nf);
+	//
+	autodelete<logoddsClassifier> classifier;
+	autofree<double>fvec;
+	deletevector<trainingSequence> trainSeq;
+public:
+	static RFMOCCA*create(motifList*motifs);
+	virtual ~RFMOCCA(){  }
+	bool trainWindow(char*buf,long long pos,int bufs,seqClass*cls);
+	bool trainFinish();
+	double do_applyWindow(char*buf,long long pos,int bufs);
+	bool flush();
+	bool printInfo();
+	bool exportAnalysisData(string path);
+	virtual vector<prediction> predictWindow(char*buf,long long pos,int bufs, corePredictionModeT cpm);
+};
+
--- /dev/null
+++ mocca-1.1/src/models/seqlda.cpp
@@ -0,0 +1,97 @@
+////////////////////////////////////////////////////////////////////////////////////
+// MOCCA
+// Copyright, Bjørn Bredesen, 2019
+// E-mail: bjorn@bjornbredesen.no
+////////////////////////////////////////////////////////////////////////////////////
+// General
+
+#include "../common.hpp"
+#include "../lib/libsvm-3.17/svm.h"
+#include "../aux.hpp"
+#include "../config.hpp"
+#include "../validation.hpp"
+#include "../motifs.hpp"
+#include "../sequences.hpp"
+#include "../sequencelist.hpp"
+#include "baseclassifier.hpp"
+#include "sequenceclassifier.hpp"
+#include "features.hpp"
+#include "seqlda.hpp"
+
+////////////////////////////////////////////////////////////////////////////////////
+// SEQLDA
+
+SEQLDA::SEQLDA(int nf):sequenceClassifier(nf){  }
+
+SEQLDA*SEQLDA::create(motifList*motifs,featureSet*fs){
+	if(!motifs){cmdError("Null-pointer argument.");return 0;}
+	autodelete<motifWindow> _mwin;
+	autodelete<featureWindow> _fwin;
+	_mwin.ptr = motifWindow::create(motifs);
+	if(!_mwin.ptr){
+		return 0;
+	}
+	_fwin.ptr = featureWindow::create(_mwin.ptr, fs);
+	if(!_fwin.ptr){
+		return 0;
+	}
+	int nfeatures = _fwin.ptr->getNFeatures();
+	if(!nfeatures){
+		cmdError("No features for SEQLDA classifier");
+		return 0;
+	}
+	autodelete<SEQLDA> r(new SEQLDA(nfeatures));
+	if(!r.ptr){
+		outOfMemory();
+		return 0;
+	}
+	r.ptr->mwin.ptr = _mwin.disown();
+	r.ptr->fwin.ptr = _fwin.disown();
+	r.ptr->moc=r.ptr->mwin.ptr->occContainer;
+	r.ptr->motifs=r.ptr->mwin.ptr->motifs;
+	r.ptr->features=fs;
+	cout << r.ptr->nFeatures;
+	r.ptr->classifier.ptr = LDAClassifier::create(nfeatures, std::string("cls"));
+	if(!r.ptr->classifier.ptr){
+		return 0;
+	}
+	std::vector<std::string> featureNames = fs->getInstFeatureNames(motifs);
+	for(int i = 0; i < nfeatures; i++){
+		r.ptr->classifier.ptr->featureNames.push_back(featureNames[i]);
+	}
+	return r.disown();
+}
+
+bool SEQLDA::trainWindow(char*buf,long long pos,int bufs,seqClass*cls){
+	double*fvec=fwin.ptr->extractFeatures(buf,pos,bufs,true);
+	if(!fvec)return false;
+	classifier.ptr->addTrain(fvec,nFeatures,cls);
+	return true;
+}
+
+bool SEQLDA::trainFinish(){
+	if(!classifier.ptr->train())return false;
+	return true;
+}
+
+bool SEQLDA::flush(){
+	return mwin.ptr->flush();
+}
+
+double SEQLDA::do_applyWindow(char*buf,long long pos,int bufs){
+	double*fvec=fwin.ptr->extractFeatures(buf,pos,bufs,true);
+	if(!fvec)return -1;
+	return classifier.ptr->apply(fvec,nFeatures);
+}
+
+bool SEQLDA::printInfo(){
+	cout << t_indent << "SEQLDA classifier\n";
+	classifier.ptr->printInfo((char*)"Random Forest");
+	return true;
+}
+
+bool SEQLDA::exportAnalysisData(string path){
+	cout << "Model analysis export not yet supported for SEQLDA";
+	return false;
+}
+
--- /dev/null
+++ mocca-1.1/src/models/seqlda.hpp
@@ -0,0 +1,32 @@
+////////////////////////////////////////////////////////////////////////////////////
+// MOCCA
+// Copyright, Bjørn Bredesen, 2019
+// E-mail: bjorn@bjornbredesen.no
+////////////////////////////////////////////////////////////////////////////////////
+// General
+
+#pragma once
+
+////////////////////////////////////////////////////////////////////////////////////
+// SEQLDA
+
+class SEQLDA:public sequenceClassifier{
+private:
+	autodelete<motifWindow> mwin;
+	motifOccContainer*moc;
+	motifList*motifs;
+	featureSet*features;
+	autodelete<featureWindow> fwin;
+	autodelete<LDAClassifier> classifier;
+	SEQLDA(int nf);
+public:
+	static SEQLDA*create(motifList*motifs,featureSet*fs);
+	virtual ~SEQLDA(){  }
+	bool trainWindow(char*buf,long long pos,int bufs,seqClass*cls);
+	bool trainFinish();
+	bool flush();
+	double do_applyWindow(char*buf,long long pos,int bufs);
+	bool printInfo();
+	bool exportAnalysisData(string path);
+};
+
--- /dev/null
+++ mocca-1.1/src/models/seqperceptron.cpp
@@ -0,0 +1,97 @@
+////////////////////////////////////////////////////////////////////////////////////
+// MOCCA
+// Copyright, Bjørn Bredesen, 2019
+// E-mail: bjorn@bjornbredesen.no
+////////////////////////////////////////////////////////////////////////////////////
+// General
+
+#include "../common.hpp"
+#include "../lib/libsvm-3.17/svm.h"
+#include "../aux.hpp"
+#include "../config.hpp"
+#include "../validation.hpp"
+#include "../motifs.hpp"
+#include "../sequences.hpp"
+#include "../sequencelist.hpp"
+#include "baseclassifier.hpp"
+#include "sequenceclassifier.hpp"
+#include "features.hpp"
+#include "seqperceptron.hpp"
+
+////////////////////////////////////////////////////////////////////////////////////
+// SEQPerceptron
+
+SEQPerceptron::SEQPerceptron(int nf):sequenceClassifier(nf){  }
+
+SEQPerceptron*SEQPerceptron::create(motifList*motifs,featureSet*fs){
+	if(!motifs){cmdError("Null-pointer argument.");return 0;}
+	autodelete<motifWindow> _mwin;
+	autodelete<featureWindow> _fwin;
+	_mwin.ptr = motifWindow::create(motifs);
+	if(!_mwin.ptr){
+		return 0;
+	}
+	_fwin.ptr = featureWindow::create(_mwin.ptr, fs);
+	if(!_fwin.ptr){
+		return 0;
+	}
+	int nfeatures = _fwin.ptr->getNFeatures();
+	if(!nfeatures){
+		cmdError("No features for SEQPerceptron classifier");
+		return 0;
+	}
+	autodelete<SEQPerceptron> r(new SEQPerceptron(nfeatures));
+	if(!r.ptr){
+		outOfMemory();
+		return 0;
+	}
+	r.ptr->mwin.ptr = _mwin.disown();
+	r.ptr->fwin.ptr = _fwin.disown();
+	r.ptr->moc=r.ptr->mwin.ptr->occContainer;
+	r.ptr->motifs=r.ptr->mwin.ptr->motifs;
+	r.ptr->features=fs;
+	cout << r.ptr->nFeatures;
+	r.ptr->classifier.ptr = PerceptronClassifier::create(nfeatures, std::string("cls"));
+	if(!r.ptr->classifier.ptr){
+		return 0;
+	}
+	std::vector<std::string> featureNames = fs->getInstFeatureNames(motifs);
+	for(int i = 0; i < nfeatures; i++){
+		r.ptr->classifier.ptr->featureNames.push_back(featureNames[i]);
+	}
+	return r.disown();
+}
+
+bool SEQPerceptron::trainWindow(char*buf,long long pos,int bufs,seqClass*cls){
+	double*fvec=fwin.ptr->extractFeatures(buf,pos,bufs,true);
+	if(!fvec)return false;
+	classifier.ptr->addTrain(fvec,nFeatures,cls);
+	return true;
+}
+
+bool SEQPerceptron::trainFinish(){
+	if(!classifier.ptr->train())return false;
+	return true;
+}
+
+bool SEQPerceptron::flush(){
+	return mwin.ptr->flush();
+}
+
+double SEQPerceptron::do_applyWindow(char*buf,long long pos,int bufs){
+	double*fvec=fwin.ptr->extractFeatures(buf,pos,bufs,true);
+	if(!fvec)return -1;
+	return classifier.ptr->apply(fvec,nFeatures);
+}
+
+bool SEQPerceptron::printInfo(){
+	cout << t_indent << "SEQPerceptron classifier\n";
+	classifier.ptr->printInfo((char*)"Random Forest");
+	return true;
+}
+
+bool SEQPerceptron::exportAnalysisData(string path){
+	cout << "Model analysis export not yet supported for SEQPerceptron";
+	return false;
+}
+
--- /dev/null
+++ mocca-1.1/src/models/seqperceptron.hpp
@@ -0,0 +1,32 @@
+////////////////////////////////////////////////////////////////////////////////////
+// MOCCA
+// Copyright, Bjørn Bredesen, 2019
+// E-mail: bjorn@bjornbredesen.no
+////////////////////////////////////////////////////////////////////////////////////
+// General
+
+#pragma once
+
+////////////////////////////////////////////////////////////////////////////////////
+// SEQPerceptron
+
+class SEQPerceptron:public sequenceClassifier{
+private:
+	autodelete<motifWindow> mwin;
+	motifOccContainer*moc;
+	motifList*motifs;
+	featureSet*features;
+	autodelete<featureWindow> fwin;
+	autodelete<PerceptronClassifier> classifier;
+	SEQPerceptron(int nf);
+public:
+	static SEQPerceptron*create(motifList*motifs,featureSet*fs);
+	virtual ~SEQPerceptron(){  }
+	bool trainWindow(char*buf,long long pos,int bufs,seqClass*cls);
+	bool trainFinish();
+	bool flush();
+	double do_applyWindow(char*buf,long long pos,int bufs);
+	bool printInfo();
+	bool exportAnalysisData(string path);
+};
+
--- /dev/null
+++ mocca-1.1/src/models/seqrf.cpp
@@ -0,0 +1,97 @@
+////////////////////////////////////////////////////////////////////////////////////
+// MOCCA
+// Copyright, Bjørn Bredesen, 2019
+// E-mail: bjorn@bjornbredesen.no
+////////////////////////////////////////////////////////////////////////////////////
+// General
+
+#include "../common.hpp"
+#include "../lib/libsvm-3.17/svm.h"
+#include "../aux.hpp"
+#include "../config.hpp"
+#include "../validation.hpp"
+#include "../motifs.hpp"
+#include "../sequences.hpp"
+#include "../sequencelist.hpp"
+#include "baseclassifier.hpp"
+#include "sequenceclassifier.hpp"
+#include "features.hpp"
+#include "seqrf.hpp"
+
+////////////////////////////////////////////////////////////////////////////////////
+// SEQRF
+
+SEQRF::SEQRF(int nf):sequenceClassifier(nf){  }
+
+SEQRF*SEQRF::create(motifList*motifs,featureSet*fs){
+	if(!motifs){cmdError("Null-pointer argument.");return 0;}
+	autodelete<motifWindow> _mwin;
+	autodelete<featureWindow> _fwin;
+	_mwin.ptr = motifWindow::create(motifs);
+	if(!_mwin.ptr){
+		return 0;
+	}
+	_fwin.ptr = featureWindow::create(_mwin.ptr, fs);
+	if(!_fwin.ptr){
+		return 0;
+	}
+	int nfeatures = _fwin.ptr->getNFeatures();
+	if(!nfeatures){
+		cmdError("No features for SEQRF classifier");
+		return 0;
+	}
+	autodelete<SEQRF> r(new SEQRF(nfeatures));
+	if(!r.ptr){
+		outOfMemory();
+		return 0;
+	}
+	r.ptr->mwin.ptr = _mwin.disown();
+	r.ptr->fwin.ptr = _fwin.disown();
+	r.ptr->moc=r.ptr->mwin.ptr->occContainer;
+	r.ptr->motifs=r.ptr->mwin.ptr->motifs;
+	r.ptr->features=fs;
+	cout << r.ptr->nFeatures;
+	r.ptr->classifier.ptr = RFClassifier::create(nfeatures, std::string("cls"));
+	if(!r.ptr->classifier.ptr){
+		return 0;
+	}
+	std::vector<std::string> featureNames = fs->getInstFeatureNames(motifs);
+	for(int i = 0; i < nfeatures; i++){
+		r.ptr->classifier.ptr->featureNames.push_back(featureNames[i]);
+	}
+	return r.disown();
+}
+
+bool SEQRF::trainWindow(char*buf,long long pos,int bufs,seqClass*cls){
+	double*fvec=fwin.ptr->extractFeatures(buf,pos,bufs,true);
+	if(!fvec)return false;
+	classifier.ptr->addTrain(fvec,nFeatures,cls);
+	return true;
+}
+
+bool SEQRF::trainFinish(){
+	if(!classifier.ptr->train())return false;
+	return true;
+}
+
+bool SEQRF::flush(){
+	return mwin.ptr->flush();
+}
+
+double SEQRF::do_applyWindow(char*buf,long long pos,int bufs){
+	double*fvec=fwin.ptr->extractFeatures(buf,pos,bufs,true);
+	if(!fvec)return -1;
+	return classifier.ptr->apply(fvec,nFeatures);
+}
+
+bool SEQRF::printInfo(){
+	cout << t_indent << "SEQRF classifier\n";
+	classifier.ptr->printInfo((char*)"Random Forest");
+	return true;
+}
+
+bool SEQRF::exportAnalysisData(string path){
+	cout << "Model analysis export not yet supported for SEQRF";
+	return false;
+}
+
--- /dev/null
+++ mocca-1.1/src/models/seqrf.hpp
@@ -0,0 +1,32 @@
+////////////////////////////////////////////////////////////////////////////////////
+// MOCCA
+// Copyright, Bjørn Bredesen, 2019
+// E-mail: bjorn@bjornbredesen.no
+////////////////////////////////////////////////////////////////////////////////////
+// General
+
+#pragma once
+
+////////////////////////////////////////////////////////////////////////////////////
+// SEQRF
+
+class SEQRF:public sequenceClassifier{
+private:
+	autodelete<motifWindow> mwin;
+	motifOccContainer*moc;
+	motifList*motifs;
+	featureSet*features;
+	autodelete<featureWindow> fwin;
+	autodelete<RFClassifier> classifier;
+	SEQRF(int nf);
+public:
+	static SEQRF*create(motifList*motifs,featureSet*fs);
+	virtual ~SEQRF(){  }
+	bool trainWindow(char*buf,long long pos,int bufs,seqClass*cls);
+	bool trainFinish();
+	bool flush();
+	double do_applyWindow(char*buf,long long pos,int bufs);
+	bool printInfo();
+	bool exportAnalysisData(string path);
+};
+
--- mocca-1.1.orig/src/models/sequenceclassifier.cpp
+++ mocca-1.1/src/models/sequenceclassifier.cpp
@@ -238,40 +238,64 @@ bool sequenceClassifier::predictGenomewi
 		int rbn;
 		long nextSi=0;
 		double cvalue;
+		vector<prediction> pred;
 		flush();
+		int lastPredWndEnd = -1;
 		for(long i=0;(rbn=ssw->get(rb));i+=cfg->windowStep){
 			cit += rbn - (cfg->windowSize-cfg->windowStep);
 			if(cit>=nextSi){
 				nextSi+=50000;
 				task.setPercent((double(cit)/double(bptotal))*100.0);
 			}
-			cvalue=applyWindow(rb,i,rbn);
+			vector<prediction> wpred = predictWindow(rb, i, rbn, cfg->corePredictionMode);
+			cvalue = wpred.size() > 0 ? wpred.back().score : -9999999999.;
 			if(ofWig.is_open())
 				ofWig << (cvalue+threshold) << "\n";
 			if(cvalue>=0.){
-				if(pEnd==-1){
-					pStart=i;
-					pScore=cvalue+threshold;
+				if(cfg->corePredictionMax){
+					// For maximum core prediction mode, find the maximally scoring
+					// core prediction, and add/replace last depending on whether
+					// the last predicted window is non-overlapping or overlapping,
+					// respectively.
+					prediction pmax = prediction(-1, -1, -999999999.);
+					if(lastPredWndEnd >= i) pmax = pred.back();
+					for(auto&p: wpred)
+						if(p.score > pmax.score)
+							pmax = p;
+					if(lastPredWndEnd >= i) pred.back() = pmax;
+					else pred.push_back(pmax);
 				}else{
-					pScore=max(cvalue+threshold,pScore);
+					// For the normal mode, just add all.
+					for(auto&p: wpred)
+						pred.push_back(p);
 				}
-				pEnd=i+rbn;
-			}else if(pEnd != -1 && i > pEnd){
-				cmdTask::wipe();
-				cout << t_indent << "Predicted: " << chromName << ":" << pStart << ".." << pEnd << " - score: " << pScore << "\n";
-				cmdTask::refresh();
-				if(ofGFF.is_open())
-					ofGFF << chromName << "\tMOCCA\tPrediction\t" << pStart << "\t" << pEnd << "\t" << pScore << "\t.\t.\t1\n";
-				pEnd = -1;
-				nPredictions++;
+				lastPredWndEnd = i + rbn;
 			}
 		}
 		if(pEnd != -1){
 			cmdTask::wipe();
-			cout << t_indent << "Predicted: " << chromName << ":" << pStart << ".." << pEnd << " - score: " << pScore << "\n";
+		}
+		// Flatten predictions
+		sort(pred.begin(),pred.end(),
+		[](const prediction a,const prediction b){
+			return a.start < b.start;
+		});
+		vector<prediction> fpred = vector<prediction>();
+		for(auto&p: pred){
+			if(fpred.size() > 0 && p.start < fpred.back().end){
+				fpred.back().end = max(fpred.back().end, p.end);
+				fpred.back().score = max(fpred.back().score, p.score);
+			}else{
+				fpred.push_back(prediction(p.start, p.end, p.score));
+			}
+		}
+		// Save predictions
+		for(auto&p: fpred){
+			cmdTask::wipe();
+			cout << t_indent << "Predicted: " << chromName << ":" << p.start << ".." << p.end << " (" << (p.end-p.start) << " bp) - score: " << p.score << "\n";
 			cmdTask::refresh();
 			if(ofGFF.is_open())
-				ofGFF << chromName << "\tMOCCA\tPrediction\t" << pStart << "\t" << pEnd << "\t" << pScore << "\t.\t.\t1\n";
+				ofGFF << chromName << "\tMOCCA\tPrediction\t" << p.start << "\t" << p.end << "\t" << p.score << "\t.\t.\t1\n";
 			nPredictions++;
 		}
 		delete ssw;
@@ -338,6 +362,14 @@ bool sequenceClassifier::predictCoreSequ
 	return true;
 }
 
+vector<prediction> sequenceClassifier::predictWindow(char*buf,long long pos,int bufs, corePredictionModeT cpm){
+	double cvalue = applyWindow(buf, pos, bufs) + threshold;
+	vector<prediction> ret = vector<prediction>();
+	if(cvalue >= threshold)
+		ret.push_back(prediction(pos, pos+bufs, cvalue));
+	return ret;
+}
+
 bool sequenceClassifier::calibrateThresholdGenomewidePrecision(seqList*calpos,double wantPrecision){
 	threshold=0.;
 	int nvp=0;
--- mocca-1.1.orig/src/models/sequenceclassifier.hpp
+++ mocca-1.1/src/models/sequenceclassifier.hpp
@@ -7,6 +7,20 @@
 
 #pragma once
 
+struct prediction{
+	int start, end, center;
+	double score;
+	int mstart, mend;
+	prediction(int _start, int _end, double _score, int _mstart = -1, int _mend = -1){
+		start = _start;
+		end = _end;
+		center = (start + end) / 2;
+		score = _score;
+		mstart = _mstart;
+		mend = _mend;
+	}
+};
+
 ////////////////////////////////////////////////////////////////////////////////////
 // Sequence classifiers
 
@@ -47,6 +61,7 @@ public:
 	virtual bool flush() = 0;
 	virtual bool printInfo() = 0;
 	virtual bool exportAnalysisData(string path) = 0;
+	virtual vector<prediction> predictWindow(char*buf,long long pos,int bufs, corePredictionModeT cpm);
 };
 
 ////////////////////////////////////////////////////////////////////////////////////
--- mocca-1.1.orig/src/models/svmmocca.cpp
+++ mocca-1.1/src/models/svmmocca.cpp
@@ -415,3 +415,190 @@ bool SVMMOCCA::exportAnalysisData(string
 	return true;
 }
 
+vector<prediction> SVMMOCCA::predictWindow(char*buf,long long pos,int bufs, corePredictionModeT cpm){
+	vector<prediction> ret = vector<prediction>();
+	if(!mwin.ptr->readWindow(buf,pos,bufs)){
+		return ret;
+	}
+	vector<prediction> motpos = vector<prediction>();
+	for(int x=0;x<motifs->nmotifs;x++){
+		MotifOccClassifier*c=subcls[x];
+		motifOcc*o=moc->getFirst(x);
+		int nc=0;
+		while(o){
+			if( c->applyOcc(o,moc,pos,buf,bufs)>0 ){
+				nc++;
+				if(cpm != cpmNone){
+					int center = (o->start + o->start + motifs->motifs[x].len) / 2;
+					double mscore = classifier.ptr->getWeight(x);
+					motpos.push_back(prediction(center - 250, center + 250, mscore,
+						o->start,
+						o->start + motifs->motifs[x].len));
+				}
+			}
+			o=moc->getNextSame(o);
+		}
+		fvec[x]=double(nc*1000)/double(bufs);
+	}
+	double cvalue = classifier.ptr->apply(fvec.ptr,nFeatures);
+	if(cpm != cpmNone){
+		if(cvalue >= threshold){
+			// Motif prediction--based core prediction algorithm
+			if(cpm == cpmMotifs || cpm == cpmMotifsStrong){
+				// Try to limit to central occurrences with cumulative scores above threshold
+				vector<prediction> cmotpos = vector<prediction>();
+				sort(motpos.begin(),motpos.end(),
+				[](const prediction a,const prediction b){
+					return a.mstart < b.mstart;
+				});
+				for(auto&oA: motpos){
+					double cumscore = 0.;
+					for(auto&oB: motpos){
+						if(oB.mstart < oA.start) continue;
+						if(oB.mend > oA.end) break;
+						cumscore += oB.score;
+					}
+					if(cumscore >= threshold)
+						cmotpos.push_back(prediction(
+							max(oA.start, 0),
+							min(oA.end, (int)pos + bufs),
+							cumscore));
+				}
+				if(cpm == cpmMotifsStrong) return cmotpos;
+				if(cmotpos.size() > 0) motpos = cmotpos;
+				// Flatten
+				for(auto&occ: motpos){
+					//if(ret.size() > 0 && occ.start < ret.back().end+500){
+					if(ret.size() > 0 && occ.start < ret.back().end){
+						ret.back().end = max(ret.back().end, occ.end);
+						ret.back().score = max(ret.back().score, occ.score);
+					}else{
+						ret.push_back(prediction(occ.start, occ.end, occ.score));
+					}
+				}
+				// If there are predictions scoring above the threshold, predict those.
+				// Otherwise, there must be many low-scoring predictions, so predict
+				// all of them.
+				// Thus: Get predictions with scores above threshold.
+				vector<prediction> pred = vector<prediction>();
+				for(auto&p: ret)
+					if(p.score >= threshold)
+						pred.push_back(p);
+				// If any, return that list
+				if(pred.size() > 0)
+					return pred;
+				// If none, return the regular list of predictions
+			// Continuous core prediction algorithm
+			}else if(cpm == cpmContinuous){
+				// For continuous predictions, we want to predict a core delimited
+				// by motif occurrences that spans potentially a larger region, and
+				// has a high score per basepair.
+				sort(motpos.begin(),motpos.end(),
+				[](const prediction a,const prediction b){
+					return a.center < b.center;
+				});
+				vector<prediction> cmotpos = vector<prediction>();
+				for(auto&m: motpos) cmotpos.push_back(m);
+				sort(cmotpos.begin(),cmotpos.end(),
+				[](const prediction a,const prediction b){
+					return a.mstart < b.mstart;
+				});
+				int maxWinStart = -1;
+				int maxWinEnd = -1;
+				double maxWinScore = -1.;
+				double maxWinCumScore = -1.;
+				int iCA = 0;
+				for(int iA = 0; iA < motpos.size() - 1; iA++){
+					auto&oA = motpos[iA];
+					int iCB = iCA;
+					double cumscoreA = 0.;
+					for(int iB = iA; iB < motpos.size(); iB++){
+						auto&oB = motpos[iB];
+						if(oB.end > oA.start + cfg->windowSize)
+							break;
+						int wA = max(oA.start, 0);
+						int wB = min(oB.end, (int)pos + bufs);
+						if(wB-wA < oA.end-oA.start) continue;
+						bool block = false;
+						double cumscore = cumscoreA;
+						for(int iC = iCB; iC < cmotpos.size(); iC++){
+							auto&oC = cmotpos[iC];
+							// Occurrences are sorted by motif start, so if they
+							// start before the window, we can safely skip.
+							if(oC.mstart < wA){
+								iCA++;
+								iCB++;
+								continue;
+							}
+							//if(oC.mstart > wB) continue;
+							// If the occurrence is past the window end, we can
+							// safely break.
+							if(oC.mstart > wB) break;
+							// If only the end is past, later shorter occurrences
+							// may still be inside, so just block updating of the
+							// bookmark position.
+							if(oC.mend > wB) {
+								block = true;
+								continue;
+							}
+							// The occurrence is inside the window, so add.
+							cumscore += oC.score;
+							// If an occurrence was past the end of the window,
+							// do not update bookmarks.
+							if(!block) {
+								iCB = iC + 1;
+								cumscoreA = cumscore;
+							}
+						}
+						double winScore = cumscore / max(double(wB - wA), 1.);
+						if(maxWinStart == -1 || winScore > maxWinScore){
+							maxWinStart = wA;
+							maxWinEnd = wB;
+							maxWinScore = winScore;
+							maxWinCumScore = cumscore;
+						}
+					}
+				}
+				if(maxWinStart == -1) return ret;
+				ret.push_back(prediction(maxWinStart, maxWinEnd, maxWinCumScore));
+				/*
+				// Unoptimized base algorithm
+				sort(motpos.begin(),motpos.end(),
+				[](const prediction a,const prediction b){
+					return a.center < b.center;
+				});
+				vector<prediction> mwnd = vector<prediction>();
+				for(int iA = 0; iA < motpos.size() - 1; iA++){
+					auto&oA = motpos[iA];
+					for(int iB = iA; iB < motpos.size(); iB++){
+						auto&oB = motpos[iB];
+						if(oB.end > oA.start + cfg->windowSize)
+							break;
+						int wA = max(oA.start, 0);
+						int wB = min(oB.end, (int)pos + bufs);
+						if(wB-wA < oA.end-oA.start) continue;
+						double cumscore = 0.;
+						for(auto&oC: motpos){
+							if(oC.mstart < wA || oC.mend > wB) continue;
+							cumscore += oC.score;
+						}
+						mwnd.push_back(prediction(wA, wB, cumscore));
+					}
+				}
+				if(mwnd.size() == 0) return mwnd;
+				sort(mwnd.begin(),mwnd.end(),
+				[](const prediction a,const prediction b){
+					double sA = a.score / max(double(a.end - a.start), 1.);
+					double sB = b.score / max(double(b.end - b.start), 1.);
+					return sA > sB;
+				});
+				ret.push_back(prediction(mwnd[0].start, mwnd[0].end, mwnd[0].score));
+				*/
+			}
+		}
+	}else{
+		ret.push_back(prediction(pos, pos+bufs, cvalue));
+	}
+	return ret;
+}
+
--- mocca-1.1.orig/src/models/svmmocca.hpp
+++ mocca-1.1/src/models/svmmocca.hpp
@@ -82,5 +82,6 @@ public:
 	bool flush();
 	bool printInfo();
 	bool exportAnalysisData(string path);
+	virtual vector<prediction> predictWindow(char*buf,long long pos,int bufs, corePredictionModeT cpm);
 };
 
--- mocca-1.1.orig/src/sequencelist.cpp
+++ mocca-1.1/src/sequencelist.cpp
@@ -35,6 +35,15 @@ seqClass*getSeqClassByName(std::string n
 	return 0;
 }
 
+seqClass*getSeqClassByValue(double cls){
+	for(auto& c: seqClasses)
+		if(c.cls == cls)
+			return &c;
+	cmdError("Class not found.");
+	cout << t_indent << "Requested class: " << cls << "\n";
+	return 0;
+}
+
 void printSeqClasses(){
 	cmdSection("Classes");
 	for(auto& c: seqClasses)
@@ -195,7 +204,48 @@ bool seqList::addRandomIid(char*tpath,in
 		ostringstream oss;
 		int seed=rand();
 		srand(seed);
-		oss << "Random " << nseq << "(seed=" << seed << ")";
+		oss << "Random (i.i.d.) " << nseq << "(seed=" << seed << ")";
+		if(!addSeq(cloneString((char*)oss.str().c_str()),buf.disown(),len,cls,tm)){
+			return false;
+		}
+	}
+	return true;
+}
+
+bool seqList::addRandomMC(char*tpath,int nadd,int len,seqClass*cls,e_trainMode tm,int order){
+	if(!cls||tm==train_Invalid)return false;
+	if(len<=0){
+		cmdError("Invalid random sequence length requested.");
+		return false;
+	}else if(nadd<=0){
+		cmdError("Invalid number of random sequences requested.");
+		return false;
+	}
+	// Train generative model
+	autodelete<seqStreamRandomMC> rss(new seqStreamRandomMC(order));
+	if(!rss.ptr){
+		outOfMemory();
+		return false;
+	}
+	autodelete<seqStreamFastaBatch> ssfb(seqStreamFastaBatch::load(tpath));
+	if(!ssfb.ptr){
+		return false;
+	}
+	for(seqStreamFastaBatchBlock*ssfbblk;(ssfbblk=ssfb.ptr->getBlock());){
+		rss.ptr->train(ssfbblk);
+	}
+	// Generate and add random sequences
+	for(int l=0;l<nadd;l++){
+		autofree<char> buf((char*)malloc(len));
+		if(!buf.ptr){
+			outOfMemory();
+			return false;
+		}
+		rss.ptr->read(len,buf.ptr);
+		ostringstream oss;
+		int seed=rand();
+		srand(seed);
+		oss << "Random (MC order " << order << ") " << nseq << "(seed=" << seed << ")";
 		if(!addSeq(cloneString((char*)oss.str().c_str()),buf.disown(),len,cls,tm)){
 			return false;
 		}
--- mocca-1.1.orig/src/sequencelist.hpp
+++ mocca-1.1/src/sequencelist.hpp
@@ -21,6 +21,7 @@ typedef struct{
 }seqClass;
 
 seqClass*getSeqClassByName(std::string name);
+seqClass*getSeqClassByValue(double cls);
 
 /*
 printSeqClasses
@@ -104,11 +105,17 @@ public:
 	*/
 	bool loadFastaBatch(char*path,seqClass*cls,e_trainMode tm);
 	/*
-	addRandom
+	addRandomIid
 		Adds N random sequences of length L with class 'cls'.
 	*/
 	bool addRandomIid(char*tpath,int nadd,int len, seqClass*cls, e_trainMode tm);
 	/*
+	addRandomMC
+		Adds N random sequences of length L with class 'cls',
+		generated by an order-th order Markov chain.
+	*/
+	bool addRandomMC(char*tpath,int nadd,int len, seqClass*cls, e_trainMode tm, int order);
+	/*
 	addClone
 		Adds a clone of the given sequence to the set.
 	*/
--- mocca-1.1.orig/src/sequences.cpp
+++ mocca-1.1/src/sequences.cpp
@@ -85,7 +85,7 @@ bool seqStreamBuffer::setpos(long pos){
 }
 
 ////////////////////////////////////////////////////////////////////////////////////
-// seqStreamRandom
+// seqStreamRandomIid
 
 seqStreamRandomIid::seqStreamRandomIid(){ // dt = 1-da-dc-dg.
 	nA = 0;
@@ -143,6 +143,113 @@ bool seqStreamRandomIid::setpos(long pos
 	return false;
 }
 
+////////////////////////////////////////////////////////////////////////////////////
+// seqStreamRandomMarkov
+
+seqStreamRandomMC::seqStreamRandomMC(int _order, int _pseudo, bool _addRC){
+	order = _order;
+	pseudo = _pseudo;
+	addRC = _addRC;
+	nspectrum = 4 << (order << 1);
+	spectrum.resize((size_t)nspectrum);
+	spectrum.fill((size_t)nspectrum, 0);
+	nprobs = 4 << ((order - 1) << 1);
+	probs.resize((size_t)nprobs);
+	probs.fill((size_t)nprobs, 0);
+	prepared = false;
+}
+
+bool seqStreamRandomMC::train(seqStream*input){
+	cmdTask task((char*)"Training Markov chain background model");
+	#define FGCSBUFSIZE 256
+	autofree<char> buf((char*)malloc(FGCSBUFSIZE));
+	unsigned int state = 0;
+	unsigned int stateRC = 0;
+	int ivalid = 0;
+	long nti=0;
+	for(long wind=0;;wind++){
+		int nread=input->read(FGCSBUFSIZE,buf.ptr);
+		if(!nread)break;
+		char*c=buf.ptr;
+		for(int y=0;y<nread;y++,c++){
+			int ntc = 0;
+			if(*c == 'A') ntc = 0;
+			else if(*c == 'T') ntc = 1;
+			else if(*c == 'G') ntc = 2;
+			else if(*c == 'C') ntc = 3;
+			else ivalid = nti + y + 1;
+			state = ((state << 2) | ntc) & (nspectrum - 1);
+			stateRC = (stateRC >> 2) | ((ntc^1) << (order << 1));
+			if(nti + y > ivalid + order){
+				spectrum[state]++;
+				if(addRC) spectrum[stateRC]++;
+			}
+		}
+		nti+=nread;
+		if(!(wind%100))task.setLongT(nti,(char*)"nt");
+	}
+	return true;
+}
+
+void seqStreamRandomMC::postprocess(){
+	// Add pseudocounts
+	if(pseudo > 0){
+		int*s = spectrum.ptr;
+		for(int i = 0; i < nspectrum; i++, s++)
+			(*s) += pseudo;
+	}
+	// Calculate weights
+	MCProbability*p = probs.ptr;
+	unsigned int ntot = 0;
+	for(int i = 0; i < nprobs; i++, p++){
+		p->nA = spectrum.ptr[(i << 2) | 0];
+		p->nT = spectrum.ptr[(i << 2) | 1];
+		p->nG = spectrum.ptr[(i << 2) | 2];
+		p->nC = spectrum.ptr[(i << 2) | 3];
+		unsigned int bptotal = p->nA + p->nT + p->nG + p->nC;
+		p->total = bptotal;
+		p->rA = double(p->nA) / double(bptotal);
+		p->rT = p->rA + (double(p->nT) / double(bptotal));
+		p->rG = p->rT + (double(p->nG) / double(bptotal));
+		ntot += bptotal;
+	}
+	// Get random start state
+	genstate = 0;
+	unsigned int irv = (rand() ^ (rand() << 12)) % ntot;
+	p = probs.ptr;
+	for(int i = 0; i < nprobs; i++, p++){
+		if(irv <= p->total){
+			genstate = i;
+			break;
+		}
+		irv -= p->total;
+	}
+}
+
+int seqStreamRandomMC::read(int len,char*dest){
+	if(!prepared){
+		postprocess();
+		prepared = true;
+	}
+	unsigned int frv;
+	char*d=dest;
+	int ntc = 0;
+	for(int x=0;x<len;x++,d++){
+		MCProbability*p = &probs.ptr[genstate];
+		frv = (rand() ^ (rand() << 12)) % p->total;
+		if(frv < p->nA) ntc = 0, (*d) = 'A';
+		else if(frv < p->nA + p->nT) ntc = 1, (*d) = 'T';
+		else if(frv < p->nA + p->nT + p->nG) ntc = 2, (*d) = 'G';
+		else ntc = 3, (*d) = 'C';
+		genstate = ((genstate << 2) | ntc) & (nprobs - 1);
+	}
+	return len;
+}
+
+bool seqStreamRandomMC::setpos(long pos){
+	return false;
+}
+
 ////////////////////////////////////////////////////////////////////////////////////
 // seqStreamFasta
 
--- mocca-1.1.orig/src/sequences.hpp
+++ mocca-1.1/src/sequences.hpp
@@ -72,7 +72,7 @@ public:
 
 /*
 seqStreamRandomIid
-	A sequence stream class for randomly generated sequences.
+	A sequence stream class for i.i.d. randomly generated sequences.
 	
 	This sequence stream has no end, and buffer()
 	should never be called.
@@ -86,6 +86,37 @@ public:
 	bool train(seqStream*input);
 	int read(int len,char*dest);
 	bool setpos(long pos);
+};
+
+typedef struct{
+	unsigned int nA, nT, nG, nC, nU, total;
+	double rA, rT, rG;
+}MCProbability;
+
+/*
+seqStreamRandomMarkov
+	A sequence stream class for randomly generated sequences by a Markov chain.
+	
+	This sequence stream has no end, and buffer()
+	should never be called.
+*/
+class seqStreamRandomMC:public seqStream{
+private:
+	int order;
+	int pseudo;
+	bool addRC;
+	int nprobs;
+	int unsigned genstate;
+	autofree<MCProbability> probs;
+	int nspectrum;
+	autofree<int> spectrum;
+	void postprocess();
+	bool prepared;
+public:
+	seqStreamRandomMC(int _order, int _pseudo = 1, bool _addRC = true);
+	bool train(seqStream*input);
+	int read(int len,char*dest);
+	bool setpos(long pos);
 };
 
 ////////////////////////////////////////////////////////////////////////////////////
